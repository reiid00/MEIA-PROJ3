{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d72859ab",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87742597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Dataset\n",
    "#Kaggle: https://www.kaggle.com/datasets/shivamb/go-emotions-google-emotions-dataset\n",
    "\n",
    "!wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_1.csv\n",
    "!wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_2.csv\n",
    "!wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_3.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b83dc85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr  6 18:02:37 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 531.14                 Driver Version: 531.14       CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 Ti    WDDM | 00000000:29:00.0  On |                  N/A |\n",
      "| 30%   37C    P0               42W / 200W|    403MiB /  8192MiB |      2%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      8212    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A     10780    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A     10804    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     11796    C+G   ...23.0_x86__zpdnekdrzrea0\\Spotify.exe    N/A      |\n",
      "|    0   N/A  N/A     12856    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     13404    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     13700    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     13708    C+G   C:\\Windows\\System32\\WWAHost.exe           N/A      |\n",
      "|    0   N/A  N/A     13768    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     14380    C+G   ...Programs\\Microsoft VS Code\\Code.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "630a3245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import preprocessor\n",
    "import contractions\n",
    "import json\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "914f4fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>rater_id</th>\n",
       "      <th>example_very_unclear</th>\n",
       "      <th>admiration</th>\n",
       "      <th>...</th>\n",
       "      <th>love</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That game hurt.</td>\n",
       "      <td>eew5j0j</td>\n",
       "      <td>Brdd9</td>\n",
       "      <td>nrl</td>\n",
       "      <td>t3_ajis4z</td>\n",
       "      <td>t1_eew18eq</td>\n",
       "      <td>1.548381e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&gt;sexuality shouldn’t be a grouping category I...</td>\n",
       "      <td>eemcysk</td>\n",
       "      <td>TheGreen888</td>\n",
       "      <td>unpopularopinion</td>\n",
       "      <td>t3_ai4q37</td>\n",
       "      <td>t3_ai4q37</td>\n",
       "      <td>1.548084e+09</td>\n",
       "      <td>37</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You do right, if you don't care then fuck 'em!</td>\n",
       "      <td>ed2mah1</td>\n",
       "      <td>Labalool</td>\n",
       "      <td>confessions</td>\n",
       "      <td>t3_abru74</td>\n",
       "      <td>t1_ed2m7g7</td>\n",
       "      <td>1.546428e+09</td>\n",
       "      <td>37</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Man I love reddit.</td>\n",
       "      <td>eeibobj</td>\n",
       "      <td>MrsRobertshaw</td>\n",
       "      <td>facepalm</td>\n",
       "      <td>t3_ahulml</td>\n",
       "      <td>t3_ahulml</td>\n",
       "      <td>1.547965e+09</td>\n",
       "      <td>18</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[NAME] was nowhere near them, he was by the Fa...</td>\n",
       "      <td>eda6yn6</td>\n",
       "      <td>American_Fascist713</td>\n",
       "      <td>starwarsspeculation</td>\n",
       "      <td>t3_ackt2f</td>\n",
       "      <td>t1_eda65q2</td>\n",
       "      <td>1.546669e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       id  \\\n",
       "0                                    That game hurt.  eew5j0j   \n",
       "1   >sexuality shouldn’t be a grouping category I...  eemcysk   \n",
       "2     You do right, if you don't care then fuck 'em!  ed2mah1   \n",
       "3                                 Man I love reddit.  eeibobj   \n",
       "4  [NAME] was nowhere near them, he was by the Fa...  eda6yn6   \n",
       "\n",
       "                author            subreddit    link_id   parent_id  \\\n",
       "0                Brdd9                  nrl  t3_ajis4z  t1_eew18eq   \n",
       "1          TheGreen888     unpopularopinion  t3_ai4q37   t3_ai4q37   \n",
       "2             Labalool          confessions  t3_abru74  t1_ed2m7g7   \n",
       "3        MrsRobertshaw             facepalm  t3_ahulml   t3_ahulml   \n",
       "4  American_Fascist713  starwarsspeculation  t3_ackt2f  t1_eda65q2   \n",
       "\n",
       "    created_utc  rater_id  example_very_unclear  admiration  ...  love  \\\n",
       "0  1.548381e+09         1                 False           0  ...     0   \n",
       "1  1.548084e+09        37                  True           0  ...     0   \n",
       "2  1.546428e+09        37                 False           0  ...     0   \n",
       "3  1.547965e+09        18                 False           0  ...     1   \n",
       "4  1.546669e+09         2                 False           0  ...     0   \n",
       "\n",
       "   nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
       "0            0         0      0            0       0        0        1   \n",
       "1            0         0      0            0       0        0        0   \n",
       "2            0         0      0            0       0        0        0   \n",
       "3            0         0      0            0       0        0        0   \n",
       "4            0         0      0            0       0        0        0   \n",
       "\n",
       "   surprise  neutral  \n",
       "0         0        0  \n",
       "1         0        0  \n",
       "2         0        1  \n",
       "3         0        0  \n",
       "4         0        1  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = 'data/full_dataset/goemotions_'\n",
    "OUTPUT_DIR = 'training_data'\n",
    "\n",
    "df1 = pd.read_csv(f'{DATA_PATH}1.csv')\n",
    "df2 = pd.read_csv(f'{DATA_PATH}2.csv')\n",
    "df3 = pd.read_csv(f'{DATA_PATH}3.csv')\n",
    "\n",
    "frames = [df1,df2,df3]\n",
    "\n",
    "df = pd.concat(frames)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ecff46d",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87fb69ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM: https://www.kaggle.com/code/esknight/emotion-classification-final\n",
    "# Function for cleaning text\n",
    "def clean_text(text):\n",
    "    re_number = re.compile('[0-9]+')\n",
    "    re_url = re.compile(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\")\n",
    "    re_tag = re.compile('\\[[A-Z]+\\]')\n",
    "    re_char = re.compile('[^0-9a-zA-Z\\s?!.,:\\'\\\"//]+')\n",
    "    re_char_clean = re.compile('[^0-9a-zA-Z\\s?!.,\\[\\]]')\n",
    "    re_punc = re.compile('[?!,.\\'\\\"]')\n",
    "  \n",
    "    text = re.sub(re_char, \"\", text) # Remove unknown character \n",
    "    text = contractions.fix(text) # Expand contraction\n",
    "    text = re.sub(re_url, ' [url] ', text) # Replace URL with number\n",
    "    text = re.sub(re_char_clean, \"\", text) # Only alphanumeric and punctuations.\n",
    "    #text = re.sub(re_punc, \"\", text) # Remove punctuation.\n",
    "    text = text.lower() # Lower text\n",
    "    text = \" \".join([w for w in text.split(' ') if w != \" \"]) # Remove whitespace\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cce4567c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "796b33311fb04b6daf6082db1cabee87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/211225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clean text\n",
    "df['clean_text'] = df['text'].progress_apply(clean_text)\n",
    "\n",
    "# Drop Useless Columns\n",
    "df = df.drop(columns=['id','example_very_unclear','author','subreddit','link_id','parent_id','created_utc','rater_id'])\n",
    "\n",
    "# Reorganize Columns\n",
    "df = df[['clean_text'] + [col for col in df.columns if col not in ['text','clean_text']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98a7282b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise',\n",
       " 'neutral']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View emotions easier\n",
    "emotions = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "482895ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {str(i):label for i, label in enumerate(emotions)}\n",
    "label2id = {label:str(i) for i, label in enumerate(emotions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f2819ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'admiration', '1': 'amusement', '2': 'anger', '3': 'annoyance', '4': 'approval', '5': 'caring', '6': 'confusion', '7': 'curiosity', '8': 'desire', '9': 'disappointment', '10': 'disapproval', '11': 'disgust', '12': 'embarrassment', '13': 'excitement', '14': 'fear', '15': 'gratitude', '16': 'grief', '17': 'joy', '18': 'love', '19': 'nervousness', '20': 'optimism', '21': 'pride', '22': 'realization', '23': 'relief', '24': 'remorse', '25': 'sadness', '26': 'surprise', '27': 'neutral'}\n",
      "{'admiration': '0', 'amusement': '1', 'anger': '2', 'annoyance': '3', 'approval': '4', 'caring': '5', 'confusion': '6', 'curiosity': '7', 'desire': '8', 'disappointment': '9', 'disapproval': '10', 'disgust': '11', 'embarrassment': '12', 'excitement': '13', 'fear': '14', 'gratitude': '15', 'grief': '16', 'joy': '17', 'love': '18', 'nervousness': '19', 'optimism': '20', 'pride': '21', 'realization': '22', 'relief': '23', 'remorse': '24', 'sadness': '25', 'surprise': '26', 'neutral': '27'}\n"
     ]
    }
   ],
   "source": [
    "print(id2label)\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dda44d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>admiration</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>approval</th>\n",
       "      <th>caring</th>\n",
       "      <th>confusion</th>\n",
       "      <th>curiosity</th>\n",
       "      <th>desire</th>\n",
       "      <th>...</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>that game hurt.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sexuality should not be a grouping category i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you do right, if you do not care then fuck them!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>man i love reddit.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>name was nowhere near them, he was by the falc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  admiration  amusement  \\\n",
       "0                                    that game hurt.           0          0   \n",
       "1   sexuality should not be a grouping category i...           0          0   \n",
       "2   you do right, if you do not care then fuck them!           0          0   \n",
       "3                                 man i love reddit.           0          0   \n",
       "4  name was nowhere near them, he was by the falc...           0          0   \n",
       "\n",
       "   anger  annoyance  approval  caring  confusion  curiosity  desire  ...  \\\n",
       "0      0          0         0       0          0          0       0  ...   \n",
       "1      0          0         0       0          0          0       0  ...   \n",
       "2      0          0         0       0          0          0       0  ...   \n",
       "3      0          0         0       0          0          0       0  ...   \n",
       "4      0          0         0       0          0          0       0  ...   \n",
       "\n",
       "   nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
       "0            0         0      0            0       0        0        1   \n",
       "1            0         0      0            0       0        0        0   \n",
       "2            0         0      0            0       0        0        0   \n",
       "3            0         0      0            0       0        0        0   \n",
       "4            0         0      0            0       0        0        0   \n",
       "\n",
       "   surprise  neutral                                             labels  \n",
       "0         0        0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1         0        0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2         0        1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3         0        0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4         0        1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-Hot Encoding all Emotions\n",
    "df[\"labels\"] = df[emotions].values.tolist()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86a5f182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((168937, 30), (42288, 30))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create train / test splits\n",
    "mask = np.random.rand(len(df)) < 0.8\n",
    "df_train = df[mask]\n",
    "df_test = df[~mask]\n",
    "\n",
    "(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6cfca4",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a9c9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotions Visualization by number of cases\n",
    "\n",
    "temp = df[list(emotions)].sum(axis=0) \\\n",
    "    .reset_index() \\\n",
    "    .rename(columns={'index': 'emotion', 0: 'n'}) \\\n",
    "    .sort_values('n', ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.tick_params(axis='x', rotation=90)\n",
    "sns.barplot(data=temp, x='n', \n",
    "            y='emotion',\n",
    "            dodge=False,\n",
    "            ax=ax).set_title('Emotions by number of appearances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3428cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating emotions\n",
    "\n",
    "pos = {'admiration','amusement','approval','caring','desire','excitement','gratitude','joy','love',\n",
    "       'optimism','pride','relief'}\n",
    "neg = {'sadness','fear','embarrassment','disapproval','disappointment','annoyance','anger','nervousness',\n",
    "       'remorse','grief','disgust'}\n",
    "amb= {'realization','surprise','curiosity','confusion','neutral'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0cbe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotions and data vis\n",
    "\n",
    "print(\"Length of data: \", len(df))\n",
    "print(\"Number of emotions: \", len(emotions))\n",
    "print(\"Number of positive emotions: \", len(pos))\n",
    "print(\"Number of negative emotions: \", len(neg))\n",
    "print(\"Number of ambiguous emotions: \", len(amb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3aed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotions dataframe to later on aggregate\n",
    "\n",
    "df_emotion = pd.DataFrame()\n",
    "df_emotion['emotion'] = list(emotions)\n",
    "df_emotion['group'] = ''\n",
    "df_emotion['group'].loc[df_emotion['emotion'].isin(pos)] = 'positive'\n",
    "df_emotion['group'].loc[df_emotion['emotion'].isin(neg)] = 'negative'\n",
    "df_emotion['group'].loc[df_emotion['emotion'].isin(amb)] = 'ambiguous'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bfe9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emotion.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7952ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotions by number of appearences but by group\n",
    "\n",
    "temp = pd.DataFrame()\n",
    "temp['true positive rate'] = df.iloc[:, 3:-1].mean(0)\n",
    "temp['emotion'] = df.columns[3:-1]\n",
    "temp = temp.merge(df_emotion, how='left', on='emotion')\n",
    "temp = temp.sort_values('true positive rate')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "ax.tick_params(axis='x', rotation=90)\n",
    "\n",
    "sns.barplot(x=temp['emotion'], \n",
    "            y=temp['true positive rate'], \n",
    "            hue=temp['group'], \n",
    "            dodge=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d51f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def represent_train_test_balance(train_df,test_df):\n",
    "    # Class representation for train/test DS\n",
    "    train_GO = (train_df.loc[:,list(emotions)].sum(axis=0) / len(train_df)) * 100\n",
    "    test_GO = (test_df.loc[:,list(emotions)].sum(axis=0) / len(test_df)) * 100\n",
    "    \n",
    "    # Unique dataset for visualization purposes\n",
    "    \n",
    "    ds_GO = pd.DataFrame(data=[train_GO, test_GO]).T.reset_index(drop=False)\n",
    "    ds_GO.columns = ['Emotion', 'Train','Test']\n",
    "    ds_GO = ds_GO.sort_values('Train',ascending=False)\n",
    "    ds_GO = ds_GO.melt(id_vars='Emotion', var_name='Dataset', value_vars=['Train','Test'],\n",
    "                      value_name='Percentage')\n",
    "    \n",
    "    # Display dataset\n",
    "    \n",
    "    display(ds_GO.head(10))\n",
    "    \n",
    "    print(\"Graph Visualization\")\n",
    "    \n",
    "    plt.figure(figsize=(20,15))\n",
    "    sns.barplot(x='Percentage', y='Emotion', data=ds_GO, orient='h', hue='Dataset')\n",
    "    plt.title('Percentage of samples per emotion in train and test datasets', fontweight='bold', fontsize=20)\n",
    "    plt.xlabel('Percentage of all samples', fontweight='bold', fontsize=16)\n",
    "    plt.ylabel('Emotions', fontweight='bold', fontsize= 16)\n",
    "    plt.show()\n",
    "represent_train_test_balance(df_train, df_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8818d771",
   "metadata": {},
   "source": [
    "# Tokenization / Encoding / Method Structuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea478700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, DistilBertForSequenceClassification, BertForSequenceClassification, RobertaForSequenceClassification, XLNetForSequenceClassification\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers.models.xlnet.modeling_xlnet import XLNetForSequenceClassificationOutput\n",
    "from torch import nn\n",
    "import random\n",
    "import torch\n",
    "import platform\n",
    "import sys\n",
    "import sklearn as sk\n",
    "from typing import Optional, Union, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b3799bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "43432890",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoEmotionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3edbe207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(df_train, df_test, tokenizer): \n",
    "  # Encodings\n",
    "\n",
    "  train_encodings = tokenizer(df_train[\"clean_text\"].values.tolist(), truncation=True)\n",
    "  test_encodings = tokenizer(df_test[\"clean_text\"].values.tolist(), truncation=True)\n",
    "\n",
    "  # labels / output\n",
    "  train_emotions = df_train[\"labels\"].values.tolist()\n",
    "  test_emotions = df_test[\"labels\"].values.tolist()\n",
    "\n",
    "  train_dataset = GoEmotionDataset(train_encodings, train_emotions)\n",
    "  test_dataset = GoEmotionDataset(test_encodings, test_emotions)\n",
    "  return train_dataset, test_dataset\n",
    "  \n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    y_pred = torch.from_numpy(logits)\n",
    "    y_true = torch.from_numpy(labels)\n",
    "    y_pred = y_pred.sigmoid()\n",
    "    y_pred = y_pred>0.5\n",
    "    y_true = y_true.bool()\n",
    "    acc = (y_pred==y_true).float().mean().item()\n",
    "\n",
    "    return {       \n",
    "      'Accuracy': acc\n",
    "    }\n",
    "    \n",
    "def set_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic=False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "def device_to_use():\n",
    "    has_gpu = torch.cuda.is_available()\n",
    "    has_mps = getattr(torch,'has_mps',False)\n",
    "    device = \"mps\" if getattr(torch,'has_mps',False) \\\n",
    "        else \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    print(f\"Python Platform: {platform.platform()}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    print()\n",
    "    print(f\"Python {sys.version}\")\n",
    "    print(f\"Pandas {pd.__version__}\")\n",
    "    print(f\"Scikit-Learn {sk.__version__}\")\n",
    "    print(\"GPU is\", \"available\" if has_gpu else \"NOT AVAILABLE\")\n",
    "    print(\"MPS (Apple Metal) is\", \"AVAILABLE\" if has_mps else \"NOT AVAILABLE\")\n",
    "    print(f\"Target device is {device}\")\n",
    "    return device\n",
    "\n",
    "def model_train(train_dataset, test_dataset, model, tokenizer, NUM_EPOCHS = 10,batch_size = 16, adam_epsilon_arg = 1e-8, learning_rate_arg = 2e-5, use_mps_device_arg = False, model_name = \"default\"):\n",
    "  training_args = TrainingArguments( \n",
    "    output_dir= OUTPUT_DIR+\"/\"+model_name,    \n",
    "    adam_epsilon = adam_epsilon_arg,\n",
    "    learning_rate = learning_rate_arg,\n",
    "    use_mps_device = use_mps_device_arg, # Mac Sylicon GPU\n",
    "    per_device_train_batch_size = batch_size, \n",
    "    per_device_eval_batch_size = batch_size*4,\n",
    "    gradient_accumulation_steps = 2, # scale batch size without needing more memory\n",
    "    num_train_epochs= NUM_EPOCHS,\n",
    "    do_eval = True,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    save_strategy = 'epoch',\n",
    "    load_best_model_at_end = True, # this allows to automatically get the best model at the end based on whatever metric we want\n",
    "    metric_for_best_model = 'Accuracy',\n",
    "    greater_is_better = True,\n",
    "    weight_decay=0.01,\n",
    "    seed = 25,\n",
    "    report_to=\"none\"\n",
    "  )\n",
    "  set_seed(training_args.seed)\n",
    "  trainer = Trainer(\n",
    "      model = model,\n",
    "      args = training_args,\n",
    "      train_dataset = train_dataset,\n",
    "      eval_dataset=test_dataset,\n",
    "      compute_metrics=compute_metrics,\n",
    "      tokenizer=tokenizer\n",
    "  )\n",
    "  return training_args, trainer\n",
    "                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "093bf286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes to Each Model\n",
    "\n",
    "class DistilBertForMultilabelSequenceClassification(DistilBertForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "      super().__init__(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[SequenceClassifierOutput, Tuple[torch.Tensor, ...]]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        distilbert_output = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n",
    "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
    "        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n",
    "        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)\n",
    "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
    "        logits = self.classifier(pooled_output)  # (bs, num_labels)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), \n",
    "                            labels.float().view(-1, self.num_labels))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + distilbert_output[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=distilbert_output.hidden_states,\n",
    "            attentions=distilbert_output.attentions)\n",
    "\n",
    "class BertForMultilabelSequenceClassification(BertForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "      super().__init__(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        bert_output = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = bert_output[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), \n",
    "                            labels.float().view(-1, self.num_labels))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + bert_output[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=bert_output.hidden_states,\n",
    "            attentions=bert_output.attentions)\n",
    "\n",
    "class RoBertaForMultilabelSequenceClassification(RobertaForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "      super().__init__(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        roberta_output = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = roberta_output[0]\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), \n",
    "                            labels.float().view(-1, self.num_labels))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + roberta_output[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=roberta_output.hidden_states,\n",
    "            attentions=roberta_output.attentions)\n",
    "\n",
    "class XLNetForMultilabelSequenceClassification(XLNetForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "      super().__init__(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        mems: Optional[torch.Tensor] = None,\n",
    "        perm_mask: Optional[torch.Tensor] = None,\n",
    "        target_mapping: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        input_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        use_mems: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs,  # delete when `use_cache` is removed in XLNetModel\n",
    "    ) -> Union[Tuple, XLNetForSequenceClassificationOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        xlnet_output = self.transformer(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            mems=mems,\n",
    "            perm_mask=perm_mask,\n",
    "            target_mapping=target_mapping,\n",
    "            token_type_ids=token_type_ids,\n",
    "            input_mask=input_mask,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_mems=use_mems,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            **kwargs)\n",
    "        output = xlnet_output[0]\n",
    "        output = self.sequence_summary(output)\n",
    "        logits = self.logits_proj(output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), \n",
    "                            labels.float().view(-1, self.num_labels))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + xlnet_output[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=xlnet_output.hidden_states,\n",
    "            attentions=xlnet_output.attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5de0b7",
   "metadata": {},
   "source": [
    "# Pre-Trained Model - DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06e24f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: Windows-10-10.0.22621-SP0\n",
      "PyTorch Version: 2.0.0\n",
      "\n",
      "Python 3.10.9 | packaged by Anaconda, Inc. | (main, Mar  8 2023, 10:42:25) [MSC v.1916 64 bit (AMD64)]\n",
      "Pandas 1.5.3\n",
      "Scikit-Learn 1.2.2\n",
      "GPU is available\n",
      "MPS (Apple Metal) is NOT AVAILABLE\n",
      "Target device is gpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForMultilabelSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForMultilabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForMultilabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForMultilabelSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": [\n",
      "    {\n",
      "      \"0\": \"admiration\",\n",
      "      \"1\": \"amusement\",\n",
      "      \"10\": \"disapproval\",\n",
      "      \"11\": \"disgust\",\n",
      "      \"12\": \"embarrassment\",\n",
      "      \"13\": \"excitement\",\n",
      "      \"14\": \"fear\",\n",
      "      \"15\": \"gratitude\",\n",
      "      \"16\": \"grief\",\n",
      "      \"17\": \"joy\",\n",
      "      \"18\": \"love\",\n",
      "      \"19\": \"nervousness\",\n",
      "      \"2\": \"anger\",\n",
      "      \"20\": \"optimism\",\n",
      "      \"21\": \"pride\",\n",
      "      \"22\": \"realization\",\n",
      "      \"23\": \"relief\",\n",
      "      \"24\": \"remorse\",\n",
      "      \"25\": \"sadness\",\n",
      "      \"26\": \"surprise\",\n",
      "      \"27\": \"neutral\",\n",
      "      \"3\": \"annoyance\",\n",
      "      \"4\": \"approval\",\n",
      "      \"5\": \"caring\",\n",
      "      \"6\": \"confusion\",\n",
      "      \"7\": \"curiosity\",\n",
      "      \"8\": \"desire\",\n",
      "      \"9\": \"disappointment\"\n",
      "    }\n",
      "  ],\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"admiration\": \"0\",\n",
      "    \"amusement\": \"1\",\n",
      "    \"anger\": \"2\",\n",
      "    \"annoyance\": \"3\",\n",
      "    \"approval\": \"4\",\n",
      "    \"caring\": \"5\",\n",
      "    \"confusion\": \"6\",\n",
      "    \"curiosity\": \"7\",\n",
      "    \"desire\": \"8\",\n",
      "    \"disappointment\": \"9\",\n",
      "    \"disapproval\": \"10\",\n",
      "    \"disgust\": \"11\",\n",
      "    \"embarrassment\": \"12\",\n",
      "    \"excitement\": \"13\",\n",
      "    \"fear\": \"14\",\n",
      "    \"gratitude\": \"15\",\n",
      "    \"grief\": \"16\",\n",
      "    \"joy\": \"17\",\n",
      "    \"love\": \"18\",\n",
      "    \"nervousness\": \"19\",\n",
      "    \"neutral\": \"27\",\n",
      "    \"optimism\": \"20\",\n",
      "    \"pride\": \"21\",\n",
      "    \"realization\": \"22\",\n",
      "    \"relief\": \"23\",\n",
      "    \"remorse\": \"24\",\n",
      "    \"sadness\": \"25\",\n",
      "    \"surprise\": \"26\"\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.27.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path_or_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)\n",
    "num_labels=len(emotions)\n",
    "device = device_to_use()\n",
    "if device == 'gpu': device = 'cuda'\n",
    "model = DistilBertForMultilabelSequenceClassification.from_pretrained(model_path_or_name, num_labels=num_labels).to(device)\n",
    "model = model_config_ids(model, id2label, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a93aa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test(df_train, df_test, tokenizer)\n",
    "training_args, trainer = model_train(train_dataset, test_dataset, model, tokenizer, NUM_EPOCHS = 3,batch_size = 16, adam_epsilon_arg = 1e-8, learning_rate_arg = 2e-5, use_mps_device_arg = False, model_name = \"distilbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ed43f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4936bfb83a0746158cb53bc63a5b76f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.697327733039856,\n",
       " 'eval_Accuracy': 0.46574118733406067,\n",
       " 'eval_runtime': 22.9739,\n",
       " 'eval_samples_per_second': 1850.969,\n",
       " 'eval_steps_per_second': 28.946}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c506dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f47b97fe4134f12a30949e7893a5d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15876 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2038, 'learning_rate': 1.9370118417737468e-05, 'epoch': 0.09}\n",
      "{'loss': 0.1395, 'learning_rate': 1.874023683547493e-05, 'epoch': 0.19}\n",
      "{'loss': 0.1276, 'learning_rate': 1.8110355253212397e-05, 'epoch': 0.28}\n",
      "{'loss': 0.1216, 'learning_rate': 1.7480473670949864e-05, 'epoch': 0.38}\n",
      "{'loss': 0.1197, 'learning_rate': 1.6850592088687327e-05, 'epoch': 0.47}\n",
      "{'loss': 0.1191, 'learning_rate': 1.6220710506424793e-05, 'epoch': 0.57}\n",
      "{'loss': 0.1168, 'learning_rate': 1.559082892416226e-05, 'epoch': 0.66}\n",
      "{'loss': 0.1168, 'learning_rate': 1.4960947341899723e-05, 'epoch': 0.76}\n",
      "{'loss': 0.117, 'learning_rate': 1.433106575963719e-05, 'epoch': 0.85}\n",
      "{'loss': 0.1158, 'learning_rate': 1.3701184177374655e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23e767cde0f47d9a8dddbaae9997ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/655 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11368750780820847, 'eval_Accuracy': 0.9615535140037537, 'eval_runtime': 23.4143, 'eval_samples_per_second': 1788.137, 'eval_steps_per_second': 27.974, 'epoch': 1.0}\n",
      "{'loss': 0.1139, 'learning_rate': 1.3071302595112122e-05, 'epoch': 1.04}\n",
      "{'loss': 0.1115, 'learning_rate': 1.2441421012849586e-05, 'epoch': 1.13}\n",
      "{'loss': 0.111, 'learning_rate': 1.1811539430587051e-05, 'epoch': 1.23}\n",
      "{'loss': 0.1112, 'learning_rate': 1.1181657848324516e-05, 'epoch': 1.32}\n",
      "{'loss': 0.1116, 'learning_rate': 1.0551776266061982e-05, 'epoch': 1.42}\n",
      "{'loss': 0.111, 'learning_rate': 9.921894683799447e-06, 'epoch': 1.51}\n",
      "{'loss': 0.1113, 'learning_rate': 9.292013101536911e-06, 'epoch': 1.61}\n",
      "{'loss': 0.111, 'learning_rate': 8.662131519274378e-06, 'epoch': 1.7}\n",
      "{'loss': 0.1101, 'learning_rate': 8.032249937011843e-06, 'epoch': 1.79}\n",
      "{'loss': 0.111, 'learning_rate': 7.402368354749307e-06, 'epoch': 1.89}\n",
      "{'loss': 0.1094, 'learning_rate': 6.772486772486773e-06, 'epoch': 1.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd6b3cb8cd743289edbb9424cffdf65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/655 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11142463237047195, 'eval_Accuracy': 0.9618861675262451, 'eval_runtime': 22.0921, 'eval_samples_per_second': 1895.156, 'eval_steps_per_second': 29.649, 'epoch': 2.0}\n",
      "{'loss': 0.1074, 'learning_rate': 6.142605190224238e-06, 'epoch': 2.08}\n",
      "{'loss': 0.1066, 'learning_rate': 5.512723607961704e-06, 'epoch': 2.17}\n",
      "{'loss': 0.1066, 'learning_rate': 4.8828420256991685e-06, 'epoch': 2.27}\n",
      "{'loss': 0.1059, 'learning_rate': 4.252960443436635e-06, 'epoch': 2.36}\n",
      "{'loss': 0.1063, 'learning_rate': 3.6230788611740995e-06, 'epoch': 2.46}\n",
      "{'loss': 0.1065, 'learning_rate': 2.993197278911565e-06, 'epoch': 2.55}\n",
      "{'loss': 0.1058, 'learning_rate': 2.36331569664903e-06, 'epoch': 2.65}\n",
      "{'loss': 0.1065, 'learning_rate': 1.7334341143864955e-06, 'epoch': 2.74}\n",
      "{'loss': 0.1054, 'learning_rate': 1.1035525321239608e-06, 'epoch': 2.83}\n",
      "{'loss': 0.1065, 'learning_rate': 4.736709498614261e-07, 'epoch': 2.93}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f4b34c879c4204bc34b94b8e21f7e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/655 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11119987070560455, 'eval_Accuracy': 0.9619091749191284, 'eval_runtime': 22.9527, 'eval_samples_per_second': 1824.103, 'eval_steps_per_second': 28.537, 'epoch': 3.0}\n",
      "{'train_runtime': 1237.3091, 'train_samples_per_second': 410.626, 'train_steps_per_second': 12.831, 'train_loss': 0.11541589485604807, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15876, training_loss=0.11541589485604807, metrics={'train_runtime': 1237.3091, 'train_samples_per_second': 410.626, 'train_steps_per_second': 12.831, 'train_loss': 0.11541589485604807, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fbac40d",
   "metadata": {},
   "source": [
    "# Pre-Trained Model - BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5b7cabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: Windows-10-10.0.22621-SP0\n",
      "PyTorch Version: 2.0.0\n",
      "\n",
      "Python 3.10.9 | packaged by Anaconda, Inc. | (main, Mar  8 2023, 10:42:25) [MSC v.1916 64 bit (AMD64)]\n",
      "Pandas 1.5.3\n",
      "Scikit-Learn 1.2.2\n",
      "GPU is available\n",
      "MPS (Apple Metal) is NOT AVAILABLE\n",
      "Target device is gpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultilabelSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForMultilabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultilabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultilabelSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": [\n",
      "    {\n",
      "      \"0\": \"admiration\",\n",
      "      \"1\": \"amusement\",\n",
      "      \"10\": \"disapproval\",\n",
      "      \"11\": \"disgust\",\n",
      "      \"12\": \"embarrassment\",\n",
      "      \"13\": \"excitement\",\n",
      "      \"14\": \"fear\",\n",
      "      \"15\": \"gratitude\",\n",
      "      \"16\": \"grief\",\n",
      "      \"17\": \"joy\",\n",
      "      \"18\": \"love\",\n",
      "      \"19\": \"nervousness\",\n",
      "      \"2\": \"anger\",\n",
      "      \"20\": \"optimism\",\n",
      "      \"21\": \"pride\",\n",
      "      \"22\": \"realization\",\n",
      "      \"23\": \"relief\",\n",
      "      \"24\": \"remorse\",\n",
      "      \"25\": \"sadness\",\n",
      "      \"26\": \"surprise\",\n",
      "      \"27\": \"neutral\",\n",
      "      \"3\": \"annoyance\",\n",
      "      \"4\": \"approval\",\n",
      "      \"5\": \"caring\",\n",
      "      \"6\": \"confusion\",\n",
      "      \"7\": \"curiosity\",\n",
      "      \"8\": \"desire\",\n",
      "      \"9\": \"disappointment\"\n",
      "    }\n",
      "  ],\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"admiration\": \"0\",\n",
      "    \"amusement\": \"1\",\n",
      "    \"anger\": \"2\",\n",
      "    \"annoyance\": \"3\",\n",
      "    \"approval\": \"4\",\n",
      "    \"caring\": \"5\",\n",
      "    \"confusion\": \"6\",\n",
      "    \"curiosity\": \"7\",\n",
      "    \"desire\": \"8\",\n",
      "    \"disappointment\": \"9\",\n",
      "    \"disapproval\": \"10\",\n",
      "    \"disgust\": \"11\",\n",
      "    \"embarrassment\": \"12\",\n",
      "    \"excitement\": \"13\",\n",
      "    \"fear\": \"14\",\n",
      "    \"gratitude\": \"15\",\n",
      "    \"grief\": \"16\",\n",
      "    \"joy\": \"17\",\n",
      "    \"love\": \"18\",\n",
      "    \"nervousness\": \"19\",\n",
      "    \"neutral\": \"27\",\n",
      "    \"optimism\": \"20\",\n",
      "    \"pride\": \"21\",\n",
      "    \"realization\": \"22\",\n",
      "    \"relief\": \"23\",\n",
      "    \"remorse\": \"24\",\n",
      "    \"sadness\": \"25\",\n",
      "    \"surprise\": \"26\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path_or_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)\n",
    "num_labels=len(emotions)\n",
    "device = device_to_use()\n",
    "if device == 'gpu': device = 'cuda'\n",
    "model = BertForMultilabelSequenceClassification.from_pretrained(model_path_or_name, num_labels=num_labels).to(device)\n",
    "model = model_config_ids(model, id2label, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c934fc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test(df_train, df_test, tokenizer)\n",
    "training_args, trainer = model_train(train_dataset, test_dataset, model, tokenizer, NUM_EPOCHS = 3,batch_size = 16, adam_epsilon_arg = 1e-8, learning_rate_arg = 2e-5, use_mps_device_arg = False, model_name = \"bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b9a9d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "766f15ab065d4c4980cc444642fb676c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15816 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1975, 'learning_rate': 1.9367728882144666e-05, 'epoch': 0.09}\n",
      "{'loss': 0.1422, 'learning_rate': 1.8735457764289327e-05, 'epoch': 0.19}\n",
      "{'loss': 0.1312, 'learning_rate': 1.810318664643399e-05, 'epoch': 0.28}\n",
      "{'loss': 0.1254, 'learning_rate': 1.7470915528578656e-05, 'epoch': 0.38}\n",
      "{'loss': 0.1208, 'learning_rate': 1.683864441072332e-05, 'epoch': 0.47}\n",
      "{'loss': 0.1187, 'learning_rate': 1.620637329286798e-05, 'epoch': 0.57}\n",
      "{'loss': 0.1185, 'learning_rate': 1.5574102175012646e-05, 'epoch': 0.66}\n",
      "{'loss': 0.1166, 'learning_rate': 1.494183105715731e-05, 'epoch': 0.76}\n",
      "{'loss': 0.1157, 'learning_rate': 1.4309559939301973e-05, 'epoch': 0.85}\n",
      "{'loss': 0.1146, 'learning_rate': 1.3677288821446637e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9eb907068d0409aa38475dbdc6f7b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11361443996429443, 'eval_Accuracy': 0.9616317749023438, 'eval_runtime': 43.0879, 'eval_samples_per_second': 986.912, 'eval_steps_per_second': 15.434, 'epoch': 1.0}\n",
      "{'loss': 0.1129, 'learning_rate': 1.30450177035913e-05, 'epoch': 1.04}\n",
      "{'loss': 0.111, 'learning_rate': 1.2412746585735965e-05, 'epoch': 1.14}\n",
      "{'loss': 0.1118, 'learning_rate': 1.1780475467880627e-05, 'epoch': 1.23}\n",
      "{'loss': 0.1106, 'learning_rate': 1.1148204350025292e-05, 'epoch': 1.33}\n",
      "{'loss': 0.1096, 'learning_rate': 1.0515933232169954e-05, 'epoch': 1.42}\n",
      "{'loss': 0.1093, 'learning_rate': 9.883662114314619e-06, 'epoch': 1.52}\n",
      "{'loss': 0.109, 'learning_rate': 9.251390996459283e-06, 'epoch': 1.61}\n",
      "{'loss': 0.1102, 'learning_rate': 8.619119878603946e-06, 'epoch': 1.71}\n",
      "{'loss': 0.1082, 'learning_rate': 7.98684876074861e-06, 'epoch': 1.8}\n",
      "{'loss': 0.1093, 'learning_rate': 7.354577642893273e-06, 'epoch': 1.9}\n",
      "{'loss': 0.1092, 'learning_rate': 6.722306525037937e-06, 'epoch': 1.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "246770b07b1e4aadbe37eeeedbab0616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11117535829544067, 'eval_Accuracy': 0.9620071649551392, 'eval_runtime': 41.9832, 'eval_samples_per_second': 1012.881, 'eval_steps_per_second': 15.84, 'epoch': 2.0}\n",
      "{'loss': 0.1044, 'learning_rate': 6.0900354071826e-06, 'epoch': 2.09}\n",
      "{'loss': 0.1047, 'learning_rate': 5.457764289327264e-06, 'epoch': 2.18}\n",
      "{'loss': 0.1041, 'learning_rate': 4.8254931714719275e-06, 'epoch': 2.28}\n",
      "{'loss': 0.1054, 'learning_rate': 4.193222053616591e-06, 'epoch': 2.37}\n",
      "{'loss': 0.1061, 'learning_rate': 3.5609509357612546e-06, 'epoch': 2.47}\n",
      "{'loss': 0.1049, 'learning_rate': 2.9286798179059182e-06, 'epoch': 2.56}\n",
      "{'loss': 0.1038, 'learning_rate': 2.296408700050582e-06, 'epoch': 2.66}\n",
      "{'loss': 0.1041, 'learning_rate': 1.6641375821952454e-06, 'epoch': 2.75}\n",
      "{'loss': 0.1032, 'learning_rate': 1.031866464339909e-06, 'epoch': 2.85}\n",
      "{'loss': 0.1046, 'learning_rate': 3.9959534648457263e-07, 'epoch': 2.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23478ed76f5d48729818432eabf4ab7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11119233071804047, 'eval_Accuracy': 0.9620088338851929, 'eval_runtime': 42.837, 'eval_samples_per_second': 992.693, 'eval_steps_per_second': 15.524, 'epoch': 3.0}\n",
      "{'train_runtime': 2243.6504, 'train_samples_per_second': 225.571, 'train_steps_per_second': 7.049, 'train_loss': 0.11455539506556231, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15816, training_loss=0.11455539506556231, metrics={'train_runtime': 2243.6504, 'train_samples_per_second': 225.571, 'train_steps_per_second': 7.049, 'train_loss': 0.11455539506556231, 'epoch': 3.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deb8a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1c51500",
   "metadata": {},
   "source": [
    "# Pre-Trained Model - RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "640c5d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: Windows-10-10.0.22621-SP0\n",
      "PyTorch Version: 2.0.0\n",
      "\n",
      "Python 3.10.9 | packaged by Anaconda, Inc. | (main, Mar  8 2023, 10:42:25) [MSC v.1916 64 bit (AMD64)]\n",
      "Pandas 1.5.3\n",
      "Scikit-Learn 1.2.2\n",
      "GPU is available\n",
      "MPS (Apple Metal) is NOT AVAILABLE\n",
      "Target device is gpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RoBertaForMultilabelSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RoBertaForMultilabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RoBertaForMultilabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RoBertaForMultilabelSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": [\n",
      "    {\n",
      "      \"0\": \"admiration\",\n",
      "      \"1\": \"amusement\",\n",
      "      \"10\": \"disapproval\",\n",
      "      \"11\": \"disgust\",\n",
      "      \"12\": \"embarrassment\",\n",
      "      \"13\": \"excitement\",\n",
      "      \"14\": \"fear\",\n",
      "      \"15\": \"gratitude\",\n",
      "      \"16\": \"grief\",\n",
      "      \"17\": \"joy\",\n",
      "      \"18\": \"love\",\n",
      "      \"19\": \"nervousness\",\n",
      "      \"2\": \"anger\",\n",
      "      \"20\": \"optimism\",\n",
      "      \"21\": \"pride\",\n",
      "      \"22\": \"realization\",\n",
      "      \"23\": \"relief\",\n",
      "      \"24\": \"remorse\",\n",
      "      \"25\": \"sadness\",\n",
      "      \"26\": \"surprise\",\n",
      "      \"27\": \"neutral\",\n",
      "      \"3\": \"annoyance\",\n",
      "      \"4\": \"approval\",\n",
      "      \"5\": \"caring\",\n",
      "      \"6\": \"confusion\",\n",
      "      \"7\": \"curiosity\",\n",
      "      \"8\": \"desire\",\n",
      "      \"9\": \"disappointment\"\n",
      "    }\n",
      "  ],\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"admiration\": \"0\",\n",
      "    \"amusement\": \"1\",\n",
      "    \"anger\": \"2\",\n",
      "    \"annoyance\": \"3\",\n",
      "    \"approval\": \"4\",\n",
      "    \"caring\": \"5\",\n",
      "    \"confusion\": \"6\",\n",
      "    \"curiosity\": \"7\",\n",
      "    \"desire\": \"8\",\n",
      "    \"disappointment\": \"9\",\n",
      "    \"disapproval\": \"10\",\n",
      "    \"disgust\": \"11\",\n",
      "    \"embarrassment\": \"12\",\n",
      "    \"excitement\": \"13\",\n",
      "    \"fear\": \"14\",\n",
      "    \"gratitude\": \"15\",\n",
      "    \"grief\": \"16\",\n",
      "    \"joy\": \"17\",\n",
      "    \"love\": \"18\",\n",
      "    \"nervousness\": \"19\",\n",
      "    \"neutral\": \"27\",\n",
      "    \"optimism\": \"20\",\n",
      "    \"pride\": \"21\",\n",
      "    \"realization\": \"22\",\n",
      "    \"relief\": \"23\",\n",
      "    \"remorse\": \"24\",\n",
      "    \"sadness\": \"25\",\n",
      "    \"surprise\": \"26\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path_or_name = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)\n",
    "num_labels=len(emotions)\n",
    "device = device_to_use()\n",
    "if device == 'gpu': device = 'cuda'\n",
    "model = RoBertaForMultilabelSequenceClassification.from_pretrained(model_path_or_name, num_labels=num_labels).to(device)\n",
    "model = model_config_ids(model, id2label, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7e8c30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test(df_train, df_test, tokenizer)\n",
    "training_args, trainer = model_train(train_dataset, test_dataset, model, tokenizer, NUM_EPOCHS = 3,batch_size = 16, adam_epsilon_arg = 1e-8, learning_rate_arg = 2e-5, use_mps_device_arg = False, model_name = \"roberta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3fd5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "881319ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b99b5f6a1a48b997d23666bd0d503b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15816 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1841, 'learning_rate': 1.9367728882144666e-05, 'epoch': 0.09}\n",
      "{'loss': 0.1384, 'learning_rate': 1.8735457764289327e-05, 'epoch': 0.19}\n",
      "{'loss': 0.1264, 'learning_rate': 1.810318664643399e-05, 'epoch': 0.28}\n",
      "{'loss': 0.1233, 'learning_rate': 1.7470915528578656e-05, 'epoch': 0.38}\n",
      "{'loss': 0.1208, 'learning_rate': 1.683864441072332e-05, 'epoch': 0.47}\n",
      "{'loss': 0.1185, 'learning_rate': 1.620637329286798e-05, 'epoch': 0.57}\n",
      "{'loss': 0.118, 'learning_rate': 1.5574102175012646e-05, 'epoch': 0.66}\n",
      "{'loss': 0.1164, 'learning_rate': 1.494183105715731e-05, 'epoch': 0.76}\n",
      "{'loss': 0.1164, 'learning_rate': 1.4309559939301973e-05, 'epoch': 0.85}\n",
      "{'loss': 0.1151, 'learning_rate': 1.3677288821446637e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c7ff7b73e1d48508fe6753d9dc659ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1133279800415039, 'eval_Accuracy': 0.9615587592124939, 'eval_runtime': 39.2707, 'eval_samples_per_second': 1082.867, 'eval_steps_per_second': 16.934, 'epoch': 1.0}\n",
      "{'loss': 0.1125, 'learning_rate': 1.30450177035913e-05, 'epoch': 1.04}\n",
      "{'loss': 0.1131, 'learning_rate': 1.2412746585735965e-05, 'epoch': 1.14}\n",
      "{'loss': 0.1107, 'learning_rate': 1.1780475467880627e-05, 'epoch': 1.23}\n",
      "{'loss': 0.1112, 'learning_rate': 1.1148204350025292e-05, 'epoch': 1.33}\n",
      "{'loss': 0.111, 'learning_rate': 1.0515933232169954e-05, 'epoch': 1.42}\n",
      "{'loss': 0.1113, 'learning_rate': 9.883662114314619e-06, 'epoch': 1.52}\n",
      "{'loss': 0.1105, 'learning_rate': 9.251390996459283e-06, 'epoch': 1.61}\n",
      "{'loss': 0.1109, 'learning_rate': 8.619119878603946e-06, 'epoch': 1.71}\n",
      "{'loss': 0.1107, 'learning_rate': 7.98684876074861e-06, 'epoch': 1.8}\n",
      "{'loss': 0.1107, 'learning_rate': 7.354577642893273e-06, 'epoch': 1.9}\n",
      "{'loss': 0.1104, 'learning_rate': 6.722306525037937e-06, 'epoch': 1.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f684c2d4dd7b4d8397ca75b61aed59ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11133845895528793, 'eval_Accuracy': 0.9617611765861511, 'eval_runtime': 41.1991, 'eval_samples_per_second': 1032.182, 'eval_steps_per_second': 16.141, 'epoch': 2.0}\n",
      "{'loss': 0.1071, 'learning_rate': 6.0900354071826e-06, 'epoch': 2.09}\n",
      "{'loss': 0.1074, 'learning_rate': 5.457764289327264e-06, 'epoch': 2.18}\n",
      "{'loss': 0.1058, 'learning_rate': 4.8254931714719275e-06, 'epoch': 2.28}\n",
      "{'loss': 0.1059, 'learning_rate': 4.193222053616591e-06, 'epoch': 2.37}\n",
      "{'loss': 0.1079, 'learning_rate': 3.5609509357612546e-06, 'epoch': 2.47}\n",
      "{'loss': 0.106, 'learning_rate': 2.9286798179059182e-06, 'epoch': 2.56}\n",
      "{'loss': 0.1065, 'learning_rate': 2.296408700050582e-06, 'epoch': 2.66}\n",
      "{'loss': 0.1063, 'learning_rate': 1.6641375821952454e-06, 'epoch': 2.75}\n",
      "{'loss': 0.1053, 'learning_rate': 1.031866464339909e-06, 'epoch': 2.85}\n",
      "{'loss': 0.1066, 'learning_rate': 3.9959534648457263e-07, 'epoch': 2.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20cb5f508b394062ad075c8404b9094c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11095210909843445, 'eval_Accuracy': 0.9616267681121826, 'eval_runtime': 41.1004, 'eval_samples_per_second': 1034.661, 'eval_steps_per_second': 16.18, 'epoch': 3.0}\n",
      "{'train_runtime': 2354.0718, 'train_samples_per_second': 214.989, 'train_steps_per_second': 6.719, 'train_loss': 0.114850394461694, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15816, training_loss=0.114850394461694, metrics={'train_runtime': 2354.0718, 'train_samples_per_second': 214.989, 'train_steps_per_second': 6.719, 'train_loss': 0.114850394461694, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "562ca376",
   "metadata": {},
   "source": [
    "# Pre-Trained Model - XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f30c042a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: Windows-10-10.0.22621-SP0\n",
      "PyTorch Version: 2.0.0\n",
      "\n",
      "Python 3.10.9 | packaged by Anaconda, Inc. | (main, Mar  8 2023, 10:42:25) [MSC v.1916 64 bit (AMD64)]\n",
      "Pandas 1.5.3\n",
      "Scikit-Learn 1.2.2\n",
      "GPU is available\n",
      "MPS (Apple Metal) is NOT AVAILABLE\n",
      "Target device is gpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultilabelSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForMultilabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultilabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultilabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLNetConfig {\n",
      "  \"_name_or_path\": \"xlnet-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"XLNetLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_type\": \"bi\",\n",
      "  \"bi_data\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"clamp_len\": -1,\n",
      "  \"d_head\": 64,\n",
      "  \"d_inner\": 3072,\n",
      "  \"d_model\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"end_n_top\": 5,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ff_activation\": \"gelu\",\n",
      "  \"id2label\": [\n",
      "    {\n",
      "      \"0\": \"admiration\",\n",
      "      \"1\": \"amusement\",\n",
      "      \"10\": \"disapproval\",\n",
      "      \"11\": \"disgust\",\n",
      "      \"12\": \"embarrassment\",\n",
      "      \"13\": \"excitement\",\n",
      "      \"14\": \"fear\",\n",
      "      \"15\": \"gratitude\",\n",
      "      \"16\": \"grief\",\n",
      "      \"17\": \"joy\",\n",
      "      \"18\": \"love\",\n",
      "      \"19\": \"nervousness\",\n",
      "      \"2\": \"anger\",\n",
      "      \"20\": \"optimism\",\n",
      "      \"21\": \"pride\",\n",
      "      \"22\": \"realization\",\n",
      "      \"23\": \"relief\",\n",
      "      \"24\": \"remorse\",\n",
      "      \"25\": \"sadness\",\n",
      "      \"26\": \"surprise\",\n",
      "      \"27\": \"neutral\",\n",
      "      \"3\": \"annoyance\",\n",
      "      \"4\": \"approval\",\n",
      "      \"5\": \"caring\",\n",
      "      \"6\": \"confusion\",\n",
      "      \"7\": \"curiosity\",\n",
      "      \"8\": \"desire\",\n",
      "      \"9\": \"disappointment\"\n",
      "    }\n",
      "  ],\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"admiration\": \"0\",\n",
      "    \"amusement\": \"1\",\n",
      "    \"anger\": \"2\",\n",
      "    \"annoyance\": \"3\",\n",
      "    \"approval\": \"4\",\n",
      "    \"caring\": \"5\",\n",
      "    \"confusion\": \"6\",\n",
      "    \"curiosity\": \"7\",\n",
      "    \"desire\": \"8\",\n",
      "    \"disappointment\": \"9\",\n",
      "    \"disapproval\": \"10\",\n",
      "    \"disgust\": \"11\",\n",
      "    \"embarrassment\": \"12\",\n",
      "    \"excitement\": \"13\",\n",
      "    \"fear\": \"14\",\n",
      "    \"gratitude\": \"15\",\n",
      "    \"grief\": \"16\",\n",
      "    \"joy\": \"17\",\n",
      "    \"love\": \"18\",\n",
      "    \"nervousness\": \"19\",\n",
      "    \"neutral\": \"27\",\n",
      "    \"optimism\": \"20\",\n",
      "    \"pride\": \"21\",\n",
      "    \"realization\": \"22\",\n",
      "    \"relief\": \"23\",\n",
      "    \"remorse\": \"24\",\n",
      "    \"sadness\": \"25\",\n",
      "    \"surprise\": \"26\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mem_len\": null,\n",
      "  \"model_type\": \"xlnet\",\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"pad_token_id\": 5,\n",
      "  \"reuse_len\": null,\n",
      "  \"same_length\": false,\n",
      "  \"start_n_top\": 5,\n",
      "  \"summary_activation\": \"tanh\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"last\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 250\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.27.2\",\n",
      "  \"untie_r\": true,\n",
      "  \"use_mems_eval\": true,\n",
      "  \"use_mems_train\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path_or_name = \"xlnet-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)\n",
    "num_labels=len(emotions)\n",
    "device = device_to_use()\n",
    "if device == 'gpu': device = 'cuda'\n",
    "model = XLNetForMultilabelSequenceClassification.from_pretrained(model_path_or_name, num_labels=num_labels).to(device)\n",
    "model = model_config_ids(model, id2label, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "905b8aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = train_test(df_train, df_test, tokenizer)\n",
    "training_args, trainer = model_train(train_dataset, test_dataset, model, tokenizer, NUM_EPOCHS = 3,batch_size = 8, adam_epsilon_arg = 1e-8, learning_rate_arg = 2e-5, use_mps_device_arg = False, model_name = \"xlnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6849aa6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e6df60e5714bc8abe8a7aed3993d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31710 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLNetTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1741, 'learning_rate': 1.968464206874803e-05, 'epoch': 0.05}\n",
      "{'loss': 0.1453, 'learning_rate': 1.936928413749606e-05, 'epoch': 0.09}\n",
      "{'loss': 0.1325, 'learning_rate': 1.9053926206244088e-05, 'epoch': 0.14}\n",
      "{'loss': 0.127, 'learning_rate': 1.8738568274992116e-05, 'epoch': 0.19}\n",
      "{'loss': 0.1238, 'learning_rate': 1.8423210343740147e-05, 'epoch': 0.24}\n",
      "{'loss': 0.1235, 'learning_rate': 1.8107852412488174e-05, 'epoch': 0.28}\n",
      "{'loss': 0.1212, 'learning_rate': 1.7792494481236205e-05, 'epoch': 0.33}\n",
      "{'loss': 0.1188, 'learning_rate': 1.7477136549984233e-05, 'epoch': 0.38}\n",
      "{'loss': 0.1196, 'learning_rate': 1.7161778618732264e-05, 'epoch': 0.43}\n",
      "{'loss': 0.1193, 'learning_rate': 1.6846420687480292e-05, 'epoch': 0.47}\n",
      "{'loss': 0.1187, 'learning_rate': 1.653106275622832e-05, 'epoch': 0.52}\n",
      "{'loss': 0.1172, 'learning_rate': 1.621570482497635e-05, 'epoch': 0.57}\n",
      "{'loss': 0.1178, 'learning_rate': 1.5900346893724378e-05, 'epoch': 0.61}\n",
      "{'loss': 0.1166, 'learning_rate': 1.5584988962472406e-05, 'epoch': 0.66}\n",
      "{'loss': 0.1176, 'learning_rate': 1.5269631031220437e-05, 'epoch': 0.71}\n",
      "{'loss': 0.1158, 'learning_rate': 1.4954273099968466e-05, 'epoch': 0.76}\n",
      "{'loss': 0.116, 'learning_rate': 1.4638915168716494e-05, 'epoch': 0.8}\n",
      "{'loss': 0.1154, 'learning_rate': 1.4323557237464523e-05, 'epoch': 0.85}\n",
      "{'loss': 0.1153, 'learning_rate': 1.4008199306212553e-05, 'epoch': 0.9}\n",
      "{'loss': 0.1153, 'learning_rate': 1.3692841374960582e-05, 'epoch': 0.95}\n",
      "{'loss': 0.1142, 'learning_rate': 1.337748344370861e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a6671e2a8ee4725afe0cae7b53e74c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11334612220525742, 'eval_Accuracy': 0.9615048170089722, 'eval_runtime': 58.1165, 'eval_samples_per_second': 724.372, 'eval_steps_per_second': 22.644, 'epoch': 1.0}\n",
      "{'loss': 0.1108, 'learning_rate': 1.306212551245664e-05, 'epoch': 1.04}\n",
      "{'loss': 0.11, 'learning_rate': 1.2746767581204668e-05, 'epoch': 1.09}\n",
      "{'loss': 0.1114, 'learning_rate': 1.2431409649952698e-05, 'epoch': 1.14}\n",
      "{'loss': 0.1102, 'learning_rate': 1.2116051718700725e-05, 'epoch': 1.18}\n",
      "{'loss': 0.1105, 'learning_rate': 1.1800693787448756e-05, 'epoch': 1.23}\n",
      "{'loss': 0.1092, 'learning_rate': 1.1485335856196784e-05, 'epoch': 1.28}\n",
      "{'loss': 0.1104, 'learning_rate': 1.1169977924944813e-05, 'epoch': 1.32}\n",
      "{'loss': 0.112, 'learning_rate': 1.0854619993692843e-05, 'epoch': 1.37}\n",
      "{'loss': 0.1102, 'learning_rate': 1.0539262062440872e-05, 'epoch': 1.42}\n",
      "{'loss': 0.111, 'learning_rate': 1.02239041311889e-05, 'epoch': 1.47}\n",
      "{'loss': 0.1102, 'learning_rate': 9.908546199936929e-06, 'epoch': 1.51}\n",
      "{'loss': 0.1097, 'learning_rate': 9.593188268684958e-06, 'epoch': 1.56}\n",
      "{'loss': 0.11, 'learning_rate': 9.277830337432988e-06, 'epoch': 1.61}\n",
      "{'loss': 0.1103, 'learning_rate': 8.962472406181017e-06, 'epoch': 1.66}\n",
      "{'loss': 0.1096, 'learning_rate': 8.647114474929045e-06, 'epoch': 1.7}\n",
      "{'loss': 0.109, 'learning_rate': 8.331756543677074e-06, 'epoch': 1.75}\n",
      "{'loss': 0.1103, 'learning_rate': 8.016398612425103e-06, 'epoch': 1.8}\n",
      "{'loss': 0.1102, 'learning_rate': 7.701040681173133e-06, 'epoch': 1.84}\n",
      "{'loss': 0.1088, 'learning_rate': 7.385682749921161e-06, 'epoch': 1.89}\n",
      "{'loss': 0.1095, 'learning_rate': 7.07032481866919e-06, 'epoch': 1.94}\n",
      "{'loss': 0.1077, 'learning_rate': 6.754966887417219e-06, 'epoch': 1.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a22eb0a09715482db512743b53b009c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11161205172538757, 'eval_Accuracy': 0.9617423415184021, 'eval_runtime': 58.8264, 'eval_samples_per_second': 715.631, 'eval_steps_per_second': 22.371, 'epoch': 2.0}\n",
      "{'loss': 0.1071, 'learning_rate': 6.4396089561652485e-06, 'epoch': 2.03}\n",
      "{'loss': 0.1049, 'learning_rate': 6.124251024913277e-06, 'epoch': 2.08}\n",
      "{'loss': 0.104, 'learning_rate': 5.808893093661306e-06, 'epoch': 2.13}\n",
      "{'loss': 0.1039, 'learning_rate': 5.493535162409335e-06, 'epoch': 2.18}\n",
      "{'loss': 0.106, 'learning_rate': 5.178177231157364e-06, 'epoch': 2.22}\n",
      "{'loss': 0.1057, 'learning_rate': 4.862819299905393e-06, 'epoch': 2.27}\n",
      "{'loss': 0.1053, 'learning_rate': 4.547461368653422e-06, 'epoch': 2.32}\n",
      "{'loss': 0.1046, 'learning_rate': 4.232103437401451e-06, 'epoch': 2.37}\n",
      "{'loss': 0.1043, 'learning_rate': 3.91674550614948e-06, 'epoch': 2.41}\n",
      "{'loss': 0.1045, 'learning_rate': 3.6013875748975093e-06, 'epoch': 2.46}\n",
      "{'loss': 0.1042, 'learning_rate': 3.286029643645538e-06, 'epoch': 2.51}\n",
      "{'loss': 0.1042, 'learning_rate': 2.9706717123935667e-06, 'epoch': 2.55}\n",
      "{'loss': 0.1064, 'learning_rate': 2.6553137811415957e-06, 'epoch': 2.6}\n",
      "{'loss': 0.1041, 'learning_rate': 2.339955849889625e-06, 'epoch': 2.65}\n",
      "{'loss': 0.1022, 'learning_rate': 2.024597918637654e-06, 'epoch': 2.7}\n",
      "{'loss': 0.103, 'learning_rate': 1.7092399873856829e-06, 'epoch': 2.74}\n",
      "{'loss': 0.1042, 'learning_rate': 1.3938820561337118e-06, 'epoch': 2.79}\n",
      "{'loss': 0.1043, 'learning_rate': 1.078524124881741e-06, 'epoch': 2.84}\n",
      "{'loss': 0.1041, 'learning_rate': 7.631661936297699e-07, 'epoch': 2.89}\n",
      "{'loss': 0.1043, 'learning_rate': 4.4780826237779886e-07, 'epoch': 2.93}\n",
      "{'loss': 0.1044, 'learning_rate': 1.3245033112582784e-07, 'epoch': 2.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c65576c0828448893faa2970ed7b599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11138853430747986, 'eval_Accuracy': 0.9615607857704163, 'eval_runtime': 58.8364, 'eval_samples_per_second': 715.509, 'eval_steps_per_second': 22.367, 'epoch': 3.0}\n",
      "{'train_runtime': 5901.5161, 'train_samples_per_second': 85.975, 'train_steps_per_second': 5.373, 'train_loss': 0.11250060360276033, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=31710, training_loss=0.11250060360276033, metrics={'train_runtime': 5901.5161, 'train_samples_per_second': 85.975, 'train_steps_per_second': 5.373, 'train_loss': 0.11250060360276033, 'epoch': 3.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b41285f4",
   "metadata": {},
   "source": [
    "# Transformer from Scratch - All you need!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71838691",
   "metadata": {},
   "source": [
    "Tokenizer - Still want to try some new ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f80cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from transformer import MultilabelSequenceClassificationTransformer\n",
    "# Imports\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, DistilBertForSequenceClassification, BertForSequenceClassification, RobertaForSequenceClassification, XLNetForSequenceClassification\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers.models.xlnet.modeling_xlnet import XLNetForSequenceClassificationOutput\n",
    "from torch import nn\n",
    "import random\n",
    "import torch\n",
    "import platform\n",
    "import sys\n",
    "import sklearn as sk\n",
    "from typing import Optional, Union, Tuple\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47e0e4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98a2cea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoEmotionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7cb923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_to_ix(df_train, df_test):\n",
    "    word_to_ix = {'<pad>': 0, '<unk>': 1}\n",
    "    for text in pd.concat([df_train[\"clean_text\"], df_test[\"clean_text\"]]):\n",
    "        for token in spacy_tokenizer(text):\n",
    "            if token not in word_to_ix:\n",
    "                word_to_ix[token] = len(word_to_ix)\n",
    "    return word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af7dd7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text, word_to_ix, max_length=128):\n",
    "    tokens = [t for t in spacy_tokenizer(text)]\n",
    "    input_ids = [word_to_ix.get(token, word_to_ix['<unk>']) for token in tokens][:max_length]\n",
    "    input_ids = input_ids + [0] * (max_length - len(input_ids))\n",
    "    attention_mask = [1 if token_id != 0 else 0 for token_id in input_ids]\n",
    "\n",
    "    return {\n",
    "        'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "        'attention_mask': torch.tensor(attention_mask, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c93436f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(df_train, df_test, tokenizer, word_to_ix):\n",
    "    # Encodings\n",
    "    train_encoded_texts = [encode_text(text, word_to_ix) for text in df_train[\"clean_text\"].values.tolist()]\n",
    "    test_encoded_texts = [encode_text(text, word_to_ix) for text in df_test[\"clean_text\"].values.tolist()]\n",
    "\n",
    "    train_encodings = {\n",
    "        'input_ids': [text_encoding['input_ids'] for text_encoding in train_encoded_texts],\n",
    "        'attention_mask': [text_encoding['attention_mask'] for text_encoding in train_encoded_texts]\n",
    "    }\n",
    "\n",
    "    test_encodings = {\n",
    "        'input_ids': [text_encoding['input_ids'] for text_encoding in test_encoded_texts],\n",
    "        'attention_mask': [text_encoding['attention_mask'] for text_encoding in test_encoded_texts]\n",
    "    }\n",
    "\n",
    "    # labels / output\n",
    "    train_emotions = df_train[\"labels\"].values.tolist()\n",
    "    test_emotions = df_test[\"labels\"].values.tolist()\n",
    "\n",
    "    train_dataset = GoEmotionDataset(train_encodings, train_emotions)\n",
    "    test_dataset = GoEmotionDataset(test_encodings, test_emotions)\n",
    "\n",
    "    return train_dataset, test_dataset, len(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17465856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, val_dataset, epochs, batch_size, device, lr=0.001, weight_decay=0.01, warmup_steps=0):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    model.to(device)\n",
    "\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for idx, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            loss, _ = model(input_ids, labels=labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            if (idx + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{epochs} | Batch {idx + 1}/{len(train_loader)} | Train Loss: {loss.item():.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_f1 = 0\n",
    "        val_acc = 0\n",
    "        num_val_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                loss, logits = model(input_ids, labels=labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                preds = (logits > 0).long()\n",
    "                f1 = f1_score(labels.cpu(), preds.cpu(), average='micro')\n",
    "                acc = accuracy_score(labels.cpu(), preds.cpu())\n",
    "\n",
    "                val_f1 += f1\n",
    "                val_acc += acc\n",
    "                num_val_batches += 1\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_f1 /= num_val_batches\n",
    "        val_acc /= num_val_batches\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f} | Val Acc: {val_acc:.4f} | Time: {elapsed_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e63f6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = create_word_to_ix(df_train, df_test)\n",
    "train_dataset, test_dataset, vocab_size = train_test(df_train, df_test, spacy_tokenizer, word_to_ix)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "src_pad_idx = word_to_ix['<pad>']\n",
    "model = MultilabelSequenceClassificationTransformer(\n",
    "    src_vocab_size= vocab_size,\n",
    "    num_classes= len(emotions),\n",
    "    src_pad_idx= src_pad_idx,\n",
    "    emb_size = 128,\n",
    "    max_len=128\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4b8b4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Epoch 1/10 | Batch 10/1320 | Train Loss: 0.3761\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Epoch 1/10 | Batch 20/1320 | Train Loss: 0.2905\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Epoch 1/10 | Batch 30/1320 | Train Loss: 0.2382\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Epoch 1/10 | Batch 40/1320 | Train Loss: 0.2124\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Epoch 1/10 | Batch 50/1320 | Train Loss: 0.1952\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(False, device='cuda:0')\n",
      "Pooled output: tensor(False, device='cuda:0')\n",
      "Logits output: tensor(False, device='cuda:0')\n",
      "Src Mask: tensor(False, device='cuda:0')\n",
      "Encoder output: tensor(True, device='cuda:0')\n",
      "Pooled output: tensor(True, device='cuda:0')\n",
      "Logits output: tensor(True, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: Error detected in BinaryCrossEntropyWithLogitsBackward0. Traceback of forward call that caused the error:\n",
      "  File \"c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\asyncio\\base_events.py\", line 1906, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3016, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3221, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3400, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3460, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\reidp\\AppData\\Local\\Temp\\ipykernel_5680\\3465158595.py\", line 4, in <module>\n",
      "    train_model(model, train_dataset, test_dataset, epochs, batch_size, device, lr)\n",
      "  File \"C:\\Users\\reidp\\AppData\\Local\\Temp\\ipykernel_5680\\2864299707.py\", line 20, in train_model\n",
      "    loss, _ = model(input_ids, labels=labels)\n",
      "  File \"c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\reidp\\Documents\\MEIA-PROJ3\\SentimentAnalysis\\Model\\transformer.py\", line 267, in forward\n",
      "    loss = loss_fct(logits, labels.float())\n",
      "  File \"c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 720, in forward\n",
      "    return F.binary_cross_entropy_with_logits(input, target,\n",
      "  File \"c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\nn\\functional.py\", line 3165, in binary_cross_entropy_with_logits\n",
      "    return torch.binary_cross_entropy_with_logits(input, target, weight, pos_weight, reduction_enum)\n",
      " (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\python_anomaly_mode.cpp:119.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function 'BinaryCrossEntropyWithLogitsBackward0' returned nan values in its 0th output.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m128\u001b[39m\n\u001b[0;32m      3\u001b[0m lr \u001b[39m=\u001b[39m \u001b[39m1e-4\u001b[39m\n\u001b[1;32m----> 4\u001b[0m train_model(model, train_dataset, test_dataset, epochs, batch_size, device, lr)\n",
      "Cell \u001b[1;32mIn[16], line 21\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_dataset, val_dataset, epochs, batch_size, device, lr, weight_decay, warmup_steps)\u001b[0m\n\u001b[0;32m     19\u001b[0m labels \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     20\u001b[0m loss, _ \u001b[39m=\u001b[39m model(input_ids, labels\u001b[39m=\u001b[39mlabels)\n\u001b[1;32m---> 21\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     22\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), max_norm\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m)\n\u001b[0;32m     23\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Function 'BinaryCrossEntropyWithLogitsBackward0' returned nan values in its 0th output."
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "train_model(model, train_dataset, test_dataset, epochs, batch_size, device, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15709e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_dataset(dataset):\n",
    "    label_counts = np.zeros(28)\n",
    "    sequence_lengths = []\n",
    "\n",
    "    for idx in range(len(dataset)):\n",
    "        sample = dataset[idx]\n",
    "        input_ids = sample['input_ids']\n",
    "        labels = sample['labels']\n",
    "\n",
    "        sequence_lengths.append(len(input_ids))\n",
    "        label_counts += labels.numpy()\n",
    "\n",
    "        # Check if there are NaN or infinite values in input data\n",
    "    if torch.isnan(input_ids).any() or torch.isinf(input_ids).any():\n",
    "        print(f\"NaN or infinite values found in input_ids at batch {idx + 1}\")\n",
    "\n",
    "    if torch.isnan(labels).any() or torch.isinf(labels).any():\n",
    "        print(f\"NaN or infinite values found in labels at batch {idx + 1}\")\n",
    "\n",
    "    # Normalize label counts\n",
    "    label_counts /= len(dataset)\n",
    "\n",
    "    # Analyze the distribution of sequence lengths\n",
    "    plt.hist(sequence_lengths, bins=50)\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Sequence Lengths')\n",
    "    plt.show()\n",
    "\n",
    "    # Analyze the distribution of labels\n",
    "    plt.bar(range(28), label_counts)\n",
    "    plt.xlabel('Label')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Labels')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Average sequence length: {np.mean(sequence_lengths):.2f}\")\n",
    "    print(f\"Standard deviation of sequence length: {np.std(sequence_lengths):.2f}\")\n",
    "    print(f\"Label frequencies: {label_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c8c80a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sequence length: 128.00\n",
      "Standard deviation of sequence length: 0.00\n",
      "Label frequencies: [0.0810798  0.04389799 0.03777407 0.06454402 0.08333037 0.02858226\n",
      " 0.03475948 0.0456333  0.01812895 0.04012532 0.05413217 0.0250998\n",
      " 0.01170297 0.02662781 0.01519136 0.05491395 0.00318041 0.03782737\n",
      " 0.03897042 0.00874762 0.0414579  0.00608839 0.04158227 0.00602324\n",
      " 0.01177404 0.03222463 0.02615401 0.26171778]\n"
     ]
    }
   ],
   "source": [
    "analyze_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545b40d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "37f16e38780d10c35648b3bd09ed40a738946f51fc07e16d845fbaf5897e263f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
