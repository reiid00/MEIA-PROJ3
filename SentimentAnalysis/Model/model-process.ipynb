{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d72859ab",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87742597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Dataset\n",
    "#Kaggle: https://www.kaggle.com/datasets/shivamb/go-emotions-google-emotions-dataset\n",
    "\n",
    "!wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_1.csv\n",
    "!wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_2.csv\n",
    "!wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_3.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b83dc85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 10 23:07:11 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 531.14                 Driver Version: 531.14       CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 Ti    WDDM | 00000000:29:00.0  On |                  N/A |\n",
      "| 30%   35C    P2               42W / 200W|   6842MiB /  8192MiB |      4%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2552    C+G   ...on\\112.0.1722.34\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A      3560    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      7952    C+G   ...oogle\\Chrome\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A      8092    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A      9096      C   ...p\\miniconda3\\envs\\tf_gpu\\python.exe    N/A      |\n",
      "|    0   N/A  N/A     10324    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     10760    C+G   ...aming\\Telegram Desktop\\Telegram.exe    N/A      |\n",
      "|    0   N/A  N/A     10948    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A     10984    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     11520    C+G   ...Programs\\Microsoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A     12680    C+G   ...Desktop\\app-3.2.1\\GitHubDesktop.exe    N/A      |\n",
      "|    0   N/A  N/A     13132    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     13476    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     14660    C+G   ...23.0_x86__zpdnekdrzrea0\\Spotify.exe    N/A      |\n",
      "|    0   N/A  N/A     15604    C+G   ...71.0_x64__8wekyb3d8bbwe\\GameBar.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "630a3245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import preprocessor\n",
    "import contractions\n",
    "import json\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "914f4fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>rater_id</th>\n",
       "      <th>example_very_unclear</th>\n",
       "      <th>admiration</th>\n",
       "      <th>...</th>\n",
       "      <th>love</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That game hurt.</td>\n",
       "      <td>eew5j0j</td>\n",
       "      <td>Brdd9</td>\n",
       "      <td>nrl</td>\n",
       "      <td>t3_ajis4z</td>\n",
       "      <td>t1_eew18eq</td>\n",
       "      <td>1.548381e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&gt;sexuality shouldnâ€™t be a grouping category I...</td>\n",
       "      <td>eemcysk</td>\n",
       "      <td>TheGreen888</td>\n",
       "      <td>unpopularopinion</td>\n",
       "      <td>t3_ai4q37</td>\n",
       "      <td>t3_ai4q37</td>\n",
       "      <td>1.548084e+09</td>\n",
       "      <td>37</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You do right, if you don't care then fuck 'em!</td>\n",
       "      <td>ed2mah1</td>\n",
       "      <td>Labalool</td>\n",
       "      <td>confessions</td>\n",
       "      <td>t3_abru74</td>\n",
       "      <td>t1_ed2m7g7</td>\n",
       "      <td>1.546428e+09</td>\n",
       "      <td>37</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Man I love reddit.</td>\n",
       "      <td>eeibobj</td>\n",
       "      <td>MrsRobertshaw</td>\n",
       "      <td>facepalm</td>\n",
       "      <td>t3_ahulml</td>\n",
       "      <td>t3_ahulml</td>\n",
       "      <td>1.547965e+09</td>\n",
       "      <td>18</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[NAME] was nowhere near them, he was by the Fa...</td>\n",
       "      <td>eda6yn6</td>\n",
       "      <td>American_Fascist713</td>\n",
       "      <td>starwarsspeculation</td>\n",
       "      <td>t3_ackt2f</td>\n",
       "      <td>t1_eda65q2</td>\n",
       "      <td>1.546669e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       id  \\\n",
       "0                                    That game hurt.  eew5j0j   \n",
       "1   >sexuality shouldnâ€™t be a grouping category I...  eemcysk   \n",
       "2     You do right, if you don't care then fuck 'em!  ed2mah1   \n",
       "3                                 Man I love reddit.  eeibobj   \n",
       "4  [NAME] was nowhere near them, he was by the Fa...  eda6yn6   \n",
       "\n",
       "                author            subreddit    link_id   parent_id  \\\n",
       "0                Brdd9                  nrl  t3_ajis4z  t1_eew18eq   \n",
       "1          TheGreen888     unpopularopinion  t3_ai4q37   t3_ai4q37   \n",
       "2             Labalool          confessions  t3_abru74  t1_ed2m7g7   \n",
       "3        MrsRobertshaw             facepalm  t3_ahulml   t3_ahulml   \n",
       "4  American_Fascist713  starwarsspeculation  t3_ackt2f  t1_eda65q2   \n",
       "\n",
       "    created_utc  rater_id  example_very_unclear  admiration  ...  love  \\\n",
       "0  1.548381e+09         1                 False           0  ...     0   \n",
       "1  1.548084e+09        37                  True           0  ...     0   \n",
       "2  1.546428e+09        37                 False           0  ...     0   \n",
       "3  1.547965e+09        18                 False           0  ...     1   \n",
       "4  1.546669e+09         2                 False           0  ...     0   \n",
       "\n",
       "   nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
       "0            0         0      0            0       0        0        1   \n",
       "1            0         0      0            0       0        0        0   \n",
       "2            0         0      0            0       0        0        0   \n",
       "3            0         0      0            0       0        0        0   \n",
       "4            0         0      0            0       0        0        0   \n",
       "\n",
       "   surprise  neutral  \n",
       "0         0        0  \n",
       "1         0        0  \n",
       "2         0        1  \n",
       "3         0        0  \n",
       "4         0        1  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = 'data/full_dataset/goemotions_'\n",
    "OUTPUT_DIR = 'training_data'\n",
    "\n",
    "df1 = pd.read_csv(f'{DATA_PATH}1.csv')\n",
    "df2 = pd.read_csv(f'{DATA_PATH}2.csv')\n",
    "df3 = pd.read_csv(f'{DATA_PATH}3.csv')\n",
    "\n",
    "frames = [df1,df2,df3]\n",
    "\n",
    "df = pd.concat(frames)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ecff46d",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87fb69ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM: https://www.kaggle.com/code/esknight/emotion-classification-final\n",
    "# Function for cleaning text\n",
    "def clean_text(text):\n",
    "    re_number = re.compile('[0-9]+')\n",
    "    re_url = re.compile(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\")\n",
    "    re_tag = re.compile('\\[[A-Z]+\\]')\n",
    "    re_char = re.compile('[^0-9a-zA-Z\\s?!.,:\\'\\\"//]+')\n",
    "    re_char_clean = re.compile('[^0-9a-zA-Z\\s?!.,\\[\\]]')\n",
    "    re_punc = re.compile('[?!,.\\'\\\"]')\n",
    "  \n",
    "    text = re.sub(re_char, \"\", text) # Remove unknown character \n",
    "    text = contractions.fix(text) # Expand contraction\n",
    "    text = re.sub(re_url, ' [url] ', text) # Replace URL with number\n",
    "    text = re.sub(re_char_clean, \"\", text) # Only alphanumeric and punctuations.\n",
    "    #text = re.sub(re_punc, \"\", text) # Remove punctuation.\n",
    "    text = text.lower() # Lower text\n",
    "    text = \" \".join([w for w in text.split(' ') if w != \" \"]) # Remove whitespace\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cce4567c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46444101e2e146d7a36b533ab8487cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/211225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clean text\n",
    "df['clean_text'] = df['text'].progress_apply(clean_text)\n",
    "\n",
    "# Drop Useless Columns\n",
    "df = df.drop(columns=['id','example_very_unclear','author','subreddit','link_id','parent_id','created_utc','rater_id'])\n",
    "\n",
    "# Reorganize Columns\n",
    "df = df[['clean_text'] + [col for col in df.columns if col not in ['text','clean_text']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98a7282b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise',\n",
       " 'neutral']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View emotions easier\n",
    "emotions = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "482895ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {str(i):label for i, label in enumerate(emotions)}\n",
    "label2id = {label:str(i) for i, label in enumerate(emotions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f2819ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'admiration', '1': 'amusement', '2': 'anger', '3': 'annoyance', '4': 'approval', '5': 'caring', '6': 'confusion', '7': 'curiosity', '8': 'desire', '9': 'disappointment', '10': 'disapproval', '11': 'disgust', '12': 'embarrassment', '13': 'excitement', '14': 'fear', '15': 'gratitude', '16': 'grief', '17': 'joy', '18': 'love', '19': 'nervousness', '20': 'optimism', '21': 'pride', '22': 'realization', '23': 'relief', '24': 'remorse', '25': 'sadness', '26': 'surprise', '27': 'neutral'}\n",
      "{'admiration': '0', 'amusement': '1', 'anger': '2', 'annoyance': '3', 'approval': '4', 'caring': '5', 'confusion': '6', 'curiosity': '7', 'desire': '8', 'disappointment': '9', 'disapproval': '10', 'disgust': '11', 'embarrassment': '12', 'excitement': '13', 'fear': '14', 'gratitude': '15', 'grief': '16', 'joy': '17', 'love': '18', 'nervousness': '19', 'optimism': '20', 'pride': '21', 'realization': '22', 'relief': '23', 'remorse': '24', 'sadness': '25', 'surprise': '26', 'neutral': '27'}\n"
     ]
    }
   ],
   "source": [
    "print(id2label)\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6dda44d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>admiration</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>approval</th>\n",
       "      <th>caring</th>\n",
       "      <th>confusion</th>\n",
       "      <th>curiosity</th>\n",
       "      <th>desire</th>\n",
       "      <th>...</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>that game hurt.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sexuality should not be a grouping category i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you do right, if you do not care then fuck them!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>man i love reddit.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>name was nowhere near them, he was by the falc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  admiration  amusement  \\\n",
       "0                                    that game hurt.           0          0   \n",
       "1   sexuality should not be a grouping category i...           0          0   \n",
       "2   you do right, if you do not care then fuck them!           0          0   \n",
       "3                                 man i love reddit.           0          0   \n",
       "4  name was nowhere near them, he was by the falc...           0          0   \n",
       "\n",
       "   anger  annoyance  approval  caring  confusion  curiosity  desire  ...  \\\n",
       "0      0          0         0       0          0          0       0  ...   \n",
       "1      0          0         0       0          0          0       0  ...   \n",
       "2      0          0         0       0          0          0       0  ...   \n",
       "3      0          0         0       0          0          0       0  ...   \n",
       "4      0          0         0       0          0          0       0  ...   \n",
       "\n",
       "   nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
       "0            0         0      0            0       0        0        1   \n",
       "1            0         0      0            0       0        0        0   \n",
       "2            0         0      0            0       0        0        0   \n",
       "3            0         0      0            0       0        0        0   \n",
       "4            0         0      0            0       0        0        0   \n",
       "\n",
       "   surprise  neutral                                             labels  \n",
       "0         0        0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1         0        0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2         0        1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3         0        0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4         0        1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-Hot Encoding all Emotions\n",
    "df[\"labels\"] = df[emotions].values.tolist()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "86a5f182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((168923, 30), (42302, 30))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create train / test splits\n",
    "mask = np.random.rand(len(df)) < 0.8\n",
    "df_train = df[mask]\n",
    "df_test = df[~mask]\n",
    "\n",
    "(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6cfca4",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a9c9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotions Visualization by number of cases\n",
    "\n",
    "temp = df[list(emotions)].sum(axis=0) \\\n",
    "    .reset_index() \\\n",
    "    .rename(columns={'index': 'emotion', 0: 'n'}) \\\n",
    "    .sort_values('n', ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.tick_params(axis='x', rotation=90)\n",
    "sns.barplot(data=temp, x='n', \n",
    "            y='emotion',\n",
    "            dodge=False,\n",
    "            ax=ax).set_title('Emotions by number of appearances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3428cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating emotions\n",
    "\n",
    "pos = {'admiration','amusement','approval','caring','desire','excitement','gratitude','joy','love',\n",
    "       'optimism','pride','relief'}\n",
    "neg = {'sadness','fear','embarrassment','disapproval','disappointment','annoyance','anger','nervousness',\n",
    "       'remorse','grief','disgust'}\n",
    "amb= {'realization','surprise','curiosity','confusion','neutral'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0cbe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotions and data vis\n",
    "\n",
    "print(\"Length of data: \", len(df))\n",
    "print(\"Number of emotions: \", len(emotions))\n",
    "print(\"Number of positive emotions: \", len(pos))\n",
    "print(\"Number of negative emotions: \", len(neg))\n",
    "print(\"Number of ambiguous emotions: \", len(amb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3aed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotions dataframe to later on aggregate\n",
    "\n",
    "df_emotion = pd.DataFrame()\n",
    "df_emotion['emotion'] = list(emotions)\n",
    "df_emotion['group'] = ''\n",
    "df_emotion['group'].loc[df_emotion['emotion'].isin(pos)] = 'positive'\n",
    "df_emotion['group'].loc[df_emotion['emotion'].isin(neg)] = 'negative'\n",
    "df_emotion['group'].loc[df_emotion['emotion'].isin(amb)] = 'ambiguous'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bfe9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emotion.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7952ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotions by number of appearences but by group\n",
    "\n",
    "temp = pd.DataFrame()\n",
    "temp['true positive rate'] = df.iloc[:, 3:-1].mean(0)\n",
    "temp['emotion'] = df.columns[3:-1]\n",
    "temp = temp.merge(df_emotion, how='left', on='emotion')\n",
    "temp = temp.sort_values('true positive rate')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "ax.tick_params(axis='x', rotation=90)\n",
    "\n",
    "sns.barplot(x=temp['emotion'], \n",
    "            y=temp['true positive rate'], \n",
    "            hue=temp['group'], \n",
    "            dodge=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d51f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def represent_train_test_balance(train_df,test_df):\n",
    "    # Class representation for train/test DS\n",
    "    train_GO = (train_df.loc[:,list(emotions)].sum(axis=0) / len(train_df)) * 100\n",
    "    test_GO = (test_df.loc[:,list(emotions)].sum(axis=0) / len(test_df)) * 100\n",
    "    \n",
    "    # Unique dataset for visualization purposes\n",
    "    \n",
    "    ds_GO = pd.DataFrame(data=[train_GO, test_GO]).T.reset_index(drop=False)\n",
    "    ds_GO.columns = ['Emotion', 'Train','Test']\n",
    "    ds_GO = ds_GO.sort_values('Train',ascending=False)\n",
    "    ds_GO = ds_GO.melt(id_vars='Emotion', var_name='Dataset', value_vars=['Train','Test'],\n",
    "                      value_name='Percentage')\n",
    "    \n",
    "    # Display dataset\n",
    "    \n",
    "    display(ds_GO.head(10))\n",
    "    \n",
    "    print(\"Graph Visualization\")\n",
    "    \n",
    "    plt.figure(figsize=(20,15))\n",
    "    sns.barplot(x='Percentage', y='Emotion', data=ds_GO, orient='h', hue='Dataset')\n",
    "    plt.title('Percentage of samples per emotion in train and test datasets', fontweight='bold', fontsize=20)\n",
    "    plt.xlabel('Percentage of all samples', fontweight='bold', fontsize=16)\n",
    "    plt.ylabel('Emotions', fontweight='bold', fontsize= 16)\n",
    "    plt.show()\n",
    "represent_train_test_balance(df_train, df_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8818d771",
   "metadata": {},
   "source": [
    "# Tokenization / Encoding / Method Structuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea478700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, DistilBertForSequenceClassification, BertForSequenceClassification, RobertaForSequenceClassification, XLNetForSequenceClassification\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers.models.xlnet.modeling_xlnet import XLNetForSequenceClassificationOutput\n",
    "from torch import nn\n",
    "import random\n",
    "import torch\n",
    "import platform\n",
    "import sys\n",
    "import sklearn as sk\n",
    "from typing import Optional, Union, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3799bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43432890",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoEmotionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edbe207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(df_train, df_test, tokenizer): \n",
    "  # Encodings\n",
    "\n",
    "  train_encodings = tokenizer(df_train[\"clean_text\"].values.tolist(), truncation=True)\n",
    "  test_encodings = tokenizer(df_test[\"clean_text\"].values.tolist(), truncation=True)\n",
    "\n",
    "  # labels / output\n",
    "  train_emotions = df_train[\"labels\"].values.tolist()\n",
    "  test_emotions = df_test[\"labels\"].values.tolist()\n",
    "\n",
    "  train_dataset = GoEmotionDataset(train_encodings, train_emotions)\n",
    "  test_dataset = GoEmotionDataset(test_encodings, test_emotions)\n",
    "  return train_dataset, test_dataset\n",
    "  \n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    y_pred = torch.from_numpy(logits)\n",
    "    y_true = torch.from_numpy(labels)\n",
    "    y_pred = y_pred.sigmoid()\n",
    "    y_pred = y_pred>0.5\n",
    "    y_true = y_true.bool()\n",
    "    acc = (y_pred==y_true).float().mean().item()\n",
    "\n",
    "    return {       \n",
    "      'Accuracy': acc\n",
    "    }\n",
    "    \n",
    "def set_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic=False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "def device_to_use():\n",
    "    has_gpu = torch.cuda.is_available()\n",
    "    has_mps = getattr(torch,'has_mps',False)\n",
    "    device = \"mps\" if getattr(torch,'has_mps',False) \\\n",
    "        else \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    print(f\"Python Platform: {platform.platform()}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    print()\n",
    "    print(f\"Python {sys.version}\")\n",
    "    print(f\"Pandas {pd.__version__}\")\n",
    "    print(f\"Scikit-Learn {sk.__version__}\")\n",
    "    print(\"GPU is\", \"available\" if has_gpu else \"NOT AVAILABLE\")\n",
    "    print(\"MPS (Apple Metal) is\", \"AVAILABLE\" if has_mps else \"NOT AVAILABLE\")\n",
    "    print(f\"Target device is {device}\")\n",
    "    return device\n",
    "\n",
    "def model_train(train_dataset, test_dataset, model, tokenizer, NUM_EPOCHS = 10,batch_size = 16, adam_epsilon_arg = 1e-8, learning_rate_arg = 2e-5, use_mps_device_arg = False, model_name = \"default\"):\n",
    "  training_args = TrainingArguments( \n",
    "    output_dir= OUTPUT_DIR+\"/\"+model_name,    \n",
    "    adam_epsilon = adam_epsilon_arg,\n",
    "    learning_rate = learning_rate_arg,\n",
    "    use_mps_device = use_mps_device_arg, # Mac Sylicon GPU\n",
    "    per_device_train_batch_size = batch_size, \n",
    "    per_device_eval_batch_size = batch_size*4,\n",
    "    gradient_accumulation_steps = 2, # scale batch size without needing more memory\n",
    "    num_train_epochs= NUM_EPOCHS,\n",
    "    do_eval = True,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    save_strategy = 'epoch',\n",
    "    load_best_model_at_end = True, # this allows to automatically get the best model at the end based on whatever metric we want\n",
    "    metric_for_best_model = 'Accuracy',\n",
    "    greater_is_better = True,\n",
    "    weight_decay=0.01,\n",
    "    seed = 25,\n",
    "    report_to=\"none\"\n",
    "  )\n",
    "  set_seed(training_args.seed)\n",
    "  trainer = Trainer(\n",
    "      model = model,\n",
    "      args = training_args,\n",
    "      train_dataset = train_dataset,\n",
    "      eval_dataset=test_dataset,\n",
    "      compute_metrics=compute_metrics,\n",
    "      tokenizer=tokenizer\n",
    "  )\n",
    "  return training_args, trainer\n",
    "                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093bf286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes to Each Model\n",
    "\n",
    "class DistilBertForMultilabelSequenceClassification(DistilBertForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "      super().__init__(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[SequenceClassifierOutput, Tuple[torch.Tensor, ...]]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        distilbert_output = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n",
    "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
    "        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n",
    "        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)\n",
    "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
    "        logits = self.classifier(pooled_output)  # (bs, num_labels)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), \n",
    "                            labels.float().view(-1, self.num_labels))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + distilbert_output[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=distilbert_output.hidden_states,\n",
    "            attentions=distilbert_output.attentions)\n",
    "\n",
    "class BertForMultilabelSequenceClassification(BertForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "      super().__init__(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        bert_output = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = bert_output[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), \n",
    "                            labels.float().view(-1, self.num_labels))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + bert_output[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=bert_output.hidden_states,\n",
    "            attentions=bert_output.attentions)\n",
    "\n",
    "class RoBertaForMultilabelSequenceClassification(RobertaForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "      super().__init__(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        roberta_output = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = roberta_output[0]\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), \n",
    "                            labels.float().view(-1, self.num_labels))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + roberta_output[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=roberta_output.hidden_states,\n",
    "            attentions=roberta_output.attentions)\n",
    "\n",
    "class XLNetForMultilabelSequenceClassification(XLNetForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "      super().__init__(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        mems: Optional[torch.Tensor] = None,\n",
    "        perm_mask: Optional[torch.Tensor] = None,\n",
    "        target_mapping: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        input_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        use_mems: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs,  # delete when `use_cache` is removed in XLNetModel\n",
    "    ) -> Union[Tuple, XLNetForSequenceClassificationOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        xlnet_output = self.transformer(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            mems=mems,\n",
    "            perm_mask=perm_mask,\n",
    "            target_mapping=target_mapping,\n",
    "            token_type_ids=token_type_ids,\n",
    "            input_mask=input_mask,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_mems=use_mems,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            **kwargs)\n",
    "        output = xlnet_output[0]\n",
    "        output = self.sequence_summary(output)\n",
    "        logits = self.logits_proj(output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), \n",
    "                            labels.float().view(-1, self.num_labels))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + xlnet_output[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=xlnet_output.hidden_states,\n",
    "            attentions=xlnet_output.attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5de0b7",
   "metadata": {},
   "source": [
    "# Pre-Trained Model - DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e24f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_or_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)\n",
    "num_labels=len(emotions)\n",
    "device = device_to_use()\n",
    "if device == 'gpu': device = 'cuda'\n",
    "model = DistilBertForMultilabelSequenceClassification.from_pretrained(model_path_or_name, num_labels=num_labels).to(device)\n",
    "model = model_config_ids(model, id2label, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a93aa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test(df_train, df_test, tokenizer)\n",
    "training_args, trainer = model_train(train_dataset, test_dataset, model, tokenizer, NUM_EPOCHS = 3,batch_size = 16, adam_epsilon_arg = 1e-8, learning_rate_arg = 2e-5, use_mps_device_arg = False, model_name = \"distilbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed43f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c506dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fbac40d",
   "metadata": {},
   "source": [
    "# Pre-Trained Model - BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b7cabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_or_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)\n",
    "num_labels=len(emotions)\n",
    "device = device_to_use()\n",
    "if device == 'gpu': device = 'cuda'\n",
    "model = BertForMultilabelSequenceClassification.from_pretrained(model_path_or_name, num_labels=num_labels).to(device)\n",
    "model = model_config_ids(model, id2label, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c934fc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test(df_train, df_test, tokenizer)\n",
    "training_args, trainer = model_train(train_dataset, test_dataset, model, tokenizer, NUM_EPOCHS = 3,batch_size = 16, adam_epsilon_arg = 1e-8, learning_rate_arg = 2e-5, use_mps_device_arg = False, model_name = \"bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9a9d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deb8a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1c51500",
   "metadata": {},
   "source": [
    "# Pre-Trained Model - RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640c5d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_or_name = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)\n",
    "num_labels=len(emotions)\n",
    "device = device_to_use()\n",
    "if device == 'gpu': device = 'cuda'\n",
    "model = RoBertaForMultilabelSequenceClassification.from_pretrained(model_path_or_name, num_labels=num_labels).to(device)\n",
    "model = model_config_ids(model, id2label, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e8c30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test(df_train, df_test, tokenizer)\n",
    "training_args, trainer = model_train(train_dataset, test_dataset, model, tokenizer, NUM_EPOCHS = 3,batch_size = 16, adam_epsilon_arg = 1e-8, learning_rate_arg = 2e-5, use_mps_device_arg = False, model_name = \"roberta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3fd5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881319ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "562ca376",
   "metadata": {},
   "source": [
    "# Pre-Trained Model - XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30c042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_or_name = \"xlnet-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)\n",
    "num_labels=len(emotions)\n",
    "device = device_to_use()\n",
    "if device == 'gpu': device = 'cuda'\n",
    "model = XLNetForMultilabelSequenceClassification.from_pretrained(model_path_or_name, num_labels=num_labels).to(device)\n",
    "model = model_config_ids(model, id2label, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905b8aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test(df_train, df_test, tokenizer)\n",
    "training_args, trainer = model_train(train_dataset, test_dataset, model, tokenizer, NUM_EPOCHS = 3,batch_size = 8, adam_epsilon_arg = 1e-8, learning_rate_arg = 2e-5, use_mps_device_arg = False, model_name = \"xlnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6849aa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b41285f4",
   "metadata": {},
   "source": [
    "# Transformer from Scratch - All you need!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71838691",
   "metadata": {},
   "source": [
    "Tokenizer - Still want to try some new ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f80cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from transformer import MultilabelSequenceClassificationTransformer\n",
    "# Imports\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, DistilBertForSequenceClassification, BertForSequenceClassification, RobertaForSequenceClassification, XLNetForSequenceClassification\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers.models.xlnet.modeling_xlnet import XLNetForSequenceClassificationOutput\n",
    "from torch import nn\n",
    "import random\n",
    "import torch\n",
    "import platform\n",
    "import sys\n",
    "import sklearn as sk\n",
    "from typing import Optional, Union, Tuple\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47e0e4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98a2cea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoEmotionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7cb923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_to_ix(df_train, df_test):\n",
    "    word_to_ix = {'<pad>': 0, '<unk>': 1}\n",
    "    for text in pd.concat([df_train[\"clean_text\"], df_test[\"clean_text\"]]):\n",
    "        for token in spacy_tokenizer(text):\n",
    "            if token not in word_to_ix:\n",
    "                word_to_ix[token] = len(word_to_ix)\n",
    "    return word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af7dd7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text, word_to_ix, max_length=128):\n",
    "    tokens = [t for t in spacy_tokenizer(text)]\n",
    "    input_ids = [word_to_ix.get(token, word_to_ix['<unk>']) for token in tokens][:max_length]\n",
    "    input_ids = input_ids + [0] * (max_length - len(input_ids))\n",
    "    attention_mask = [1 if token_id != 0 else 0 for token_id in input_ids]\n",
    "\n",
    "    return {\n",
    "        'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "        'attention_mask': torch.tensor(attention_mask, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c93436f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(df_train, df_test, tokenizer, word_to_ix):\n",
    "    # Encodings\n",
    "    train_encoded_texts = [encode_text(text, word_to_ix) for text in df_train[\"clean_text\"].values.tolist()]\n",
    "    test_encoded_texts = [encode_text(text, word_to_ix) for text in df_test[\"clean_text\"].values.tolist()]\n",
    "\n",
    "    train_encodings = {\n",
    "        'input_ids': [text_encoding['input_ids'] for text_encoding in train_encoded_texts],\n",
    "        'attention_mask': [text_encoding['attention_mask'] for text_encoding in train_encoded_texts]\n",
    "    }\n",
    "\n",
    "    test_encodings = {\n",
    "        'input_ids': [text_encoding['input_ids'] for text_encoding in test_encoded_texts],\n",
    "        'attention_mask': [text_encoding['attention_mask'] for text_encoding in test_encoded_texts]\n",
    "    }\n",
    "\n",
    "    # labels / output\n",
    "    train_emotions = df_train[\"labels\"].values.tolist()\n",
    "    test_emotions = df_test[\"labels\"].values.tolist()\n",
    "\n",
    "    train_dataset = GoEmotionDataset(train_encodings, train_emotions)\n",
    "    test_dataset = GoEmotionDataset(test_encodings, test_emotions)\n",
    "\n",
    "    return train_dataset, test_dataset, len(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17465856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, val_dataset, epochs, batch_size, device, lr=0.001, weight_decay=0.01, warmup_steps=0):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,pin_memory=True)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    model.to(device)\n",
    "\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for idx, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            loss, _ = model(input_ids, labels=labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{epochs} | Batch {idx + 1}/{len(train_loader)} | Train Loss: {loss.item():.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_f1 = 0\n",
    "        val_acc = 0\n",
    "        num_val_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                loss, logits = model(input_ids, labels=labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                preds = (logits > 0).long()\n",
    "                f1 = f1_score(labels.cpu(), preds.cpu(), average='micro')\n",
    "                acc = accuracy_score(labels.cpu(), preds.cpu())\n",
    "\n",
    "                val_f1 += f1\n",
    "                val_acc += acc\n",
    "                num_val_batches += 1\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_f1 /= num_val_batches\n",
    "        val_acc /= num_val_batches\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f} | Val Acc: {val_acc:.4f} | Time: {elapsed_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e63f6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = create_word_to_ix(df_train, df_test)\n",
    "train_dataset, test_dataset, vocab_size = train_test(df_train, df_test, spacy_tokenizer, word_to_ix)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "src_pad_idx = word_to_ix['<pad>']\n",
    "model = MultilabelSequenceClassificationTransformer(\n",
    "    src_vocab_size= vocab_size,\n",
    "    num_classes= len(emotions),\n",
    "    src_pad_idx= src_pad_idx,\n",
    "    emb_size = 128,\n",
    "    max_len=128\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4b8b4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "loss, logits, labels = train_model(model, train_dataset, test_dataset, epochs, batch_size, device, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229338e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_debug(model, train_dataset, val_dataset, epochs, batch_size, device, lr=0.001, weight_decay=0.01, warmup_steps=0):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,pin_memory=True)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    model.to(device)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        for _, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            loss, logits = model(input_ids, labels=labels)\n",
    "            return loss, logits, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f3d9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To verify everything was correct, which is\n",
    "loss, logits, labels = train_model_debug(model, train_dataset, test_dataset, epochs, batch_size, device, lr)\n",
    "print(logits[0]) # Which will then be normalized\n",
    "print(labels[0])\n",
    "loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "loss = loss_fct(logits[0], labels[0].float())\n",
    "print(loss)\n",
    "loss = loss_fct(logits, labels.float())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15709e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_dataset(dataset):\n",
    "    label_counts = np.zeros(28)\n",
    "    sequence_lengths = []\n",
    "\n",
    "    for idx in range(len(dataset)):\n",
    "        sample = dataset[idx]\n",
    "        input_ids = sample['input_ids']\n",
    "        labels = sample['labels']\n",
    "\n",
    "        sequence_lengths.append(len(input_ids))\n",
    "        label_counts += labels.numpy()\n",
    "\n",
    "        # Check if there are NaN or infinite values in input data\n",
    "    if torch.isnan(input_ids).any() or torch.isinf(input_ids).any():\n",
    "        print(f\"NaN or infinite values found in input_ids at batch {idx + 1}\")\n",
    "\n",
    "    if torch.isnan(labels).any() or torch.isinf(labels).any():\n",
    "        print(f\"NaN or infinite values found in labels at batch {idx + 1}\")\n",
    "\n",
    "    # Normalize label counts\n",
    "    label_counts /= len(dataset)\n",
    "\n",
    "    # Analyze the distribution of sequence lengths\n",
    "    plt.hist(sequence_lengths, bins=50)\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Sequence Lengths')\n",
    "    plt.show()\n",
    "\n",
    "    # Analyze the distribution of labels\n",
    "    plt.bar(range(28), label_counts)\n",
    "    plt.xlabel('Label')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Labels')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Average sequence length: {np.mean(sequence_lengths):.2f}\")\n",
    "    print(f\"Standard deviation of sequence length: {np.std(sequence_lengths):.2f}\")\n",
    "    print(f\"Label frequencies: {label_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c80a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_dataset(train_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dde27b3b",
   "metadata": {},
   "source": [
    "# Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "983cec0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i: label for i, label in enumerate(emotions)}\n",
    "label2id = {label: i for i, label in enumerate(emotions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f0af36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0be20141",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at training_data/distilbert/checkpoint-15876 were not used when initializing DistilBertModel: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at training_data/bert/checkpoint-15816 were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at training_data/roberta/checkpoint-15816 were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at training_data/roberta/checkpoint-15816 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at training_data/xlnet/checkpoint-31710 were not used when initializing XLNetModel: ['sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias']\n",
      "- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "DISTILBERT_PATH = \"training_data/distilbert/checkpoint-15876\"\n",
    "tokenizer_distil = AutoTokenizer.from_pretrained(DISTILBERT_PATH)\n",
    "model_distil = AutoModel.from_pretrained(DISTILBERT_PATH)\n",
    "\n",
    "BERT_PATH = \"training_data/bert/checkpoint-15816\"\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(BERT_PATH)\n",
    "model_bert = AutoModel.from_pretrained(BERT_PATH)\n",
    "\n",
    "ROBERTA_PATH = \"training_data/roberta/checkpoint-15816\"\n",
    "tokenizer_roberta = AutoTokenizer.from_pretrained(ROBERTA_PATH)\n",
    "model_roberta = AutoModel.from_pretrained(ROBERTA_PATH)\n",
    "\n",
    "XLNET_PATH = \"training_data/xlnet/checkpoint-31710\"\n",
    "config_xlnet = AutoConfig.from_pretrained(XLNET_PATH)\n",
    "tokenizer_xlnet = AutoTokenizer.from_pretrained(XLNET_PATH)\n",
    "model_xlnet = AutoModel.from_pretrained(XLNET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04bf39ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>admiration</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>approval</th>\n",
       "      <th>caring</th>\n",
       "      <th>confusion</th>\n",
       "      <th>curiosity</th>\n",
       "      <th>desire</th>\n",
       "      <th>...</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>that game hurt.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sexuality should not be a grouping category i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you do right, if you do not care then fuck them!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>man i love reddit.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>name was nowhere near them, he was by the falc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  admiration  amusement  \\\n",
       "0                                    that game hurt.           0          0   \n",
       "1   sexuality should not be a grouping category i...           0          0   \n",
       "2   you do right, if you do not care then fuck them!           0          0   \n",
       "3                                 man i love reddit.           0          0   \n",
       "4  name was nowhere near them, he was by the falc...           0          0   \n",
       "\n",
       "   anger  annoyance  approval  caring  confusion  curiosity  desire  ...  \\\n",
       "0      0          0         0       0          0          0       0  ...   \n",
       "1      0          0         0       0          0          0       0  ...   \n",
       "2      0          0         0       0          0          0       0  ...   \n",
       "3      0          0         0       0          0          0       0  ...   \n",
       "4      0          0         0       0          0          0       0  ...   \n",
       "\n",
       "   nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
       "0            0         0      0            0       0        0        1   \n",
       "1            0         0      0            0       0        0        0   \n",
       "2            0         0      0            0       0        0        0   \n",
       "3            0         0      0            0       0        0        0   \n",
       "4            0         0      0            0       0        0        0   \n",
       "\n",
       "   surprise  neutral                                             labels  \n",
       "0         0        0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1         0        0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2         0        1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3         0        0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4         0        1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f18e414",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "tokenizers = [tokenizer_distil, tokenizer_bert, tokenizer_roberta, tokenizer_xlnet]\n",
    "eval_datasets = []\n",
    "\n",
    "for tokenizer in tokenizers:\n",
    "    test_emotions = df_test[\"labels\"].values.tolist()\n",
    "    test_encodings = tokenizer(df_test[\"clean_text\"].values.tolist(), truncation=True)\n",
    "    eval_datasets.append(GoEmotionDataset(test_encodings, test_emotions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ffcd203",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 27\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m model, tokenizer, eval_dataset \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(models, tokenizers, eval_datasets):\n\u001b[0;32m     20\u001b[0m     trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     21\u001b[0m         model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     22\u001b[0m         args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m     23\u001b[0m         tokenizer\u001b[39m=\u001b[39mtokenizer,\n\u001b[0;32m     24\u001b[0m         compute_metrics\u001b[39m=\u001b[39mcompute_metrics,\n\u001b[0;32m     25\u001b[0m     )\n\u001b[1;32m---> 27\u001b[0m     result \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mevaluate(eval_dataset)\n\u001b[0;32m     28\u001b[0m     results\u001b[39m.\u001b[39mappend(result)\n",
      "File \u001b[1;32mc:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\transformers\\trainer.py:2932\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2929\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m   2931\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 2932\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[0;32m   2933\u001b[0m     eval_dataloader,\n\u001b[0;32m   2934\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   2935\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   2936\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   2937\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   2938\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[0;32m   2939\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[0;32m   2940\u001b[0m )\n\u001b[0;32m   2942\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[0;32m   2943\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmetric_key_prefix\u001b[39m}\u001b[39;00m\u001b[39m_jit_compilation_time\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m output\u001b[39m.\u001b[39mmetrics:\n",
      "File \u001b[1;32mc:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\transformers\\trainer.py:3103\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3101\u001b[0m observed_num_examples \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m   3102\u001b[0m \u001b[39m# Main evaluation loop\u001b[39;00m\n\u001b[1;32m-> 3103\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m   3104\u001b[0m     \u001b[39m# Update the observed num examples\u001b[39;00m\n\u001b[0;32m   3105\u001b[0m     observed_batch_size \u001b[39m=\u001b[39m find_batch_size(inputs)\n\u001b[0;32m   3106\u001b[0m     \u001b[39mif\u001b[39;00m observed_batch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[29], line 33\u001b[0m, in \u001b[0;36mGoEmotionDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[1;32m---> 33\u001b[0m     item \u001b[39m=\u001b[39m {key: torch\u001b[39m.\u001b[39mtensor(val[idx]) \u001b[39mfor\u001b[39;00m key, val \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencodings}\n\u001b[0;32m     34\u001b[0m     item[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels[idx])\n\u001b[0;32m     35\u001b[0m     \u001b[39mreturn\u001b[39;00m item\n",
      "Cell \u001b[1;32mIn[29], line 33\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[1;32m---> 33\u001b[0m     item \u001b[39m=\u001b[39m {key: torch\u001b[39m.\u001b[39mtensor(val[idx]) \u001b[39mfor\u001b[39;00m key, val \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencodings}\n\u001b[0;32m     34\u001b[0m     item[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels[idx])\n\u001b[0;32m     35\u001b[0m     \u001b[39mreturn\u001b[39;00m item\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = np.sum(predictions == labels) / len(labels)\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"training_data/eval_models\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    no_cuda=False,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "models = [model_distil, model_bert, model_roberta, model_xlnet]\n",
    "results = []\n",
    "\n",
    "for model, tokenizer, eval_dataset in zip(models, tokenizers, eval_datasets):\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    result = trainer.evaluate(eval_dataset)\n",
    "    results.append(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7fe95972",
   "metadata": {},
   "source": [
    "# Implementation with own Dataset and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545b40d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "37f16e38780d10c35648b3bd09ed40a738946f51fc07e16d845fbaf5897e263f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
