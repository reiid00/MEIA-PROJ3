{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d72859ab",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87742597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Dataset\n",
    "#Kaggle: https://www.kaggle.com/datasets/shivamb/go-emotions-google-emotions-dataset\n",
    "\n",
    "!wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_1.csv\n",
    "!wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_2.csv\n",
    "!wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_3.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b83dc85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr  6 18:02:37 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 531.14                 Driver Version: 531.14       CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 Ti    WDDM | 00000000:29:00.0  On |                  N/A |\n",
      "| 30%   37C    P0               42W / 200W|    403MiB /  8192MiB |      2%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      8212    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A     10780    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A     10804    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     11796    C+G   ...23.0_x86__zpdnekdrzrea0\\Spotify.exe    N/A      |\n",
      "|    0   N/A  N/A     12856    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     13404    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     13700    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     13708    C+G   C:\\Windows\\System32\\WWAHost.exe           N/A      |\n",
      "|    0   N/A  N/A     13768    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     14380    C+G   ...Programs\\Microsoft VS Code\\Code.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "630a3245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import preprocessor\n",
    "import contractions\n",
    "import json\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "914f4fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>rater_id</th>\n",
       "      <th>example_very_unclear</th>\n",
       "      <th>admiration</th>\n",
       "      <th>...</th>\n",
       "      <th>love</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That game hurt.</td>\n",
       "      <td>eew5j0j</td>\n",
       "      <td>Brdd9</td>\n",
       "      <td>nrl</td>\n",
       "      <td>t3_ajis4z</td>\n",
       "      <td>t1_eew18eq</td>\n",
       "      <td>1.548381e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&gt;sexuality shouldn’t be a grouping category I...</td>\n",
       "      <td>eemcysk</td>\n",
       "      <td>TheGreen888</td>\n",
       "      <td>unpopularopinion</td>\n",
       "      <td>t3_ai4q37</td>\n",
       "      <td>t3_ai4q37</td>\n",
       "      <td>1.548084e+09</td>\n",
       "      <td>37</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You do right, if you don't care then fuck 'em!</td>\n",
       "      <td>ed2mah1</td>\n",
       "      <td>Labalool</td>\n",
       "      <td>confessions</td>\n",
       "      <td>t3_abru74</td>\n",
       "      <td>t1_ed2m7g7</td>\n",
       "      <td>1.546428e+09</td>\n",
       "      <td>37</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Man I love reddit.</td>\n",
       "      <td>eeibobj</td>\n",
       "      <td>MrsRobertshaw</td>\n",
       "      <td>facepalm</td>\n",
       "      <td>t3_ahulml</td>\n",
       "      <td>t3_ahulml</td>\n",
       "      <td>1.547965e+09</td>\n",
       "      <td>18</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[NAME] was nowhere near them, he was by the Fa...</td>\n",
       "      <td>eda6yn6</td>\n",
       "      <td>American_Fascist713</td>\n",
       "      <td>starwarsspeculation</td>\n",
       "      <td>t3_ackt2f</td>\n",
       "      <td>t1_eda65q2</td>\n",
       "      <td>1.546669e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       id  \\\n",
       "0                                    That game hurt.  eew5j0j   \n",
       "1   >sexuality shouldn’t be a grouping category I...  eemcysk   \n",
       "2     You do right, if you don't care then fuck 'em!  ed2mah1   \n",
       "3                                 Man I love reddit.  eeibobj   \n",
       "4  [NAME] was nowhere near them, he was by the Fa...  eda6yn6   \n",
       "\n",
       "                author            subreddit    link_id   parent_id  \\\n",
       "0                Brdd9                  nrl  t3_ajis4z  t1_eew18eq   \n",
       "1          TheGreen888     unpopularopinion  t3_ai4q37   t3_ai4q37   \n",
       "2             Labalool          confessions  t3_abru74  t1_ed2m7g7   \n",
       "3        MrsRobertshaw             facepalm  t3_ahulml   t3_ahulml   \n",
       "4  American_Fascist713  starwarsspeculation  t3_ackt2f  t1_eda65q2   \n",
       "\n",
       "    created_utc  rater_id  example_very_unclear  admiration  ...  love  \\\n",
       "0  1.548381e+09         1                 False           0  ...     0   \n",
       "1  1.548084e+09        37                  True           0  ...     0   \n",
       "2  1.546428e+09        37                 False           0  ...     0   \n",
       "3  1.547965e+09        18                 False           0  ...     1   \n",
       "4  1.546669e+09         2                 False           0  ...     0   \n",
       "\n",
       "   nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
       "0            0         0      0            0       0        0        1   \n",
       "1            0         0      0            0       0        0        0   \n",
       "2            0         0      0            0       0        0        0   \n",
       "3            0         0      0            0       0        0        0   \n",
       "4            0         0      0            0       0        0        0   \n",
       "\n",
       "   surprise  neutral  \n",
       "0         0        0  \n",
       "1         0        0  \n",
       "2         0        1  \n",
       "3         0        0  \n",
       "4         0        1  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = 'data/full_dataset/goemotions_'\n",
    "OUTPUT_DIR = 'training_data'\n",
    "\n",
    "df1 = pd.read_csv(f'{DATA_PATH}1.csv')\n",
    "df2 = pd.read_csv(f'{DATA_PATH}2.csv')\n",
    "df3 = pd.read_csv(f'{DATA_PATH}3.csv')\n",
    "\n",
    "frames = [df1,df2,df3]\n",
    "\n",
    "df = pd.concat(frames)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ecff46d",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87fb69ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM: https://www.kaggle.com/code/esknight/emotion-classification-final\n",
    "# Function for cleaning text\n",
    "def clean_text(text):\n",
    "    re_number = re.compile('[0-9]+')\n",
    "    re_url = re.compile(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\")\n",
    "    re_tag = re.compile('\\[[A-Z]+\\]')\n",
    "    re_char = re.compile('[^0-9a-zA-Z\\s?!.,:\\'\\\"//]+')\n",
    "    re_char_clean = re.compile('[^0-9a-zA-Z\\s?!.,\\[\\]]')\n",
    "    re_punc = re.compile('[?!,.\\'\\\"]')\n",
    "  \n",
    "    text = re.sub(re_char, \"\", text) # Remove unknown character \n",
    "    text = contractions.fix(text) # Expand contraction\n",
    "    text = re.sub(re_url, ' [url] ', text) # Replace URL with number\n",
    "    text = re.sub(re_char_clean, \"\", text) # Only alphanumeric and punctuations.\n",
    "    #text = re.sub(re_punc, \"\", text) # Remove punctuation.\n",
    "    text = text.lower() # Lower text\n",
    "    text = \" \".join([w for w in text.split(' ') if w != \" \"]) # Remove whitespace\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cce4567c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97623d1a1ba7402b9401d87affa56a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/211225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clean text\n",
    "df['clean_text'] = df['text'].progress_apply(clean_text)\n",
    "\n",
    "# Drop Useless Columns\n",
    "df = df.drop(columns=['id','example_very_unclear','author','subreddit','link_id','parent_id','created_utc','rater_id'])\n",
    "\n",
    "# Reorganize Columns\n",
    "df = df[['clean_text'] + [col for col in df.columns if col not in ['text','clean_text']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98a7282b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise',\n",
       " 'neutral']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View emotions easier\n",
    "emotions = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "482895ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {str(i):label for i, label in enumerate(emotions)}\n",
    "label2id = {label:str(i) for i, label in enumerate(emotions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f2819ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'admiration', '1': 'amusement', '2': 'anger', '3': 'annoyance', '4': 'approval', '5': 'caring', '6': 'confusion', '7': 'curiosity', '8': 'desire', '9': 'disappointment', '10': 'disapproval', '11': 'disgust', '12': 'embarrassment', '13': 'excitement', '14': 'fear', '15': 'gratitude', '16': 'grief', '17': 'joy', '18': 'love', '19': 'nervousness', '20': 'optimism', '21': 'pride', '22': 'realization', '23': 'relief', '24': 'remorse', '25': 'sadness', '26': 'surprise', '27': 'neutral'}\n",
      "{'admiration': '0', 'amusement': '1', 'anger': '2', 'annoyance': '3', 'approval': '4', 'caring': '5', 'confusion': '6', 'curiosity': '7', 'desire': '8', 'disappointment': '9', 'disapproval': '10', 'disgust': '11', 'embarrassment': '12', 'excitement': '13', 'fear': '14', 'gratitude': '15', 'grief': '16', 'joy': '17', 'love': '18', 'nervousness': '19', 'optimism': '20', 'pride': '21', 'realization': '22', 'relief': '23', 'remorse': '24', 'sadness': '25', 'surprise': '26', 'neutral': '27'}\n"
     ]
    }
   ],
   "source": [
    "print(id2label)\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dda44d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>admiration</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>approval</th>\n",
       "      <th>caring</th>\n",
       "      <th>confusion</th>\n",
       "      <th>curiosity</th>\n",
       "      <th>desire</th>\n",
       "      <th>...</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>that game hurt.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sexuality should not be a grouping category i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you do right, if you do not care then fuck them!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>man i love reddit.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>name was nowhere near them, he was by the falc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  admiration  amusement  \\\n",
       "0                                    that game hurt.           0          0   \n",
       "1   sexuality should not be a grouping category i...           0          0   \n",
       "2   you do right, if you do not care then fuck them!           0          0   \n",
       "3                                 man i love reddit.           0          0   \n",
       "4  name was nowhere near them, he was by the falc...           0          0   \n",
       "\n",
       "   anger  annoyance  approval  caring  confusion  curiosity  desire  ...  \\\n",
       "0      0          0         0       0          0          0       0  ...   \n",
       "1      0          0         0       0          0          0       0  ...   \n",
       "2      0          0         0       0          0          0       0  ...   \n",
       "3      0          0         0       0          0          0       0  ...   \n",
       "4      0          0         0       0          0          0       0  ...   \n",
       "\n",
       "   nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
       "0            0         0      0            0       0        0        1   \n",
       "1            0         0      0            0       0        0        0   \n",
       "2            0         0      0            0       0        0        0   \n",
       "3            0         0      0            0       0        0        0   \n",
       "4            0         0      0            0       0        0        0   \n",
       "\n",
       "   surprise  neutral                                             labels  \n",
       "0         0        0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1         0        0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2         0        1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3         0        0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4         0        1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-Hot Encoding all Emotions\n",
    "df[\"labels\"] = df[emotions].values.tolist()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86a5f182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((169228, 30), (41997, 30))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create train / test splits\n",
    "mask = np.random.rand(len(df)) < 0.8\n",
    "df_train = df[mask]\n",
    "df_test = df[~mask]\n",
    "\n",
    "(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6cfca4",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a9c9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotions Visualization by number of cases\n",
    "\n",
    "temp = df[list(emotions)].sum(axis=0) \\\n",
    "    .reset_index() \\\n",
    "    .rename(columns={'index': 'emotion', 0: 'n'}) \\\n",
    "    .sort_values('n', ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.tick_params(axis='x', rotation=90)\n",
    "sns.barplot(data=temp, x='n', \n",
    "            y='emotion',\n",
    "            dodge=False,\n",
    "            ax=ax).set_title('Emotions by number of appearances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3428cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating emotions\n",
    "\n",
    "pos = {'admiration','amusement','approval','caring','desire','excitement','gratitude','joy','love',\n",
    "       'optimism','pride','relief'}\n",
    "neg = {'sadness','fear','embarrassment','disapproval','disappointment','annoyance','anger','nervousness',\n",
    "       'remorse','grief','disgust'}\n",
    "amb= {'realization','surprise','curiosity','confusion','neutral'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0cbe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotions and data vis\n",
    "\n",
    "print(\"Length of data: \", len(df))\n",
    "print(\"Number of emotions: \", len(emotions))\n",
    "print(\"Number of positive emotions: \", len(pos))\n",
    "print(\"Number of negative emotions: \", len(neg))\n",
    "print(\"Number of ambiguous emotions: \", len(amb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3aed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotions dataframe to later on aggregate\n",
    "\n",
    "df_emotion = pd.DataFrame()\n",
    "df_emotion['emotion'] = list(emotions)\n",
    "df_emotion['group'] = ''\n",
    "df_emotion['group'].loc[df_emotion['emotion'].isin(pos)] = 'positive'\n",
    "df_emotion['group'].loc[df_emotion['emotion'].isin(neg)] = 'negative'\n",
    "df_emotion['group'].loc[df_emotion['emotion'].isin(amb)] = 'ambiguous'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bfe9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emotion.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7952ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotions by number of appearences but by group\n",
    "\n",
    "temp = pd.DataFrame()\n",
    "temp['true positive rate'] = df.iloc[:, 3:-1].mean(0)\n",
    "temp['emotion'] = df.columns[3:-1]\n",
    "temp = temp.merge(df_emotion, how='left', on='emotion')\n",
    "temp = temp.sort_values('true positive rate')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "ax.tick_params(axis='x', rotation=90)\n",
    "\n",
    "sns.barplot(x=temp['emotion'], \n",
    "            y=temp['true positive rate'], \n",
    "            hue=temp['group'], \n",
    "            dodge=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d51f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def represent_train_test_balance(train_df,test_df):\n",
    "    # Class representation for train/test DS\n",
    "    train_GO = (train_df.loc[:,list(emotions)].sum(axis=0) / len(train_df)) * 100\n",
    "    test_GO = (test_df.loc[:,list(emotions)].sum(axis=0) / len(test_df)) * 100\n",
    "    \n",
    "    # Unique dataset for visualization purposes\n",
    "    \n",
    "    ds_GO = pd.DataFrame(data=[train_GO, test_GO]).T.reset_index(drop=False)\n",
    "    ds_GO.columns = ['Emotion', 'Train','Test']\n",
    "    ds_GO = ds_GO.sort_values('Train',ascending=False)\n",
    "    ds_GO = ds_GO.melt(id_vars='Emotion', var_name='Dataset', value_vars=['Train','Test'],\n",
    "                      value_name='Percentage')\n",
    "    \n",
    "    # Display dataset\n",
    "    \n",
    "    display(ds_GO.head(10))\n",
    "    \n",
    "    print(\"Graph Visualization\")\n",
    "    \n",
    "    plt.figure(figsize=(20,15))\n",
    "    sns.barplot(x='Percentage', y='Emotion', data=ds_GO, orient='h', hue='Dataset')\n",
    "    plt.title('Percentage of samples per emotion in train and test datasets', fontweight='bold', fontsize=20)\n",
    "    plt.xlabel('Percentage of all samples', fontweight='bold', fontsize=16)\n",
    "    plt.ylabel('Emotions', fontweight='bold', fontsize= 16)\n",
    "    plt.show()\n",
    "represent_train_test_balance(df_train, df_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8818d771",
   "metadata": {},
   "source": [
    "# Tokenization / Encoding / Method Structuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea478700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, DistilBertForSequenceClassification, BertForSequenceClassification, RobertaForSequenceClassification, XLNetForSequenceClassification\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers.models.xlnet.modeling_xlnet import XLNetForSequenceClassificationOutput\n",
    "from torch import nn\n",
    "import random\n",
    "import torch\n",
    "import platform\n",
    "import sys\n",
    "import sklearn as sk\n",
    "from typing import Optional, Union, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b3799bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "43432890",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoEmotionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3edbe207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(df_train, df_test, tokenizer): \n",
    "  # Encodings\n",
    "\n",
    "  train_encodings = tokenizer(df_train[\"clean_text\"].values.tolist(), truncation=True)\n",
    "  test_encodings = tokenizer(df_test[\"clean_text\"].values.tolist(), truncation=True)\n",
    "\n",
    "  # labels / output\n",
    "  train_emotions = df_train[\"labels\"].values.tolist()\n",
    "  test_emotions = df_test[\"labels\"].values.tolist()\n",
    "\n",
    "  train_dataset = GoEmotionDataset(train_encodings, train_emotions)\n",
    "  test_dataset = GoEmotionDataset(test_encodings, test_emotions)\n",
    "  return train_dataset, test_dataset\n",
    "  \n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    y_pred = torch.from_numpy(logits)\n",
    "    y_true = torch.from_numpy(labels)\n",
    "    y_pred = y_pred.sigmoid()\n",
    "    y_pred = y_pred>0.5\n",
    "    y_true = y_true.bool()\n",
    "    acc = (y_pred==y_true).float().mean().item()\n",
    "\n",
    "    return {       \n",
    "      'Accuracy': acc\n",
    "    }\n",
    "    \n",
    "def set_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic=False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "def device_to_use():\n",
    "    has_gpu = torch.cuda.is_available()\n",
    "    has_mps = getattr(torch,'has_mps',False)\n",
    "    device = \"mps\" if getattr(torch,'has_mps',False) \\\n",
    "        else \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    print(f\"Python Platform: {platform.platform()}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    print()\n",
    "    print(f\"Python {sys.version}\")\n",
    "    print(f\"Pandas {pd.__version__}\")\n",
    "    print(f\"Scikit-Learn {sk.__version__}\")\n",
    "    print(\"GPU is\", \"available\" if has_gpu else \"NOT AVAILABLE\")\n",
    "    print(\"MPS (Apple Metal) is\", \"AVAILABLE\" if has_mps else \"NOT AVAILABLE\")\n",
    "    print(f\"Target device is {device}\")\n",
    "    return device\n",
    "\n",
    "def model_train(train_dataset, test_dataset, model, tokenizer, NUM_EPOCHS = 10,batch_size = 16, adam_epsilon_arg = 1e-8, learning_rate_arg = 2e-5, use_mps_device_arg = False, model_name = \"default\"):\n",
    "  training_args = TrainingArguments( \n",
    "    output_dir= OUTPUT_DIR+\"/\"+model_name,    \n",
    "    adam_epsilon = adam_epsilon_arg,\n",
    "    learning_rate = learning_rate_arg,\n",
    "    use_mps_device = use_mps_device_arg, # Mac Sylicon GPU\n",
    "    per_device_train_batch_size = batch_size, \n",
    "    per_device_eval_batch_size = batch_size*4,\n",
    "    gradient_accumulation_steps = 2, # scale batch size without needing more memory\n",
    "    num_train_epochs= NUM_EPOCHS,\n",
    "    do_eval = True,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    save_strategy = 'epoch',\n",
    "    load_best_model_at_end = True, # this allows to automatically get the best model at the end based on whatever metric we want\n",
    "    metric_for_best_model = 'Accuracy',\n",
    "    greater_is_better = True,\n",
    "    weight_decay=0.01,\n",
    "    seed = 25,\n",
    "    report_to=\"none\"\n",
    "  )\n",
    "  set_seed(training_args.seed)\n",
    "  trainer = Trainer(\n",
    "      model = model,\n",
    "      args = training_args,\n",
    "      train_dataset = train_dataset,\n",
    "      eval_dataset=test_dataset,\n",
    "      compute_metrics=compute_metrics,\n",
    "      tokenizer=tokenizer\n",
    "  )\n",
    "  return training_args, trainer\n",
    "                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "093bf286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes to Each Model\n",
    "\n",
    "class DistilBertForMultilabelSequenceClassification(DistilBertForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "      super().__init__(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[SequenceClassifierOutput, Tuple[torch.Tensor, ...]]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        distilbert_output = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n",
    "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
    "        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n",
    "        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)\n",
    "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
    "        logits = self.classifier(pooled_output)  # (bs, num_labels)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), \n",
    "                            labels.float().view(-1, self.num_labels))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + distilbert_output[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=distilbert_output.hidden_states,\n",
    "            attentions=distilbert_output.attentions)\n",
    "\n",
    "class BertForMultilabelSequenceClassification(BertForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "      super().__init__(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        bert_output = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = bert_output[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), \n",
    "                            labels.float().view(-1, self.num_labels))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + bert_output[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=bert_output.hidden_states,\n",
    "            attentions=bert_output.attentions)\n",
    "\n",
    "class RoBertaForMultilabelSequenceClassification(RobertaForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "      super().__init__(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        roberta_output = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = roberta_output[0]\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), \n",
    "                            labels.float().view(-1, self.num_labels))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + roberta_output[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=roberta_output.hidden_states,\n",
    "            attentions=roberta_output.attentions)\n",
    "\n",
    "class XLNetForMultilabelSequenceClassification(XLNetForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "      super().__init__(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        mems: Optional[torch.Tensor] = None,\n",
    "        perm_mask: Optional[torch.Tensor] = None,\n",
    "        target_mapping: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        input_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        use_mems: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs,  # delete when `use_cache` is removed in XLNetModel\n",
    "    ) -> Union[Tuple, XLNetForSequenceClassificationOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        xlnet_output = self.transformer(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            mems=mems,\n",
    "            perm_mask=perm_mask,\n",
    "            target_mapping=target_mapping,\n",
    "            token_type_ids=token_type_ids,\n",
    "            input_mask=input_mask,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_mems=use_mems,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            **kwargs)\n",
    "        output = xlnet_output[0]\n",
    "        output = self.sequence_summary(output)\n",
    "        logits = self.logits_proj(output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), \n",
    "                            labels.float().view(-1, self.num_labels))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + xlnet_output[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=xlnet_output.hidden_states,\n",
    "            attentions=xlnet_output.attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5de0b7",
   "metadata": {},
   "source": [
    "# Pre-Trained Model - DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06e24f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: Windows-10-10.0.22621-SP0\n",
      "PyTorch Version: 2.0.0\n",
      "\n",
      "Python 3.10.9 | packaged by Anaconda, Inc. | (main, Mar  8 2023, 10:42:25) [MSC v.1916 64 bit (AMD64)]\n",
      "Pandas 1.5.3\n",
      "Scikit-Learn 1.2.2\n",
      "GPU is available\n",
      "MPS (Apple Metal) is NOT AVAILABLE\n",
      "Target device is gpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForMultilabelSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForMultilabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForMultilabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForMultilabelSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": [\n",
      "    {\n",
      "      \"0\": \"admiration\",\n",
      "      \"1\": \"amusement\",\n",
      "      \"10\": \"disapproval\",\n",
      "      \"11\": \"disgust\",\n",
      "      \"12\": \"embarrassment\",\n",
      "      \"13\": \"excitement\",\n",
      "      \"14\": \"fear\",\n",
      "      \"15\": \"gratitude\",\n",
      "      \"16\": \"grief\",\n",
      "      \"17\": \"joy\",\n",
      "      \"18\": \"love\",\n",
      "      \"19\": \"nervousness\",\n",
      "      \"2\": \"anger\",\n",
      "      \"20\": \"optimism\",\n",
      "      \"21\": \"pride\",\n",
      "      \"22\": \"realization\",\n",
      "      \"23\": \"relief\",\n",
      "      \"24\": \"remorse\",\n",
      "      \"25\": \"sadness\",\n",
      "      \"26\": \"surprise\",\n",
      "      \"27\": \"neutral\",\n",
      "      \"3\": \"annoyance\",\n",
      "      \"4\": \"approval\",\n",
      "      \"5\": \"caring\",\n",
      "      \"6\": \"confusion\",\n",
      "      \"7\": \"curiosity\",\n",
      "      \"8\": \"desire\",\n",
      "      \"9\": \"disappointment\"\n",
      "    }\n",
      "  ],\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"admiration\": \"0\",\n",
      "    \"amusement\": \"1\",\n",
      "    \"anger\": \"2\",\n",
      "    \"annoyance\": \"3\",\n",
      "    \"approval\": \"4\",\n",
      "    \"caring\": \"5\",\n",
      "    \"confusion\": \"6\",\n",
      "    \"curiosity\": \"7\",\n",
      "    \"desire\": \"8\",\n",
      "    \"disappointment\": \"9\",\n",
      "    \"disapproval\": \"10\",\n",
      "    \"disgust\": \"11\",\n",
      "    \"embarrassment\": \"12\",\n",
      "    \"excitement\": \"13\",\n",
      "    \"fear\": \"14\",\n",
      "    \"gratitude\": \"15\",\n",
      "    \"grief\": \"16\",\n",
      "    \"joy\": \"17\",\n",
      "    \"love\": \"18\",\n",
      "    \"nervousness\": \"19\",\n",
      "    \"neutral\": \"27\",\n",
      "    \"optimism\": \"20\",\n",
      "    \"pride\": \"21\",\n",
      "    \"realization\": \"22\",\n",
      "    \"relief\": \"23\",\n",
      "    \"remorse\": \"24\",\n",
      "    \"sadness\": \"25\",\n",
      "    \"surprise\": \"26\"\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.27.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path_or_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)\n",
    "num_labels=len(emotions)\n",
    "device = device_to_use()\n",
    "if device == 'gpu': device = 'cuda'\n",
    "model = DistilBertForMultilabelSequenceClassification.from_pretrained(model_path_or_name, num_labels=num_labels).to(device)\n",
    "model = model_config_ids(model, id2label, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a93aa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test(df_train, df_test, tokenizer)\n",
    "training_args, trainer = model_train(train_dataset, test_dataset, model, tokenizer, NUM_EPOCHS = 3,batch_size = 16, adam_epsilon_arg = 1e-8, learning_rate_arg = 2e-5, use_mps_device_arg = False, model_name = \"distilbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ed43f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4936bfb83a0746158cb53bc63a5b76f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.697327733039856,\n",
       " 'eval_Accuracy': 0.46574118733406067,\n",
       " 'eval_runtime': 22.9739,\n",
       " 'eval_samples_per_second': 1850.969,\n",
       " 'eval_steps_per_second': 28.946}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c506dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f47b97fe4134f12a30949e7893a5d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15876 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2038, 'learning_rate': 1.9370118417737468e-05, 'epoch': 0.09}\n",
      "{'loss': 0.1395, 'learning_rate': 1.874023683547493e-05, 'epoch': 0.19}\n",
      "{'loss': 0.1276, 'learning_rate': 1.8110355253212397e-05, 'epoch': 0.28}\n",
      "{'loss': 0.1216, 'learning_rate': 1.7480473670949864e-05, 'epoch': 0.38}\n",
      "{'loss': 0.1197, 'learning_rate': 1.6850592088687327e-05, 'epoch': 0.47}\n",
      "{'loss': 0.1191, 'learning_rate': 1.6220710506424793e-05, 'epoch': 0.57}\n",
      "{'loss': 0.1168, 'learning_rate': 1.559082892416226e-05, 'epoch': 0.66}\n",
      "{'loss': 0.1168, 'learning_rate': 1.4960947341899723e-05, 'epoch': 0.76}\n",
      "{'loss': 0.117, 'learning_rate': 1.433106575963719e-05, 'epoch': 0.85}\n",
      "{'loss': 0.1158, 'learning_rate': 1.3701184177374655e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23e767cde0f47d9a8dddbaae9997ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/655 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11368750780820847, 'eval_Accuracy': 0.9615535140037537, 'eval_runtime': 23.4143, 'eval_samples_per_second': 1788.137, 'eval_steps_per_second': 27.974, 'epoch': 1.0}\n",
      "{'loss': 0.1139, 'learning_rate': 1.3071302595112122e-05, 'epoch': 1.04}\n",
      "{'loss': 0.1115, 'learning_rate': 1.2441421012849586e-05, 'epoch': 1.13}\n",
      "{'loss': 0.111, 'learning_rate': 1.1811539430587051e-05, 'epoch': 1.23}\n",
      "{'loss': 0.1112, 'learning_rate': 1.1181657848324516e-05, 'epoch': 1.32}\n",
      "{'loss': 0.1116, 'learning_rate': 1.0551776266061982e-05, 'epoch': 1.42}\n",
      "{'loss': 0.111, 'learning_rate': 9.921894683799447e-06, 'epoch': 1.51}\n",
      "{'loss': 0.1113, 'learning_rate': 9.292013101536911e-06, 'epoch': 1.61}\n",
      "{'loss': 0.111, 'learning_rate': 8.662131519274378e-06, 'epoch': 1.7}\n",
      "{'loss': 0.1101, 'learning_rate': 8.032249937011843e-06, 'epoch': 1.79}\n",
      "{'loss': 0.111, 'learning_rate': 7.402368354749307e-06, 'epoch': 1.89}\n",
      "{'loss': 0.1094, 'learning_rate': 6.772486772486773e-06, 'epoch': 1.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd6b3cb8cd743289edbb9424cffdf65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/655 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11142463237047195, 'eval_Accuracy': 0.9618861675262451, 'eval_runtime': 22.0921, 'eval_samples_per_second': 1895.156, 'eval_steps_per_second': 29.649, 'epoch': 2.0}\n",
      "{'loss': 0.1074, 'learning_rate': 6.142605190224238e-06, 'epoch': 2.08}\n",
      "{'loss': 0.1066, 'learning_rate': 5.512723607961704e-06, 'epoch': 2.17}\n",
      "{'loss': 0.1066, 'learning_rate': 4.8828420256991685e-06, 'epoch': 2.27}\n",
      "{'loss': 0.1059, 'learning_rate': 4.252960443436635e-06, 'epoch': 2.36}\n",
      "{'loss': 0.1063, 'learning_rate': 3.6230788611740995e-06, 'epoch': 2.46}\n",
      "{'loss': 0.1065, 'learning_rate': 2.993197278911565e-06, 'epoch': 2.55}\n",
      "{'loss': 0.1058, 'learning_rate': 2.36331569664903e-06, 'epoch': 2.65}\n",
      "{'loss': 0.1065, 'learning_rate': 1.7334341143864955e-06, 'epoch': 2.74}\n",
      "{'loss': 0.1054, 'learning_rate': 1.1035525321239608e-06, 'epoch': 2.83}\n",
      "{'loss': 0.1065, 'learning_rate': 4.736709498614261e-07, 'epoch': 2.93}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f4b34c879c4204bc34b94b8e21f7e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/655 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11119987070560455, 'eval_Accuracy': 0.9619091749191284, 'eval_runtime': 22.9527, 'eval_samples_per_second': 1824.103, 'eval_steps_per_second': 28.537, 'epoch': 3.0}\n",
      "{'train_runtime': 1237.3091, 'train_samples_per_second': 410.626, 'train_steps_per_second': 12.831, 'train_loss': 0.11541589485604807, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15876, training_loss=0.11541589485604807, metrics={'train_runtime': 1237.3091, 'train_samples_per_second': 410.626, 'train_steps_per_second': 12.831, 'train_loss': 0.11541589485604807, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fbac40d",
   "metadata": {},
   "source": [
    "# Pre-Trained Model - BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5b7cabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: Windows-10-10.0.22621-SP0\n",
      "PyTorch Version: 2.0.0\n",
      "\n",
      "Python 3.10.9 | packaged by Anaconda, Inc. | (main, Mar  8 2023, 10:42:25) [MSC v.1916 64 bit (AMD64)]\n",
      "Pandas 1.5.3\n",
      "Scikit-Learn 1.2.2\n",
      "GPU is available\n",
      "MPS (Apple Metal) is NOT AVAILABLE\n",
      "Target device is gpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultilabelSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForMultilabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultilabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultilabelSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": [\n",
      "    {\n",
      "      \"0\": \"admiration\",\n",
      "      \"1\": \"amusement\",\n",
      "      \"10\": \"disapproval\",\n",
      "      \"11\": \"disgust\",\n",
      "      \"12\": \"embarrassment\",\n",
      "      \"13\": \"excitement\",\n",
      "      \"14\": \"fear\",\n",
      "      \"15\": \"gratitude\",\n",
      "      \"16\": \"grief\",\n",
      "      \"17\": \"joy\",\n",
      "      \"18\": \"love\",\n",
      "      \"19\": \"nervousness\",\n",
      "      \"2\": \"anger\",\n",
      "      \"20\": \"optimism\",\n",
      "      \"21\": \"pride\",\n",
      "      \"22\": \"realization\",\n",
      "      \"23\": \"relief\",\n",
      "      \"24\": \"remorse\",\n",
      "      \"25\": \"sadness\",\n",
      "      \"26\": \"surprise\",\n",
      "      \"27\": \"neutral\",\n",
      "      \"3\": \"annoyance\",\n",
      "      \"4\": \"approval\",\n",
      "      \"5\": \"caring\",\n",
      "      \"6\": \"confusion\",\n",
      "      \"7\": \"curiosity\",\n",
      "      \"8\": \"desire\",\n",
      "      \"9\": \"disappointment\"\n",
      "    }\n",
      "  ],\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"admiration\": \"0\",\n",
      "    \"amusement\": \"1\",\n",
      "    \"anger\": \"2\",\n",
      "    \"annoyance\": \"3\",\n",
      "    \"approval\": \"4\",\n",
      "    \"caring\": \"5\",\n",
      "    \"confusion\": \"6\",\n",
      "    \"curiosity\": \"7\",\n",
      "    \"desire\": \"8\",\n",
      "    \"disappointment\": \"9\",\n",
      "    \"disapproval\": \"10\",\n",
      "    \"disgust\": \"11\",\n",
      "    \"embarrassment\": \"12\",\n",
      "    \"excitement\": \"13\",\n",
      "    \"fear\": \"14\",\n",
      "    \"gratitude\": \"15\",\n",
      "    \"grief\": \"16\",\n",
      "    \"joy\": \"17\",\n",
      "    \"love\": \"18\",\n",
      "    \"nervousness\": \"19\",\n",
      "    \"neutral\": \"27\",\n",
      "    \"optimism\": \"20\",\n",
      "    \"pride\": \"21\",\n",
      "    \"realization\": \"22\",\n",
      "    \"relief\": \"23\",\n",
      "    \"remorse\": \"24\",\n",
      "    \"sadness\": \"25\",\n",
      "    \"surprise\": \"26\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path_or_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)\n",
    "num_labels=len(emotions)\n",
    "device = device_to_use()\n",
    "if device == 'gpu': device = 'cuda'\n",
    "model = BertForMultilabelSequenceClassification.from_pretrained(model_path_or_name, num_labels=num_labels).to(device)\n",
    "model = model_config_ids(model, id2label, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c934fc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test(df_train, df_test, tokenizer)\n",
    "training_args, trainer = model_train(train_dataset, test_dataset, model, tokenizer, NUM_EPOCHS = 3,batch_size = 16, adam_epsilon_arg = 1e-8, learning_rate_arg = 2e-5, use_mps_device_arg = False, model_name = \"bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b9a9d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "766f15ab065d4c4980cc444642fb676c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15816 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1975, 'learning_rate': 1.9367728882144666e-05, 'epoch': 0.09}\n",
      "{'loss': 0.1422, 'learning_rate': 1.8735457764289327e-05, 'epoch': 0.19}\n",
      "{'loss': 0.1312, 'learning_rate': 1.810318664643399e-05, 'epoch': 0.28}\n",
      "{'loss': 0.1254, 'learning_rate': 1.7470915528578656e-05, 'epoch': 0.38}\n",
      "{'loss': 0.1208, 'learning_rate': 1.683864441072332e-05, 'epoch': 0.47}\n",
      "{'loss': 0.1187, 'learning_rate': 1.620637329286798e-05, 'epoch': 0.57}\n",
      "{'loss': 0.1185, 'learning_rate': 1.5574102175012646e-05, 'epoch': 0.66}\n",
      "{'loss': 0.1166, 'learning_rate': 1.494183105715731e-05, 'epoch': 0.76}\n",
      "{'loss': 0.1157, 'learning_rate': 1.4309559939301973e-05, 'epoch': 0.85}\n",
      "{'loss': 0.1146, 'learning_rate': 1.3677288821446637e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9eb907068d0409aa38475dbdc6f7b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11361443996429443, 'eval_Accuracy': 0.9616317749023438, 'eval_runtime': 43.0879, 'eval_samples_per_second': 986.912, 'eval_steps_per_second': 15.434, 'epoch': 1.0}\n",
      "{'loss': 0.1129, 'learning_rate': 1.30450177035913e-05, 'epoch': 1.04}\n",
      "{'loss': 0.111, 'learning_rate': 1.2412746585735965e-05, 'epoch': 1.14}\n",
      "{'loss': 0.1118, 'learning_rate': 1.1780475467880627e-05, 'epoch': 1.23}\n",
      "{'loss': 0.1106, 'learning_rate': 1.1148204350025292e-05, 'epoch': 1.33}\n",
      "{'loss': 0.1096, 'learning_rate': 1.0515933232169954e-05, 'epoch': 1.42}\n",
      "{'loss': 0.1093, 'learning_rate': 9.883662114314619e-06, 'epoch': 1.52}\n",
      "{'loss': 0.109, 'learning_rate': 9.251390996459283e-06, 'epoch': 1.61}\n",
      "{'loss': 0.1102, 'learning_rate': 8.619119878603946e-06, 'epoch': 1.71}\n",
      "{'loss': 0.1082, 'learning_rate': 7.98684876074861e-06, 'epoch': 1.8}\n",
      "{'loss': 0.1093, 'learning_rate': 7.354577642893273e-06, 'epoch': 1.9}\n",
      "{'loss': 0.1092, 'learning_rate': 6.722306525037937e-06, 'epoch': 1.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "246770b07b1e4aadbe37eeeedbab0616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11117535829544067, 'eval_Accuracy': 0.9620071649551392, 'eval_runtime': 41.9832, 'eval_samples_per_second': 1012.881, 'eval_steps_per_second': 15.84, 'epoch': 2.0}\n",
      "{'loss': 0.1044, 'learning_rate': 6.0900354071826e-06, 'epoch': 2.09}\n",
      "{'loss': 0.1047, 'learning_rate': 5.457764289327264e-06, 'epoch': 2.18}\n",
      "{'loss': 0.1041, 'learning_rate': 4.8254931714719275e-06, 'epoch': 2.28}\n",
      "{'loss': 0.1054, 'learning_rate': 4.193222053616591e-06, 'epoch': 2.37}\n",
      "{'loss': 0.1061, 'learning_rate': 3.5609509357612546e-06, 'epoch': 2.47}\n",
      "{'loss': 0.1049, 'learning_rate': 2.9286798179059182e-06, 'epoch': 2.56}\n",
      "{'loss': 0.1038, 'learning_rate': 2.296408700050582e-06, 'epoch': 2.66}\n",
      "{'loss': 0.1041, 'learning_rate': 1.6641375821952454e-06, 'epoch': 2.75}\n",
      "{'loss': 0.1032, 'learning_rate': 1.031866464339909e-06, 'epoch': 2.85}\n",
      "{'loss': 0.1046, 'learning_rate': 3.9959534648457263e-07, 'epoch': 2.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23478ed76f5d48729818432eabf4ab7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11119233071804047, 'eval_Accuracy': 0.9620088338851929, 'eval_runtime': 42.837, 'eval_samples_per_second': 992.693, 'eval_steps_per_second': 15.524, 'epoch': 3.0}\n",
      "{'train_runtime': 2243.6504, 'train_samples_per_second': 225.571, 'train_steps_per_second': 7.049, 'train_loss': 0.11455539506556231, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15816, training_loss=0.11455539506556231, metrics={'train_runtime': 2243.6504, 'train_samples_per_second': 225.571, 'train_steps_per_second': 7.049, 'train_loss': 0.11455539506556231, 'epoch': 3.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deb8a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1c51500",
   "metadata": {},
   "source": [
    "# Pre-Trained Model - RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "640c5d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: Windows-10-10.0.22621-SP0\n",
      "PyTorch Version: 2.0.0\n",
      "\n",
      "Python 3.10.9 | packaged by Anaconda, Inc. | (main, Mar  8 2023, 10:42:25) [MSC v.1916 64 bit (AMD64)]\n",
      "Pandas 1.5.3\n",
      "Scikit-Learn 1.2.2\n",
      "GPU is available\n",
      "MPS (Apple Metal) is NOT AVAILABLE\n",
      "Target device is gpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RoBertaForMultilabelSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RoBertaForMultilabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RoBertaForMultilabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RoBertaForMultilabelSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": [\n",
      "    {\n",
      "      \"0\": \"admiration\",\n",
      "      \"1\": \"amusement\",\n",
      "      \"10\": \"disapproval\",\n",
      "      \"11\": \"disgust\",\n",
      "      \"12\": \"embarrassment\",\n",
      "      \"13\": \"excitement\",\n",
      "      \"14\": \"fear\",\n",
      "      \"15\": \"gratitude\",\n",
      "      \"16\": \"grief\",\n",
      "      \"17\": \"joy\",\n",
      "      \"18\": \"love\",\n",
      "      \"19\": \"nervousness\",\n",
      "      \"2\": \"anger\",\n",
      "      \"20\": \"optimism\",\n",
      "      \"21\": \"pride\",\n",
      "      \"22\": \"realization\",\n",
      "      \"23\": \"relief\",\n",
      "      \"24\": \"remorse\",\n",
      "      \"25\": \"sadness\",\n",
      "      \"26\": \"surprise\",\n",
      "      \"27\": \"neutral\",\n",
      "      \"3\": \"annoyance\",\n",
      "      \"4\": \"approval\",\n",
      "      \"5\": \"caring\",\n",
      "      \"6\": \"confusion\",\n",
      "      \"7\": \"curiosity\",\n",
      "      \"8\": \"desire\",\n",
      "      \"9\": \"disappointment\"\n",
      "    }\n",
      "  ],\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"admiration\": \"0\",\n",
      "    \"amusement\": \"1\",\n",
      "    \"anger\": \"2\",\n",
      "    \"annoyance\": \"3\",\n",
      "    \"approval\": \"4\",\n",
      "    \"caring\": \"5\",\n",
      "    \"confusion\": \"6\",\n",
      "    \"curiosity\": \"7\",\n",
      "    \"desire\": \"8\",\n",
      "    \"disappointment\": \"9\",\n",
      "    \"disapproval\": \"10\",\n",
      "    \"disgust\": \"11\",\n",
      "    \"embarrassment\": \"12\",\n",
      "    \"excitement\": \"13\",\n",
      "    \"fear\": \"14\",\n",
      "    \"gratitude\": \"15\",\n",
      "    \"grief\": \"16\",\n",
      "    \"joy\": \"17\",\n",
      "    \"love\": \"18\",\n",
      "    \"nervousness\": \"19\",\n",
      "    \"neutral\": \"27\",\n",
      "    \"optimism\": \"20\",\n",
      "    \"pride\": \"21\",\n",
      "    \"realization\": \"22\",\n",
      "    \"relief\": \"23\",\n",
      "    \"remorse\": \"24\",\n",
      "    \"sadness\": \"25\",\n",
      "    \"surprise\": \"26\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path_or_name = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)\n",
    "num_labels=len(emotions)\n",
    "device = device_to_use()\n",
    "if device == 'gpu': device = 'cuda'\n",
    "model = RoBertaForMultilabelSequenceClassification.from_pretrained(model_path_or_name, num_labels=num_labels).to(device)\n",
    "model = model_config_ids(model, id2label, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7e8c30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test(df_train, df_test, tokenizer)\n",
    "training_args, trainer = model_train(train_dataset, test_dataset, model, tokenizer, NUM_EPOCHS = 3,batch_size = 16, adam_epsilon_arg = 1e-8, learning_rate_arg = 2e-5, use_mps_device_arg = False, model_name = \"roberta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3fd5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "881319ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b99b5f6a1a48b997d23666bd0d503b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15816 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1841, 'learning_rate': 1.9367728882144666e-05, 'epoch': 0.09}\n",
      "{'loss': 0.1384, 'learning_rate': 1.8735457764289327e-05, 'epoch': 0.19}\n",
      "{'loss': 0.1264, 'learning_rate': 1.810318664643399e-05, 'epoch': 0.28}\n",
      "{'loss': 0.1233, 'learning_rate': 1.7470915528578656e-05, 'epoch': 0.38}\n",
      "{'loss': 0.1208, 'learning_rate': 1.683864441072332e-05, 'epoch': 0.47}\n",
      "{'loss': 0.1185, 'learning_rate': 1.620637329286798e-05, 'epoch': 0.57}\n",
      "{'loss': 0.118, 'learning_rate': 1.5574102175012646e-05, 'epoch': 0.66}\n",
      "{'loss': 0.1164, 'learning_rate': 1.494183105715731e-05, 'epoch': 0.76}\n",
      "{'loss': 0.1164, 'learning_rate': 1.4309559939301973e-05, 'epoch': 0.85}\n",
      "{'loss': 0.1151, 'learning_rate': 1.3677288821446637e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c7ff7b73e1d48508fe6753d9dc659ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1133279800415039, 'eval_Accuracy': 0.9615587592124939, 'eval_runtime': 39.2707, 'eval_samples_per_second': 1082.867, 'eval_steps_per_second': 16.934, 'epoch': 1.0}\n",
      "{'loss': 0.1125, 'learning_rate': 1.30450177035913e-05, 'epoch': 1.04}\n",
      "{'loss': 0.1131, 'learning_rate': 1.2412746585735965e-05, 'epoch': 1.14}\n",
      "{'loss': 0.1107, 'learning_rate': 1.1780475467880627e-05, 'epoch': 1.23}\n",
      "{'loss': 0.1112, 'learning_rate': 1.1148204350025292e-05, 'epoch': 1.33}\n",
      "{'loss': 0.111, 'learning_rate': 1.0515933232169954e-05, 'epoch': 1.42}\n",
      "{'loss': 0.1113, 'learning_rate': 9.883662114314619e-06, 'epoch': 1.52}\n",
      "{'loss': 0.1105, 'learning_rate': 9.251390996459283e-06, 'epoch': 1.61}\n",
      "{'loss': 0.1109, 'learning_rate': 8.619119878603946e-06, 'epoch': 1.71}\n",
      "{'loss': 0.1107, 'learning_rate': 7.98684876074861e-06, 'epoch': 1.8}\n",
      "{'loss': 0.1107, 'learning_rate': 7.354577642893273e-06, 'epoch': 1.9}\n",
      "{'loss': 0.1104, 'learning_rate': 6.722306525037937e-06, 'epoch': 1.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f684c2d4dd7b4d8397ca75b61aed59ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11133845895528793, 'eval_Accuracy': 0.9617611765861511, 'eval_runtime': 41.1991, 'eval_samples_per_second': 1032.182, 'eval_steps_per_second': 16.141, 'epoch': 2.0}\n",
      "{'loss': 0.1071, 'learning_rate': 6.0900354071826e-06, 'epoch': 2.09}\n",
      "{'loss': 0.1074, 'learning_rate': 5.457764289327264e-06, 'epoch': 2.18}\n",
      "{'loss': 0.1058, 'learning_rate': 4.8254931714719275e-06, 'epoch': 2.28}\n",
      "{'loss': 0.1059, 'learning_rate': 4.193222053616591e-06, 'epoch': 2.37}\n",
      "{'loss': 0.1079, 'learning_rate': 3.5609509357612546e-06, 'epoch': 2.47}\n",
      "{'loss': 0.106, 'learning_rate': 2.9286798179059182e-06, 'epoch': 2.56}\n",
      "{'loss': 0.1065, 'learning_rate': 2.296408700050582e-06, 'epoch': 2.66}\n",
      "{'loss': 0.1063, 'learning_rate': 1.6641375821952454e-06, 'epoch': 2.75}\n",
      "{'loss': 0.1053, 'learning_rate': 1.031866464339909e-06, 'epoch': 2.85}\n",
      "{'loss': 0.1066, 'learning_rate': 3.9959534648457263e-07, 'epoch': 2.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20cb5f508b394062ad075c8404b9094c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11095210909843445, 'eval_Accuracy': 0.9616267681121826, 'eval_runtime': 41.1004, 'eval_samples_per_second': 1034.661, 'eval_steps_per_second': 16.18, 'epoch': 3.0}\n",
      "{'train_runtime': 2354.0718, 'train_samples_per_second': 214.989, 'train_steps_per_second': 6.719, 'train_loss': 0.114850394461694, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15816, training_loss=0.114850394461694, metrics={'train_runtime': 2354.0718, 'train_samples_per_second': 214.989, 'train_steps_per_second': 6.719, 'train_loss': 0.114850394461694, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "562ca376",
   "metadata": {},
   "source": [
    "# Pre-Trained Model - XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f30c042a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: Windows-10-10.0.22621-SP0\n",
      "PyTorch Version: 2.0.0\n",
      "\n",
      "Python 3.10.9 | packaged by Anaconda, Inc. | (main, Mar  8 2023, 10:42:25) [MSC v.1916 64 bit (AMD64)]\n",
      "Pandas 1.5.3\n",
      "Scikit-Learn 1.2.2\n",
      "GPU is available\n",
      "MPS (Apple Metal) is NOT AVAILABLE\n",
      "Target device is gpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForMultilabelSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForMultilabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForMultilabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForMultilabelSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLNetConfig {\n",
      "  \"_name_or_path\": \"xlnet-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"XLNetLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_type\": \"bi\",\n",
      "  \"bi_data\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"clamp_len\": -1,\n",
      "  \"d_head\": 64,\n",
      "  \"d_inner\": 3072,\n",
      "  \"d_model\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"end_n_top\": 5,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ff_activation\": \"gelu\",\n",
      "  \"id2label\": [\n",
      "    {\n",
      "      \"0\": \"admiration\",\n",
      "      \"1\": \"amusement\",\n",
      "      \"10\": \"disapproval\",\n",
      "      \"11\": \"disgust\",\n",
      "      \"12\": \"embarrassment\",\n",
      "      \"13\": \"excitement\",\n",
      "      \"14\": \"fear\",\n",
      "      \"15\": \"gratitude\",\n",
      "      \"16\": \"grief\",\n",
      "      \"17\": \"joy\",\n",
      "      \"18\": \"love\",\n",
      "      \"19\": \"nervousness\",\n",
      "      \"2\": \"anger\",\n",
      "      \"20\": \"optimism\",\n",
      "      \"21\": \"pride\",\n",
      "      \"22\": \"realization\",\n",
      "      \"23\": \"relief\",\n",
      "      \"24\": \"remorse\",\n",
      "      \"25\": \"sadness\",\n",
      "      \"26\": \"surprise\",\n",
      "      \"27\": \"neutral\",\n",
      "      \"3\": \"annoyance\",\n",
      "      \"4\": \"approval\",\n",
      "      \"5\": \"caring\",\n",
      "      \"6\": \"confusion\",\n",
      "      \"7\": \"curiosity\",\n",
      "      \"8\": \"desire\",\n",
      "      \"9\": \"disappointment\"\n",
      "    }\n",
      "  ],\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"admiration\": \"0\",\n",
      "    \"amusement\": \"1\",\n",
      "    \"anger\": \"2\",\n",
      "    \"annoyance\": \"3\",\n",
      "    \"approval\": \"4\",\n",
      "    \"caring\": \"5\",\n",
      "    \"confusion\": \"6\",\n",
      "    \"curiosity\": \"7\",\n",
      "    \"desire\": \"8\",\n",
      "    \"disappointment\": \"9\",\n",
      "    \"disapproval\": \"10\",\n",
      "    \"disgust\": \"11\",\n",
      "    \"embarrassment\": \"12\",\n",
      "    \"excitement\": \"13\",\n",
      "    \"fear\": \"14\",\n",
      "    \"gratitude\": \"15\",\n",
      "    \"grief\": \"16\",\n",
      "    \"joy\": \"17\",\n",
      "    \"love\": \"18\",\n",
      "    \"nervousness\": \"19\",\n",
      "    \"neutral\": \"27\",\n",
      "    \"optimism\": \"20\",\n",
      "    \"pride\": \"21\",\n",
      "    \"realization\": \"22\",\n",
      "    \"relief\": \"23\",\n",
      "    \"remorse\": \"24\",\n",
      "    \"sadness\": \"25\",\n",
      "    \"surprise\": \"26\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mem_len\": null,\n",
      "  \"model_type\": \"xlnet\",\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"pad_token_id\": 5,\n",
      "  \"reuse_len\": null,\n",
      "  \"same_length\": false,\n",
      "  \"start_n_top\": 5,\n",
      "  \"summary_activation\": \"tanh\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"last\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 250\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.27.2\",\n",
      "  \"untie_r\": true,\n",
      "  \"use_mems_eval\": true,\n",
      "  \"use_mems_train\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path_or_name = \"xlnet-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)\n",
    "num_labels=len(emotions)\n",
    "device = device_to_use()\n",
    "if device == 'gpu': device = 'cuda'\n",
    "model = XLNetForMultilabelSequenceClassification.from_pretrained(model_path_or_name, num_labels=num_labels).to(device)\n",
    "model = model_config_ids(model, id2label, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "905b8aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = train_test(df_train, df_test, tokenizer)\n",
    "training_args, trainer = model_train(train_dataset, test_dataset, model, tokenizer, NUM_EPOCHS = 3,batch_size = 8, adam_epsilon_arg = 1e-8, learning_rate_arg = 2e-5, use_mps_device_arg = False, model_name = \"xlnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6849aa6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e6df60e5714bc8abe8a7aed3993d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31710 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLNetTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1741, 'learning_rate': 1.968464206874803e-05, 'epoch': 0.05}\n",
      "{'loss': 0.1453, 'learning_rate': 1.936928413749606e-05, 'epoch': 0.09}\n",
      "{'loss': 0.1325, 'learning_rate': 1.9053926206244088e-05, 'epoch': 0.14}\n",
      "{'loss': 0.127, 'learning_rate': 1.8738568274992116e-05, 'epoch': 0.19}\n",
      "{'loss': 0.1238, 'learning_rate': 1.8423210343740147e-05, 'epoch': 0.24}\n",
      "{'loss': 0.1235, 'learning_rate': 1.8107852412488174e-05, 'epoch': 0.28}\n",
      "{'loss': 0.1212, 'learning_rate': 1.7792494481236205e-05, 'epoch': 0.33}\n",
      "{'loss': 0.1188, 'learning_rate': 1.7477136549984233e-05, 'epoch': 0.38}\n",
      "{'loss': 0.1196, 'learning_rate': 1.7161778618732264e-05, 'epoch': 0.43}\n",
      "{'loss': 0.1193, 'learning_rate': 1.6846420687480292e-05, 'epoch': 0.47}\n",
      "{'loss': 0.1187, 'learning_rate': 1.653106275622832e-05, 'epoch': 0.52}\n",
      "{'loss': 0.1172, 'learning_rate': 1.621570482497635e-05, 'epoch': 0.57}\n",
      "{'loss': 0.1178, 'learning_rate': 1.5900346893724378e-05, 'epoch': 0.61}\n",
      "{'loss': 0.1166, 'learning_rate': 1.5584988962472406e-05, 'epoch': 0.66}\n",
      "{'loss': 0.1176, 'learning_rate': 1.5269631031220437e-05, 'epoch': 0.71}\n",
      "{'loss': 0.1158, 'learning_rate': 1.4954273099968466e-05, 'epoch': 0.76}\n",
      "{'loss': 0.116, 'learning_rate': 1.4638915168716494e-05, 'epoch': 0.8}\n",
      "{'loss': 0.1154, 'learning_rate': 1.4323557237464523e-05, 'epoch': 0.85}\n",
      "{'loss': 0.1153, 'learning_rate': 1.4008199306212553e-05, 'epoch': 0.9}\n",
      "{'loss': 0.1153, 'learning_rate': 1.3692841374960582e-05, 'epoch': 0.95}\n",
      "{'loss': 0.1142, 'learning_rate': 1.337748344370861e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a6671e2a8ee4725afe0cae7b53e74c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11334612220525742, 'eval_Accuracy': 0.9615048170089722, 'eval_runtime': 58.1165, 'eval_samples_per_second': 724.372, 'eval_steps_per_second': 22.644, 'epoch': 1.0}\n",
      "{'loss': 0.1108, 'learning_rate': 1.306212551245664e-05, 'epoch': 1.04}\n",
      "{'loss': 0.11, 'learning_rate': 1.2746767581204668e-05, 'epoch': 1.09}\n",
      "{'loss': 0.1114, 'learning_rate': 1.2431409649952698e-05, 'epoch': 1.14}\n",
      "{'loss': 0.1102, 'learning_rate': 1.2116051718700725e-05, 'epoch': 1.18}\n",
      "{'loss': 0.1105, 'learning_rate': 1.1800693787448756e-05, 'epoch': 1.23}\n",
      "{'loss': 0.1092, 'learning_rate': 1.1485335856196784e-05, 'epoch': 1.28}\n",
      "{'loss': 0.1104, 'learning_rate': 1.1169977924944813e-05, 'epoch': 1.32}\n",
      "{'loss': 0.112, 'learning_rate': 1.0854619993692843e-05, 'epoch': 1.37}\n",
      "{'loss': 0.1102, 'learning_rate': 1.0539262062440872e-05, 'epoch': 1.42}\n",
      "{'loss': 0.111, 'learning_rate': 1.02239041311889e-05, 'epoch': 1.47}\n",
      "{'loss': 0.1102, 'learning_rate': 9.908546199936929e-06, 'epoch': 1.51}\n",
      "{'loss': 0.1097, 'learning_rate': 9.593188268684958e-06, 'epoch': 1.56}\n",
      "{'loss': 0.11, 'learning_rate': 9.277830337432988e-06, 'epoch': 1.61}\n",
      "{'loss': 0.1103, 'learning_rate': 8.962472406181017e-06, 'epoch': 1.66}\n",
      "{'loss': 0.1096, 'learning_rate': 8.647114474929045e-06, 'epoch': 1.7}\n",
      "{'loss': 0.109, 'learning_rate': 8.331756543677074e-06, 'epoch': 1.75}\n",
      "{'loss': 0.1103, 'learning_rate': 8.016398612425103e-06, 'epoch': 1.8}\n",
      "{'loss': 0.1102, 'learning_rate': 7.701040681173133e-06, 'epoch': 1.84}\n",
      "{'loss': 0.1088, 'learning_rate': 7.385682749921161e-06, 'epoch': 1.89}\n",
      "{'loss': 0.1095, 'learning_rate': 7.07032481866919e-06, 'epoch': 1.94}\n",
      "{'loss': 0.1077, 'learning_rate': 6.754966887417219e-06, 'epoch': 1.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a22eb0a09715482db512743b53b009c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11161205172538757, 'eval_Accuracy': 0.9617423415184021, 'eval_runtime': 58.8264, 'eval_samples_per_second': 715.631, 'eval_steps_per_second': 22.371, 'epoch': 2.0}\n",
      "{'loss': 0.1071, 'learning_rate': 6.4396089561652485e-06, 'epoch': 2.03}\n",
      "{'loss': 0.1049, 'learning_rate': 6.124251024913277e-06, 'epoch': 2.08}\n",
      "{'loss': 0.104, 'learning_rate': 5.808893093661306e-06, 'epoch': 2.13}\n",
      "{'loss': 0.1039, 'learning_rate': 5.493535162409335e-06, 'epoch': 2.18}\n",
      "{'loss': 0.106, 'learning_rate': 5.178177231157364e-06, 'epoch': 2.22}\n",
      "{'loss': 0.1057, 'learning_rate': 4.862819299905393e-06, 'epoch': 2.27}\n",
      "{'loss': 0.1053, 'learning_rate': 4.547461368653422e-06, 'epoch': 2.32}\n",
      "{'loss': 0.1046, 'learning_rate': 4.232103437401451e-06, 'epoch': 2.37}\n",
      "{'loss': 0.1043, 'learning_rate': 3.91674550614948e-06, 'epoch': 2.41}\n",
      "{'loss': 0.1045, 'learning_rate': 3.6013875748975093e-06, 'epoch': 2.46}\n",
      "{'loss': 0.1042, 'learning_rate': 3.286029643645538e-06, 'epoch': 2.51}\n",
      "{'loss': 0.1042, 'learning_rate': 2.9706717123935667e-06, 'epoch': 2.55}\n",
      "{'loss': 0.1064, 'learning_rate': 2.6553137811415957e-06, 'epoch': 2.6}\n",
      "{'loss': 0.1041, 'learning_rate': 2.339955849889625e-06, 'epoch': 2.65}\n",
      "{'loss': 0.1022, 'learning_rate': 2.024597918637654e-06, 'epoch': 2.7}\n",
      "{'loss': 0.103, 'learning_rate': 1.7092399873856829e-06, 'epoch': 2.74}\n",
      "{'loss': 0.1042, 'learning_rate': 1.3938820561337118e-06, 'epoch': 2.79}\n",
      "{'loss': 0.1043, 'learning_rate': 1.078524124881741e-06, 'epoch': 2.84}\n",
      "{'loss': 0.1041, 'learning_rate': 7.631661936297699e-07, 'epoch': 2.89}\n",
      "{'loss': 0.1043, 'learning_rate': 4.4780826237779886e-07, 'epoch': 2.93}\n",
      "{'loss': 0.1044, 'learning_rate': 1.3245033112582784e-07, 'epoch': 2.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c65576c0828448893faa2970ed7b599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11138853430747986, 'eval_Accuracy': 0.9615607857704163, 'eval_runtime': 58.8364, 'eval_samples_per_second': 715.509, 'eval_steps_per_second': 22.367, 'epoch': 3.0}\n",
      "{'train_runtime': 5901.5161, 'train_samples_per_second': 85.975, 'train_steps_per_second': 5.373, 'train_loss': 0.11250060360276033, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=31710, training_loss=0.11250060360276033, metrics={'train_runtime': 5901.5161, 'train_samples_per_second': 85.975, 'train_steps_per_second': 5.373, 'train_loss': 0.11250060360276033, 'epoch': 3.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b41285f4",
   "metadata": {},
   "source": [
    "# Transformer from Scratch - All you need!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71838691",
   "metadata": {},
   "source": [
    "Tokenizer - Still want to try some new ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f80cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from transformer import MultilabelSequenceClassificationTransformer\n",
    "# Imports\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, DistilBertForSequenceClassification, BertForSequenceClassification, RobertaForSequenceClassification, XLNetForSequenceClassification\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers.models.xlnet.modeling_xlnet import XLNetForSequenceClassificationOutput\n",
    "from torch import nn\n",
    "import random\n",
    "import torch\n",
    "import platform\n",
    "import sys\n",
    "import sklearn as sk\n",
    "from typing import Optional, Union, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47e0e4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98a2cea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoEmotionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7cb923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_to_ix(df_train, df_test):\n",
    "    word_to_ix = {'<pad>': 0, '<unk>': 1}\n",
    "    for text in pd.concat([df_train[\"clean_text\"], df_test[\"clean_text\"]]):\n",
    "        for token in spacy_tokenizer(text):\n",
    "            if token not in word_to_ix:\n",
    "                word_to_ix[token] = len(word_to_ix)\n",
    "    return word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af7dd7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text, word_to_ix, max_length=128):\n",
    "    tokens = [t for t in spacy_tokenizer(text)]\n",
    "    input_ids = [word_to_ix.get(token, word_to_ix['<unk>']) for token in tokens][:max_length]\n",
    "    input_ids = input_ids + [0] * (max_length - len(input_ids))\n",
    "    attention_mask = [1 if token_id != 0 else 0 for token_id in input_ids]\n",
    "\n",
    "    return {\n",
    "        'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "        'attention_mask': torch.tensor(attention_mask, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c93436f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(df_train, df_test, tokenizer, word_to_ix):\n",
    "    # Encodings\n",
    "    train_encoded_texts = [encode_text(text, word_to_ix) for text in df_train[\"clean_text\"].values.tolist()]\n",
    "    test_encoded_texts = [encode_text(text, word_to_ix) for text in df_test[\"clean_text\"].values.tolist()]\n",
    "\n",
    "    train_encodings = {\n",
    "        'input_ids': [text_encoding['input_ids'] for text_encoding in train_encoded_texts],\n",
    "        'attention_mask': [text_encoding['attention_mask'] for text_encoding in train_encoded_texts]\n",
    "    }\n",
    "\n",
    "    test_encodings = {\n",
    "        'input_ids': [text_encoding['input_ids'] for text_encoding in test_encoded_texts],\n",
    "        'attention_mask': [text_encoding['attention_mask'] for text_encoding in test_encoded_texts]\n",
    "    }\n",
    "\n",
    "    # labels / output\n",
    "    train_emotions = df_train[\"labels\"].values.tolist()\n",
    "    test_emotions = df_test[\"labels\"].values.tolist()\n",
    "\n",
    "    train_dataset = GoEmotionDataset(train_encodings, train_emotions)\n",
    "    test_dataset = GoEmotionDataset(test_encodings, test_emotions)\n",
    "\n",
    "    return train_dataset, test_dataset, len(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17465856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, val_dataset, epochs, batch_size, device, lr=0.001):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            loss, _ = model(input_ids, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                loss, _ = model(input_ids, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Time: {elapsed_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e63f6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = create_word_to_ix(df_train, df_test)\n",
    "train_dataset, test_dataset, vocab_size = train_test(df_train, df_test, spacy_tokenizer, word_to_ix)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device =torch.device('cpu')\n",
    "src_pad_idx = word_to_ix['<pad>']\n",
    "model = MultilabelSequenceClassificationTransformer(\n",
    "    src_vocab_size= vocab_size,\n",
    "    num_classes= len(emotions),\n",
    "    src_pad_idx= src_pad_idx,\n",
    "    max_len=512\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4b8b4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 4\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5db3abd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, train_dataset, test_dataset, epochs, batch_size, device, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760fbeae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "37f16e38780d10c35648b3bd09ed40a738946f51fc07e16d845fbaf5897e263f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
