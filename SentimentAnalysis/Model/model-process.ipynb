{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d72859ab",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87742597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Dataset\n",
    "#Kaggle: https://www.kaggle.com/datasets/shivamb/go-emotions-google-emotions-dataset\n",
    "\n",
    "!wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_1.csv\n",
    "!wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_2.csv\n",
    "!wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_3.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b83dc85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 12 16:33:34 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 531.14                 Driver Version: 531.14       CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 Ti    WDDM | 00000000:29:00.0  On |                  N/A |\n",
      "| 35%   39C    P0               43W / 200W|    290MiB /  8192MiB |      4%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1184    C+G   ...61.0_x64__8wekyb3d8bbwe\\GameBar.exe    N/A      |\n",
      "|    0   N/A  N/A      2996    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A      5600    C+G   ...__8wekyb3d8bbwe\\Notepad\\Notepad.exe    N/A      |\n",
      "|    0   N/A  N/A      8984    C+G   ...Programs\\Microsoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A      9300    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A     10612    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     10756    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     11328    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A     11352    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     12120    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     12940    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     13388    C+G   ...al\\Discord\\app-1.0.9012\\Discord.exe    N/A      |\n",
      "|    0   N/A  N/A     13960    C+G   ...on\\112.0.1722.39\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     14112    C+G   ...siveControlPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     14188    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     14484    C+G   ...oogle\\Chrome\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     14720    C+G   ...23.0_x86__zpdnekdrzrea0\\Spotify.exe    N/A      |\n",
      "|    0   N/A  N/A     16108    C+G   ...__8wekyb3d8bbwe\\WindowsTerminal.exe    N/A      |\n",
      "|    0   N/A  N/A     17428    C+G   ...Desktop\\app-3.2.1\\GitHubDesktop.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "630a3245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import preprocessor\n",
    "import contractions\n",
    "import json\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "914f4fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>rater_id</th>\n",
       "      <th>example_very_unclear</th>\n",
       "      <th>admiration</th>\n",
       "      <th>...</th>\n",
       "      <th>love</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That game hurt.</td>\n",
       "      <td>eew5j0j</td>\n",
       "      <td>Brdd9</td>\n",
       "      <td>nrl</td>\n",
       "      <td>t3_ajis4z</td>\n",
       "      <td>t1_eew18eq</td>\n",
       "      <td>1.548381e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&gt;sexuality shouldnâ€™t be a grouping category I...</td>\n",
       "      <td>eemcysk</td>\n",
       "      <td>TheGreen888</td>\n",
       "      <td>unpopularopinion</td>\n",
       "      <td>t3_ai4q37</td>\n",
       "      <td>t3_ai4q37</td>\n",
       "      <td>1.548084e+09</td>\n",
       "      <td>37</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You do right, if you don't care then fuck 'em!</td>\n",
       "      <td>ed2mah1</td>\n",
       "      <td>Labalool</td>\n",
       "      <td>confessions</td>\n",
       "      <td>t3_abru74</td>\n",
       "      <td>t1_ed2m7g7</td>\n",
       "      <td>1.546428e+09</td>\n",
       "      <td>37</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Man I love reddit.</td>\n",
       "      <td>eeibobj</td>\n",
       "      <td>MrsRobertshaw</td>\n",
       "      <td>facepalm</td>\n",
       "      <td>t3_ahulml</td>\n",
       "      <td>t3_ahulml</td>\n",
       "      <td>1.547965e+09</td>\n",
       "      <td>18</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[NAME] was nowhere near them, he was by the Fa...</td>\n",
       "      <td>eda6yn6</td>\n",
       "      <td>American_Fascist713</td>\n",
       "      <td>starwarsspeculation</td>\n",
       "      <td>t3_ackt2f</td>\n",
       "      <td>t1_eda65q2</td>\n",
       "      <td>1.546669e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       id  \\\n",
       "0                                    That game hurt.  eew5j0j   \n",
       "1   >sexuality shouldnâ€™t be a grouping category I...  eemcysk   \n",
       "2     You do right, if you don't care then fuck 'em!  ed2mah1   \n",
       "3                                 Man I love reddit.  eeibobj   \n",
       "4  [NAME] was nowhere near them, he was by the Fa...  eda6yn6   \n",
       "\n",
       "                author            subreddit    link_id   parent_id  \\\n",
       "0                Brdd9                  nrl  t3_ajis4z  t1_eew18eq   \n",
       "1          TheGreen888     unpopularopinion  t3_ai4q37   t3_ai4q37   \n",
       "2             Labalool          confessions  t3_abru74  t1_ed2m7g7   \n",
       "3        MrsRobertshaw             facepalm  t3_ahulml   t3_ahulml   \n",
       "4  American_Fascist713  starwarsspeculation  t3_ackt2f  t1_eda65q2   \n",
       "\n",
       "    created_utc  rater_id  example_very_unclear  admiration  ...  love  \\\n",
       "0  1.548381e+09         1                 False           0  ...     0   \n",
       "1  1.548084e+09        37                  True           0  ...     0   \n",
       "2  1.546428e+09        37                 False           0  ...     0   \n",
       "3  1.547965e+09        18                 False           0  ...     1   \n",
       "4  1.546669e+09         2                 False           0  ...     0   \n",
       "\n",
       "   nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
       "0            0         0      0            0       0        0        1   \n",
       "1            0         0      0            0       0        0        0   \n",
       "2            0         0      0            0       0        0        0   \n",
       "3            0         0      0            0       0        0        0   \n",
       "4            0         0      0            0       0        0        0   \n",
       "\n",
       "   surprise  neutral  \n",
       "0         0        0  \n",
       "1         0        0  \n",
       "2         0        1  \n",
       "3         0        0  \n",
       "4         0        1  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = 'data/full_dataset/goemotions_'\n",
    "OUTPUT_DIR = 'training_data'\n",
    "\n",
    "df1 = pd.read_csv(f'{DATA_PATH}1.csv')\n",
    "df2 = pd.read_csv(f'{DATA_PATH}2.csv')\n",
    "df3 = pd.read_csv(f'{DATA_PATH}3.csv')\n",
    "\n",
    "frames = [df1,df2,df3]\n",
    "\n",
    "df = pd.concat(frames)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ecff46d",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87fb69ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM: https://www.kaggle.com/code/esknight/emotion-classification-final\n",
    "# Function for cleaning text\n",
    "def clean_text(text):\n",
    "    re_number = re.compile('[0-9]+')\n",
    "    re_url = re.compile(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\")\n",
    "    re_tag = re.compile('\\[[A-Z]+\\]')\n",
    "    re_char = re.compile('[^0-9a-zA-Z\\s?!.,:\\'\\\"//]+')\n",
    "    re_char_clean = re.compile('[^0-9a-zA-Z\\s?!.,\\[\\]]')\n",
    "    re_punc = re.compile('[?!,.\\'\\\"]')\n",
    "  \n",
    "    text = re.sub(re_char, \"\", text) # Remove unknown character \n",
    "    text = contractions.fix(text) # Expand contraction\n",
    "    text = re.sub(re_url, ' [url] ', text) # Replace URL with number\n",
    "    text = re.sub(re_char_clean, \"\", text) # Only alphanumeric and punctuations.\n",
    "    #text = re.sub(re_punc, \"\", text) # Remove punctuation.\n",
    "    text = text.lower() # Lower text\n",
    "    text = \" \".join([w for w in text.split(' ') if w != \" \"]) # Remove whitespace\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cce4567c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6791bbfa18ca46aa8503867df6f1a5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/211225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clean text\n",
    "df['clean_text'] = df['text'].progress_apply(clean_text)\n",
    "\n",
    "# Drop Useless Columns\n",
    "df = df.drop(columns=['id','example_very_unclear','author','subreddit','link_id','parent_id','created_utc','rater_id'])\n",
    "\n",
    "# Reorganize Columns\n",
    "df = df[['clean_text'] + [col for col in df.columns if col not in ['text','clean_text']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98a7282b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise',\n",
       " 'neutral']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View emotions easier\n",
    "emotions = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "482895ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {str(i):label for i, label in enumerate(emotions)}\n",
    "label2id = {label:str(i) for i, label in enumerate(emotions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f2819ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'admiration', '1': 'amusement', '2': 'anger', '3': 'annoyance', '4': 'approval', '5': 'caring', '6': 'confusion', '7': 'curiosity', '8': 'desire', '9': 'disappointment', '10': 'disapproval', '11': 'disgust', '12': 'embarrassment', '13': 'excitement', '14': 'fear', '15': 'gratitude', '16': 'grief', '17': 'joy', '18': 'love', '19': 'nervousness', '20': 'optimism', '21': 'pride', '22': 'realization', '23': 'relief', '24': 'remorse', '25': 'sadness', '26': 'surprise', '27': 'neutral'}\n",
      "{'admiration': '0', 'amusement': '1', 'anger': '2', 'annoyance': '3', 'approval': '4', 'caring': '5', 'confusion': '6', 'curiosity': '7', 'desire': '8', 'disappointment': '9', 'disapproval': '10', 'disgust': '11', 'embarrassment': '12', 'excitement': '13', 'fear': '14', 'gratitude': '15', 'grief': '16', 'joy': '17', 'love': '18', 'nervousness': '19', 'optimism': '20', 'pride': '21', 'realization': '22', 'relief': '23', 'remorse': '24', 'sadness': '25', 'surprise': '26', 'neutral': '27'}\n"
     ]
    }
   ],
   "source": [
    "print(id2label)\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dda44d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>admiration</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>approval</th>\n",
       "      <th>caring</th>\n",
       "      <th>confusion</th>\n",
       "      <th>curiosity</th>\n",
       "      <th>desire</th>\n",
       "      <th>...</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>that game hurt.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sexuality should not be a grouping category i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you do right, if you do not care then fuck them!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>man i love reddit.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>name was nowhere near them, he was by the falc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  admiration  amusement  \\\n",
       "0                                    that game hurt.           0          0   \n",
       "1   sexuality should not be a grouping category i...           0          0   \n",
       "2   you do right, if you do not care then fuck them!           0          0   \n",
       "3                                 man i love reddit.           0          0   \n",
       "4  name was nowhere near them, he was by the falc...           0          0   \n",
       "\n",
       "   anger  annoyance  approval  caring  confusion  curiosity  desire  ...  \\\n",
       "0      0          0         0       0          0          0       0  ...   \n",
       "1      0          0         0       0          0          0       0  ...   \n",
       "2      0          0         0       0          0          0       0  ...   \n",
       "3      0          0         0       0          0          0       0  ...   \n",
       "4      0          0         0       0          0          0       0  ...   \n",
       "\n",
       "   nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
       "0            0         0      0            0       0        0        1   \n",
       "1            0         0      0            0       0        0        0   \n",
       "2            0         0      0            0       0        0        0   \n",
       "3            0         0      0            0       0        0        0   \n",
       "4            0         0      0            0       0        0        0   \n",
       "\n",
       "   surprise  neutral                                             labels  \n",
       "0         0        0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1         0        0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2         0        1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3         0        0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4         0        1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-Hot Encoding all Emotions\n",
    "df[\"labels\"] = df[emotions].values.tolist()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86a5f182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((169054, 30), (42171, 30))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create train / test splits\n",
    "mask = np.random.rand(len(df)) < 0.8\n",
    "df_train = df[mask]\n",
    "df_test = df[~mask]\n",
    "\n",
    "(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6cfca4",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a9c9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotions Visualization by number of cases\n",
    "\n",
    "temp = df[list(emotions)].sum(axis=0) \\\n",
    "    .reset_index() \\\n",
    "    .rename(columns={'index': 'emotion', 0: 'n'}) \\\n",
    "    .sort_values('n', ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.tick_params(axis='x', rotation=90)\n",
    "sns.barplot(data=temp, x='n', \n",
    "            y='emotion',\n",
    "            dodge=False,\n",
    "            ax=ax).set_title('Emotions by number of appearances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3428cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating emotions\n",
    "\n",
    "pos = {'admiration','amusement','approval','caring','desire','excitement','gratitude','joy','love',\n",
    "       'optimism','pride','relief'}\n",
    "neg = {'sadness','fear','embarrassment','disapproval','disappointment','annoyance','anger','nervousness',\n",
    "       'remorse','grief','disgust'}\n",
    "amb= {'realization','surprise','curiosity','confusion','neutral'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0cbe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotions and data vis\n",
    "\n",
    "print(\"Length of data: \", len(df))\n",
    "print(\"Number of emotions: \", len(emotions))\n",
    "print(\"Number of positive emotions: \", len(pos))\n",
    "print(\"Number of negative emotions: \", len(neg))\n",
    "print(\"Number of ambiguous emotions: \", len(amb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3aed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotions dataframe to later on aggregate\n",
    "\n",
    "df_emotion = pd.DataFrame()\n",
    "df_emotion['emotion'] = list(emotions)\n",
    "df_emotion['group'] = ''\n",
    "df_emotion['group'].loc[df_emotion['emotion'].isin(pos)] = 'positive'\n",
    "df_emotion['group'].loc[df_emotion['emotion'].isin(neg)] = 'negative'\n",
    "df_emotion['group'].loc[df_emotion['emotion'].isin(amb)] = 'ambiguous'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bfe9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emotion.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7952ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotions by number of appearences but by group\n",
    "\n",
    "temp = pd.DataFrame()\n",
    "temp['true positive rate'] = df.iloc[:, 3:-1].mean(0)\n",
    "temp['emotion'] = df.columns[3:-1]\n",
    "temp = temp.merge(df_emotion, how='left', on='emotion')\n",
    "temp = temp.sort_values('true positive rate')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "ax.tick_params(axis='x', rotation=90)\n",
    "\n",
    "sns.barplot(x=temp['emotion'], \n",
    "            y=temp['true positive rate'], \n",
    "            hue=temp['group'], \n",
    "            dodge=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d51f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def represent_train_test_balance(train_df,test_df):\n",
    "    # Class representation for train/test DS\n",
    "    train_GO = (train_df.loc[:,list(emotions)].sum(axis=0) / len(train_df)) * 100\n",
    "    test_GO = (test_df.loc[:,list(emotions)].sum(axis=0) / len(test_df)) * 100\n",
    "    \n",
    "    # Unique dataset for visualization purposes\n",
    "    \n",
    "    ds_GO = pd.DataFrame(data=[train_GO, test_GO]).T.reset_index(drop=False)\n",
    "    ds_GO.columns = ['Emotion', 'Train','Test']\n",
    "    ds_GO = ds_GO.sort_values('Train',ascending=False)\n",
    "    ds_GO = ds_GO.melt(id_vars='Emotion', var_name='Dataset', value_vars=['Train','Test'],\n",
    "                      value_name='Percentage')\n",
    "    \n",
    "    # Display dataset\n",
    "    \n",
    "    display(ds_GO.head(10))\n",
    "    \n",
    "    print(\"Graph Visualization\")\n",
    "    \n",
    "    plt.figure(figsize=(20,15))\n",
    "    sns.barplot(x='Percentage', y='Emotion', data=ds_GO, orient='h', hue='Dataset')\n",
    "    plt.title('Percentage of samples per emotion in train and test datasets', fontweight='bold', fontsize=20)\n",
    "    plt.xlabel('Percentage of all samples', fontweight='bold', fontsize=16)\n",
    "    plt.ylabel('Emotions', fontweight='bold', fontsize= 16)\n",
    "    plt.show()\n",
    "represent_train_test_balance(df_train, df_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8818d771",
   "metadata": {},
   "source": [
    "# Tokenization / Encoding / Method Structuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea478700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, DistilBertForSequenceClassification, BertForSequenceClassification, RobertaForSequenceClassification, XLNetForSequenceClassification\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers.models.xlnet.modeling_xlnet import XLNetForSequenceClassificationOutput\n",
    "from torch import nn\n",
    "import random\n",
    "import torch\n",
    "import platform\n",
    "import sys\n",
    "import sklearn as sk\n",
    "from typing import Optional, Union, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3799bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43432890",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoEmotionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: self.encodings[key][idx].clone().detach() for key, val in self.encodings}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edbe207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(df_train, df_test, tokenizer): \n",
    "  # Encodings\n",
    "\n",
    "  train_encodings = tokenizer(df_train[\"clean_text\"].values.tolist(), truncation=True)\n",
    "  test_encodings = tokenizer(df_test[\"clean_text\"].values.tolist(), truncation=True)\n",
    "\n",
    "  # labels / output\n",
    "  train_emotions = df_train[\"labels\"].values.tolist()\n",
    "  test_emotions = df_test[\"labels\"].values.tolist()\n",
    "\n",
    "  train_dataset = GoEmotionDataset(train_encodings, train_emotions)\n",
    "  test_dataset = GoEmotionDataset(test_encodings, test_emotions)\n",
    "  return train_dataset, test_dataset\n",
    "  \n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    y_pred = torch.from_numpy(logits)\n",
    "    y_true = torch.from_numpy(labels)\n",
    "    y_pred = y_pred.sigmoid()\n",
    "    y_pred = y_pred>0.5\n",
    "    y_true = y_true.bool()\n",
    "    acc = (y_pred==y_true).float().mean().item()\n",
    "\n",
    "    return {       \n",
    "      'Accuracy': acc\n",
    "    }\n",
    "    \n",
    "def set_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic=False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "def device_to_use():\n",
    "    has_gpu = torch.cuda.is_available()\n",
    "    has_mps = getattr(torch,'has_mps',False)\n",
    "    device = \"mps\" if getattr(torch,'has_mps',False) \\\n",
    "        else \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    print(f\"Python Platform: {platform.platform()}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    print()\n",
    "    print(f\"Python {sys.version}\")\n",
    "    print(f\"Pandas {pd.__version__}\")\n",
    "    print(f\"Scikit-Learn {sk.__version__}\")\n",
    "    print(\"GPU is\", \"available\" if has_gpu else \"NOT AVAILABLE\")\n",
    "    print(\"MPS (Apple Metal) is\", \"AVAILABLE\" if has_mps else \"NOT AVAILABLE\")\n",
    "    print(f\"Target device is {device}\")\n",
    "    return device\n",
    "\n",
    "def model_train(train_dataset, test_dataset, model, tokenizer, NUM_EPOCHS = 10,batch_size = 16, adam_epsilon_arg = 1e-8, learning_rate_arg = 2e-5, use_mps_device_arg = False, model_name = \"default\"):\n",
    "  training_args = TrainingArguments( \n",
    "    output_dir= OUTPUT_DIR+\"/\"+model_name,    \n",
    "    adam_epsilon = adam_epsilon_arg,\n",
    "    learning_rate = learning_rate_arg,\n",
    "    use_mps_device = use_mps_device_arg, # Mac Sylicon GPU\n",
    "    per_device_train_batch_size = batch_size, \n",
    "    per_device_eval_batch_size = batch_size*4,\n",
    "    gradient_accumulation_steps = 2, # scale batch size without needing more memory\n",
    "    num_train_epochs= NUM_EPOCHS,\n",
    "    do_eval = True,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    save_strategy = 'epoch',\n",
    "    load_best_model_at_end = True, # this allows to automatically get the best model at the end based on whatever metric we want\n",
    "    metric_for_best_model = 'Accuracy',\n",
    "    greater_is_better = True,\n",
    "    weight_decay=0.01,\n",
    "    seed = 25,\n",
    "    report_to=\"none\"\n",
    "  )\n",
    "  set_seed(training_args.seed)\n",
    "  trainer = Trainer(\n",
    "      model = model,\n",
    "      args = training_args,\n",
    "      train_dataset = train_dataset,\n",
    "      eval_dataset=test_dataset,\n",
    "      compute_metrics=compute_metrics,\n",
    "      tokenizer=tokenizer\n",
    "  )\n",
    "  return training_args, trainer\n",
    "                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093bf286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes to Each Model\n",
    "\n",
    "class DistilBertForMultilabelSequenceClassification(DistilBertForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "      super().__init__(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[SequenceClassifierOutput, Tuple[torch.Tensor, ...]]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        distilbert_output = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n",
    "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
    "        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n",
    "        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)\n",
    "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
    "        logits = self.classifier(pooled_output)  # (bs, num_labels)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), \n",
    "                            labels.float().view(-1, self.num_labels))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + distilbert_output[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=distilbert_output.hidden_states,\n",
    "            attentions=distilbert_output.attentions)\n",
    "\n",
    "class BertForMultilabelSequenceClassification(BertForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "      super().__init__(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        bert_output = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = bert_output[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), \n",
    "                            labels.float().view(-1, self.num_labels))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + bert_output[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=bert_output.hidden_states,\n",
    "            attentions=bert_output.attentions)\n",
    "\n",
    "class RoBertaForMultilabelSequenceClassification(RobertaForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "      super().__init__(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        roberta_output = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = roberta_output[0]\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), \n",
    "                            labels.float().view(-1, self.num_labels))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + roberta_output[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=roberta_output.hidden_states,\n",
    "            attentions=roberta_output.attentions)\n",
    "\n",
    "class XLNetForMultilabelSequenceClassification(XLNetForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "      super().__init__(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        mems: Optional[torch.Tensor] = None,\n",
    "        perm_mask: Optional[torch.Tensor] = None,\n",
    "        target_mapping: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        input_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        use_mems: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs,  # delete when `use_cache` is removed in XLNetModel\n",
    "    ) -> Union[Tuple, XLNetForSequenceClassificationOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        xlnet_output = self.transformer(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            mems=mems,\n",
    "            perm_mask=perm_mask,\n",
    "            target_mapping=target_mapping,\n",
    "            token_type_ids=token_type_ids,\n",
    "            input_mask=input_mask,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_mems=use_mems,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            **kwargs)\n",
    "        output = xlnet_output[0]\n",
    "        output = self.sequence_summary(output)\n",
    "        logits = self.logits_proj(output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), \n",
    "                            labels.float().view(-1, self.num_labels))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + xlnet_output[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=xlnet_output.hidden_states,\n",
    "            attentions=xlnet_output.attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5de0b7",
   "metadata": {},
   "source": [
    "# Pre-Trained Model - DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e24f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_or_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)\n",
    "num_labels=len(emotions)\n",
    "device = device_to_use()\n",
    "if device == 'gpu': device = 'cuda'\n",
    "model = DistilBertForMultilabelSequenceClassification.from_pretrained(model_path_or_name, num_labels=num_labels).to(device)\n",
    "model = model_config_ids(model, id2label, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a93aa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test(df_train, df_test, tokenizer)\n",
    "training_args, trainer = model_train(train_dataset, test_dataset, model, tokenizer, NUM_EPOCHS = 3,batch_size = 16, adam_epsilon_arg = 1e-8, learning_rate_arg = 2e-5, use_mps_device_arg = False, model_name = \"distilbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed43f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c506dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fbac40d",
   "metadata": {},
   "source": [
    "# Pre-Trained Model - BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b7cabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_or_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)\n",
    "num_labels=len(emotions)\n",
    "device = device_to_use()\n",
    "if device == 'gpu': device = 'cuda'\n",
    "model = BertForMultilabelSequenceClassification.from_pretrained(model_path_or_name, num_labels=num_labels).to(device)\n",
    "model = model_config_ids(model, id2label, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c934fc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test(df_train, df_test, tokenizer)\n",
    "training_args, trainer = model_train(train_dataset, test_dataset, model, tokenizer, NUM_EPOCHS = 3,batch_size = 16, adam_epsilon_arg = 1e-8, learning_rate_arg = 2e-5, use_mps_device_arg = False, model_name = \"bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9a9d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deb8a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1c51500",
   "metadata": {},
   "source": [
    "# Pre-Trained Model - RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640c5d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_or_name = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)\n",
    "num_labels=len(emotions)\n",
    "device = device_to_use()\n",
    "if device == 'gpu': device = 'cuda'\n",
    "model = RoBertaForMultilabelSequenceClassification.from_pretrained(model_path_or_name, num_labels=num_labels).to(device)\n",
    "model = model_config_ids(model, id2label, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e8c30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test(df_train, df_test, tokenizer)\n",
    "training_args, trainer = model_train(train_dataset, test_dataset, model, tokenizer, NUM_EPOCHS = 3,batch_size = 16, adam_epsilon_arg = 1e-8, learning_rate_arg = 2e-5, use_mps_device_arg = False, model_name = \"roberta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3fd5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881319ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "562ca376",
   "metadata": {},
   "source": [
    "# Pre-Trained Model - XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30c042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_or_name = \"xlnet-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)\n",
    "num_labels=len(emotions)\n",
    "device = device_to_use()\n",
    "if device == 'gpu': device = 'cuda'\n",
    "model = XLNetForMultilabelSequenceClassification.from_pretrained(model_path_or_name, num_labels=num_labels).to(device)\n",
    "model = model_config_ids(model, id2label, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905b8aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test(df_train, df_test, tokenizer)\n",
    "training_args, trainer = model_train(train_dataset, test_dataset, model, tokenizer, NUM_EPOCHS = 3,batch_size = 8, adam_epsilon_arg = 1e-8, learning_rate_arg = 2e-5, use_mps_device_arg = False, model_name = \"xlnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6849aa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b41285f4",
   "metadata": {},
   "source": [
    "# Transformer from Scratch - All you need!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71838691",
   "metadata": {},
   "source": [
    "Tokenizer - Still want to try some new ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f80cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from transformer import MultilabelSequenceClassificationTransformer, MultilabelLocalAttentionSequenceClassificationTransformer\n",
    "# Imports\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, DistilBertForSequenceClassification, BertForSequenceClassification, RobertaForSequenceClassification, XLNetForSequenceClassification\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers.models.xlnet.modeling_xlnet import XLNetForSequenceClassificationOutput\n",
    "from torch import nn\n",
    "import random\n",
    "import torch\n",
    "import platform\n",
    "import sys\n",
    "import sklearn as sk\n",
    "from typing import Optional, Union, Tuple\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47e0e4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98a2cea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoEmotionDatasetScratch(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: self.encodings[key][idx].clone().detach() for key in self.encodings}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7cb923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_to_ix(df_train, df_test):\n",
    "    word_to_ix = {'<pad>': 0, '<unk>': 1}\n",
    "    for text in pd.concat([df_train[\"clean_text\"], df_test[\"clean_text\"]]):\n",
    "        for token in spacy_tokenizer(text):\n",
    "            if token not in word_to_ix:\n",
    "                word_to_ix[token] = len(word_to_ix)\n",
    "    return word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af7dd7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text, word_to_ix, max_length=128):\n",
    "    tokens = [t for t in spacy_tokenizer(text)]\n",
    "    input_ids = [word_to_ix.get(token, word_to_ix['<unk>']) for token in tokens][:max_length]\n",
    "    input_ids = input_ids + [0] * (max_length - len(input_ids))\n",
    "    attention_mask = [1 if token_id != 0 else 0 for token_id in input_ids]\n",
    "\n",
    "    return {\n",
    "        'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "        'attention_mask': torch.tensor(attention_mask, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c93436f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(df_train, df_test, tokenizer, word_to_ix):\n",
    "    # Encodings\n",
    "    train_encoded_texts = [encode_text(text, word_to_ix) for text in df_train[\"clean_text\"].values.tolist()]\n",
    "    test_encoded_texts = [encode_text(text, word_to_ix) for text in df_test[\"clean_text\"].values.tolist()]\n",
    "\n",
    "    train_encodings = {\n",
    "        'input_ids': [text_encoding['input_ids'] for text_encoding in train_encoded_texts],\n",
    "        'attention_mask': [text_encoding['attention_mask'] for text_encoding in train_encoded_texts]\n",
    "    }\n",
    "\n",
    "    test_encodings = {\n",
    "        'input_ids': [text_encoding['input_ids'] for text_encoding in test_encoded_texts],\n",
    "        'attention_mask': [text_encoding['attention_mask'] for text_encoding in test_encoded_texts]\n",
    "    }\n",
    "\n",
    "    # labels / output\n",
    "    train_emotions = df_train[\"labels\"].values.tolist()\n",
    "    test_emotions = df_test[\"labels\"].values.tolist()\n",
    "\n",
    "    train_dataset = GoEmotionDatasetScratch(train_encodings, train_emotions)\n",
    "    test_dataset = GoEmotionDatasetScratch(test_encodings, test_emotions)\n",
    "\n",
    "    return train_dataset, test_dataset, len(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17465856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, val_dataset, epochs, batch_size, device, lr=0.001, weight_decay=0.01, warmup_steps=0):\n",
    "    \n",
    "    # Create DataLoaders for the training and validation datasets\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,pin_memory=True)\n",
    "\n",
    "    # Initialize the optimizer with model parameters, learning rate, and weight decay\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    model.to(device)\n",
    "\n",
    "    # Calculate the total number of training steps and create the learning rate scheduler\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for idx, batch in enumerate(train_loader):\n",
    "\n",
    "            # Reset the gradients for the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            loss, _ = model(input_ids, labels=labels)\n",
    "\n",
    "            # Backward pass to compute gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip gradients to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            # Update model parameters using the optimizer\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the learning rate using the scheduler\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            if (idx + 1) % 500 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{epochs} | Batch {idx + 1}/{len(train_loader)} | Train Loss: {loss.item():.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        num_val_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                loss, logits = model(input_ids, labels=labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                y_pred = logits.cpu()\n",
    "                y_true = labels.cpu()\n",
    "                y_pred = y_pred.sigmoid()\n",
    "                y_pred = y_pred>0.5\n",
    "                y_true = y_true.bool()\n",
    "                acc = (y_pred==y_true).float().mean().item()\n",
    "                val_acc += acc\n",
    "                num_val_batches += 1\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc /= num_val_batches\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Time: {elapsed_time:.2f}s\")\n",
    "\n",
    "    return val_loss, val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ff8d15c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m word_to_ix \u001b[39m=\u001b[39m create_word_to_ix(df_train, df_test)\n\u001b[0;32m      2\u001b[0m train_dataset, test_dataset, vocab_size \u001b[39m=\u001b[39m train_test(df_train, df_test, spacy_tokenizer, word_to_ix)\n\u001b[0;32m      3\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m, in \u001b[0;36mcreate_word_to_ix\u001b[1;34m(df_train, df_test)\u001b[0m\n\u001b[0;32m      2\u001b[0m word_to_ix \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39m<pad>\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m<unk>\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m}\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m pd\u001b[39m.\u001b[39mconcat([df_train[\u001b[39m\"\u001b[39m\u001b[39mclean_text\u001b[39m\u001b[39m\"\u001b[39m], df_test[\u001b[39m\"\u001b[39m\u001b[39mclean_text\u001b[39m\u001b[39m\"\u001b[39m]]):\n\u001b[1;32m----> 4\u001b[0m     \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m spacy_tokenizer(text):\n\u001b[0;32m      5\u001b[0m         \u001b[39mif\u001b[39;00m token \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m word_to_ix:\n\u001b[0;32m      6\u001b[0m             word_to_ix[token] \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(word_to_ix)\n",
      "File \u001b[1;32mc:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torchtext\\data\\utils.py:14\u001b[0m, in \u001b[0;36m_spacy_tokenize\u001b[1;34m(x, spacy)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_spacy_tokenize\u001b[39m(x, spacy):\n\u001b[1;32m---> 14\u001b[0m     \u001b[39mreturn\u001b[39;00m [tok\u001b[39m.\u001b[39mtext \u001b[39mfor\u001b[39;00m tok \u001b[39min\u001b[39;00m spacy\u001b[39m.\u001b[39mtokenizer(x)]\n",
      "File \u001b[1;32mc:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torchtext\\data\\utils.py:14\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_spacy_tokenize\u001b[39m(x, spacy):\n\u001b[1;32m---> 14\u001b[0m     \u001b[39mreturn\u001b[39;00m [tok\u001b[39m.\u001b[39mtext \u001b[39mfor\u001b[39;00m tok \u001b[39min\u001b[39;00m spacy\u001b[39m.\u001b[39mtokenizer(x)]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "word_to_ix = create_word_to_ix(df_train, df_test)\n",
    "train_dataset, test_dataset, vocab_size = train_test(df_train, df_test, spacy_tokenizer, word_to_ix)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "src_pad_idx = word_to_ix['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e63f6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal \n",
    "model2 = MultilabelSequenceClassificationTransformer(\n",
    "    src_vocab_size= vocab_size,\n",
    "    num_classes= len(emotions),\n",
    "    src_pad_idx= src_pad_idx,\n",
    "    emb_size = 128,\n",
    "    max_len=128\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4ef5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with Local Attention\n",
    "model = MultilabelLocalAttentionSequenceClassificationTransformer(\n",
    "    src_vocab_size= vocab_size,\n",
    "    num_classes= len(emotions),\n",
    "    src_pad_idx= src_pad_idx,\n",
    "    emb_size = 128,\n",
    "    max_len=128\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a2cef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "torch.manual_seed(42)\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "train_model(model, train_dataset, test_dataset, epochs, batch_size, device, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9af4fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "torch.manual_seed(42)\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "train_model(model2, train_dataset, test_dataset, epochs, batch_size, device, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229338e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_debug(model, train_dataset, val_dataset, epochs, batch_size, device, lr=0.001, weight_decay=0.01, warmup_steps=0):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,pin_memory=True)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    model.to(device)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        for _, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            loss, logits = model(input_ids, labels=labels)\n",
    "            return loss, logits, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f3d9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To verify everything was correct, which is\n",
    "loss, logits, labels = train_model_debug(model, train_dataset, test_dataset, epochs, batch_size, device, lr)\n",
    "print(logits[0]) # Which will then be normalized\n",
    "print(labels[0])\n",
    "loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "loss = loss_fct(logits[0], labels[0].float())\n",
    "print(loss)\n",
    "loss = loss_fct(logits, labels.float())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15709e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_dataset(dataset):\n",
    "    label_counts = np.zeros(28)\n",
    "    sequence_lengths = []\n",
    "\n",
    "    for idx in range(len(dataset)):\n",
    "        sample = dataset[idx]\n",
    "        input_ids = sample['input_ids']\n",
    "        labels = sample['labels']\n",
    "\n",
    "        sequence_lengths.append(len(input_ids))\n",
    "        label_counts += labels.numpy()\n",
    "\n",
    "        # Check if there are NaN or infinite values in input data\n",
    "    if torch.isnan(input_ids).any() or torch.isinf(input_ids).any():\n",
    "        print(f\"NaN or infinite values found in input_ids at batch {idx + 1}\")\n",
    "\n",
    "    if torch.isnan(labels).any() or torch.isinf(labels).any():\n",
    "        print(f\"NaN or infinite values found in labels at batch {idx + 1}\")\n",
    "\n",
    "    # Normalize label counts\n",
    "    label_counts /= len(dataset)\n",
    "\n",
    "    # Analyze the distribution of sequence lengths\n",
    "    plt.hist(sequence_lengths, bins=50)\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Sequence Lengths')\n",
    "    plt.show()\n",
    "\n",
    "    # Analyze the distribution of labels\n",
    "    plt.bar(range(28), label_counts)\n",
    "    plt.xlabel('Label')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Labels')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Average sequence length: {np.mean(sequence_lengths):.2f}\")\n",
    "    print(f\"Standard deviation of sequence length: {np.std(sequence_lengths):.2f}\")\n",
    "    print(f\"Label frequencies: {label_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c80a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_dataset(train_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3adc3446",
   "metadata": {},
   "source": [
    "# Transformers - Hyper Parameters Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe3ed69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71dfadba",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333db071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_search(data, hyperparameters, device):\n",
    "    results = []\n",
    "\n",
    "    for values in itertools.product(*hyperparameters.values()):\n",
    "        params = dict(zip(hyperparameters.keys(), values))\n",
    "        print(f\"Training with hyperparameters: {params}\")\n",
    "\n",
    "        # Create a new model with the current hyperparameters\n",
    "        model = MultilabelSequenceClassificationTransformer(\n",
    "            src_vocab_size=vocab_size,\n",
    "            num_classes=len(emotions),\n",
    "            src_pad_idx=src_pad_idx,\n",
    "            emb_size=params[\"emb_size\"],\n",
    "            num_layers=params[\"num_layers\"],\n",
    "            forward_expansion=params[\"forward_expansion\"],\n",
    "            heads=params[\"heads\"],\n",
    "            dropout=params[\"dropout\"],\n",
    "            device=device,\n",
    "            max_len=MAX_LEN\n",
    "        ).to(device)\n",
    "\n",
    "        # Train the model and get the validation loss\n",
    "        val_loss, val_acc = train_model(\n",
    "            model, train_dataset, test_dataset, epochs=5, batch_size = params[\"batch_size\"], device = device, lr=params[\"lr\"], weight_decay=params['weight_decay'], warmup_steps=0\n",
    "        )\n",
    "\n",
    "        # Save the results\n",
    "        results.append({\"params\": params, \"val_loss\": val_loss, \"val_acc\": val_acc})\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9db5fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample = df.sample(frac=0.01, random_state=42)\n",
    "mask = np.random.rand(len(data_sample)) < 0.8\n",
    "df_train_sample = data_sample[mask]\n",
    "df_test_sample = data_sample[~mask]\n",
    "\n",
    "word_to_ix = create_word_to_ix(df_train, df_test)\n",
    "train_dataset, test_dataset, vocab_size = train_test(df_train_sample, df_test_sample, spacy_tokenizer, word_to_ix)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "src_pad_idx = word_to_ix['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede9c326",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"lr\": [1e-5, 1e-3],\n",
    "    \"num_layers\": [6, 12],\n",
    "    \"heads\": [8, 16],\n",
    "    \"dropout\": [0.2],\n",
    "    \"emb_size\": [512],\n",
    "    \"forward_expansion\": [4, 8],\n",
    "    \"batch_size\": [64],\n",
    "    \"weight_decay\": [1e-5,1e-3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a5b23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the hyperparameter search\n",
    "search_results = hyperparameter_search(data_sample, hyperparameters, device)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "best_result = min(search_results, key=lambda x: x[\"val_loss\"])\n",
    "print(f\"Best hyperparameters: {best_result['params']} with validation loss: {best_result['val_loss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff1ab9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = min(results_hyperparam, key=lambda x: x[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fd8486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some new options just to verify some hyper params that weren't searched. Â´\n",
    "# This is because they're not so relevant and were causing problems running by the fact of GPU VRAM limitations\n",
    "# Separated in two dicts in order to work with the 8GB VRAM\n",
    "params_hp_1 = {\n",
    "    \"dropout\": [0.1, 0.4],\n",
    "    \"emb_size\": [128,256]\n",
    "    }\n",
    "params_hp_2 = {\n",
    "    \"batch_size\": [32, 128],\n",
    "    \"weight_decay\":[0]\n",
    "\n",
    "}\n",
    "list_params = []\n",
    "for key, value in params_hp_1.items():\n",
    "    for v in value:\n",
    "        new_params = dict(best_result[\"params\"])\n",
    "        new_params.update({key:v})\n",
    "        list_params.append(new_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "765ea0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_research_ft(params_list, results,device):\n",
    "    for params in params_list:\n",
    "        print(f\"Training with hyperparameters: {params}\")\n",
    "        # Create a new model with the current hyperparameters\n",
    "        model = MultilabelSequenceClassificationTransformer(\n",
    "            src_vocab_size=vocab_size,\n",
    "            num_classes=len(emotions),\n",
    "            src_pad_idx=src_pad_idx,\n",
    "            emb_size=params[\"emb_size\"],\n",
    "            num_layers=params[\"num_layers\"],\n",
    "            forward_expansion=params[\"forward_expansion\"],\n",
    "            heads=params[\"heads\"],\n",
    "            dropout=params[\"dropout\"],\n",
    "            device=device,\n",
    "            max_len=MAX_LEN\n",
    "        ).to(device)\n",
    "\n",
    "        # Train the model and get the validation loss\n",
    "        val_loss, val_acc = train_model(\n",
    "            model, train_dataset, test_dataset, epochs=5, batch_size = params[\"batch_size\"], device = device, lr=params[\"lr\"], weight_decay=params['weight_decay'], warmup_steps=0\n",
    "        )\n",
    "\n",
    "        # Save the results\n",
    "        results.append({\"params\": params, \"val_loss\": val_loss, \"val_acc\": val_acc})\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31d745b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c4cd848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results hyper parameters dictionary\n",
      "[{'params': {'lr': 1e-05, 'num_layers': 6, 'heads': 8, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 4, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.1694728625672204, 'val_acc': 0.9588509287152972}, {'params': {'lr': 1e-05, 'num_layers': 6, 'heads': 8, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 4, 'batch_size': 64, 'weight_decay': 0.001}, 'val_loss': 0.16154434851237706, 'val_acc': 0.9588509287152972}, {'params': {'lr': 1e-05, 'num_layers': 6, 'heads': 8, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.17052964653287614, 'val_acc': 0.9588509287152972}, {'params': {'lr': 1e-05, 'num_layers': 6, 'heads': 8, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 0.001}, 'val_loss': 0.1628226169518062, 'val_acc': 0.9588509287152972}, {'params': {'lr': 1e-05, 'num_layers': 6, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 4, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.1733415573835373, 'val_acc': 0.9588509287152972}, {'params': {'lr': 1e-05, 'num_layers': 6, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 4, 'batch_size': 64, 'weight_decay': 0.001}, 'val_loss': 0.1589777788945607, 'val_acc': 0.9588509287152972}, {'params': {'lr': 1e-05, 'num_layers': 6, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.16529120717729842, 'val_acc': 0.9588509287152972}, {'params': {'lr': 1e-05, 'num_layers': 6, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 0.001}, 'val_loss': 0.1553161612578801, 'val_acc': 0.9588509287152972}, {'params': {'lr': 1e-05, 'num_layers': 12, 'heads': 8, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 4, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.1671474426984787, 'val_acc': 0.9588509287152972}, {'params': {'lr': 1e-05, 'num_layers': 12, 'heads': 8, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 4, 'batch_size': 64, 'weight_decay': 0.001}, 'val_loss': 0.16708412340709142, 'val_acc': 0.9588509287152972}, {'params': {'lr': 1e-05, 'num_layers': 12, 'heads': 8, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.16354919331414358, 'val_acc': 0.9588509287152972}, {'params': {'lr': 1e-05, 'num_layers': 12, 'heads': 8, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 0.001}, 'val_loss': 0.15895125269889832, 'val_acc': 0.9588509287152972}, {'params': {'lr': 1e-05, 'num_layers': 12, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 4, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.16620116787297384, 'val_acc': 0.9588509287152972}, {'params': {'lr': 1e-05, 'num_layers': 12, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 4, 'batch_size': 64, 'weight_decay': 0.001}, 'val_loss': 0.16665744355746678, 'val_acc': 0.9588509287152972}, {'params': {'lr': 1e-05, 'num_layers': 12, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.15971017735345022, 'val_acc': 0.9588509287152972}, {'params': {'lr': 1e-05, 'num_layers': 12, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 0.001}, 'val_loss': 0.16398029242243087, 'val_acc': 0.9588509287152972}, {'params': {'lr': 0.001, 'num_layers': 6, 'heads': 8, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 4, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.15341281252247946, 'val_acc': 0.9588509287152972}, {'params': {'lr': 0.001, 'num_layers': 6, 'heads': 8, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 4, 'batch_size': 64, 'weight_decay': 0.001}, 'val_loss': 0.15339339630944387, 'val_acc': 0.9588509287152972}, {'params': {'lr': 0.001, 'num_layers': 6, 'heads': 8, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.15346688245024, 'val_acc': 0.9588509287152972}, {'params': {'lr': 0.001, 'num_layers': 6, 'heads': 8, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 0.001}, 'val_loss': 0.1532680413552693, 'val_acc': 0.9588509287152972}, {'params': {'lr': 0.001, 'num_layers': 6, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 4, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.15341971814632416, 'val_acc': 0.9588509287152972}, {'params': {'lr': 0.001, 'num_layers': 6, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 4, 'batch_size': 64, 'weight_decay': 0.001}, 'val_loss': 0.15329302421637944, 'val_acc': 0.9588509287152972}, {'params': {'lr': 0.001, 'num_layers': 6, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.1537885240146092, 'val_acc': 0.9588509287152972}, {'params': {'lr': 0.001, 'num_layers': 6, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 0.001}, 'val_loss': 0.15356084917272841, 'val_acc': 0.9588509287152972}, {'params': {'lr': 0.001, 'num_layers': 12, 'heads': 8, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 4, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.1532393800360816, 'val_acc': 0.9588509287152972}, {'params': {'lr': 0.001, 'num_layers': 12, 'heads': 8, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 4, 'batch_size': 64, 'weight_decay': 0.001}, 'val_loss': 0.15336079469748906, 'val_acc': 0.9588509287152972}, {'params': {'lr': 0.001, 'num_layers': 12, 'heads': 8, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.15367268664496286, 'val_acc': 0.9588509287152972}, {'params': {'lr': 0.001, 'num_layers': 12, 'heads': 8, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 0.001}, 'val_loss': 0.15342403522559575, 'val_acc': 0.9588509287152972}, {'params': {'lr': 0.001, 'num_layers': 12, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 4, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.1532111189195088, 'val_acc': 0.9588509287152972}, {'params': {'lr': 0.001, 'num_layers': 12, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 4, 'batch_size': 64, 'weight_decay': 0.001}, 'val_loss': 0.15333122866494314, 'val_acc': 0.9588509287152972}, {'params': {'lr': 0.001, 'num_layers': 12, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.15317635451044356, 'val_acc': 0.9588509287152972}, {'params': {'lr': 0.001, 'num_layers': 12, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 0.001}, 'val_loss': 0.15327459147998265, 'val_acc': 0.9588509287152972}]\n"
     ]
    }
   ],
   "source": [
    "# Load File\n",
    "\n",
    "with open('dict_results_hyperparams.pkl', 'rb') as fp:\n",
    "    results_hyperparam = pickle.load(fp)\n",
    "    print('Results hyper parameters dictionary')\n",
    "    print(results_hyperparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "595aa0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with hyperparameters: {'lr': 0.001, 'num_layers': 12, 'heads': 16, 'dropout': 0.1, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 1e-05}\n",
      "Epoch 1/5 | Train Loss: 0.1968 | Val Loss: 0.1609 | Val Acc: 0.9579 | Time: 35.53s\n",
      "Epoch 2/5 | Train Loss: 0.1612 | Val Loss: 0.1564 | Val Acc: 0.9579 | Time: 32.63s\n",
      "Epoch 3/5 | Train Loss: 0.1601 | Val Loss: 0.1561 | Val Acc: 0.9579 | Time: 32.13s\n",
      "Epoch 4/5 | Train Loss: 0.1595 | Val Loss: 0.1545 | Val Acc: 0.9579 | Time: 33.67s\n",
      "Epoch 5/5 | Train Loss: 0.1590 | Val Loss: 0.1540 | Val Acc: 0.9579 | Time: 33.92s\n",
      "Training with hyperparameters: {'lr': 0.001, 'num_layers': 12, 'heads': 16, 'dropout': 0.4, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 1e-05}\n",
      "Epoch 1/5 | Train Loss: 0.2121 | Val Loss: 0.1569 | Val Acc: 0.9579 | Time: 33.59s\n",
      "Epoch 2/5 | Train Loss: 0.1683 | Val Loss: 0.1558 | Val Acc: 0.9579 | Time: 33.61s\n",
      "Epoch 3/5 | Train Loss: 0.1645 | Val Loss: 0.1547 | Val Acc: 0.9579 | Time: 32.01s\n",
      "Epoch 4/5 | Train Loss: 0.1637 | Val Loss: 0.1547 | Val Acc: 0.9579 | Time: 32.10s\n",
      "Epoch 5/5 | Train Loss: 0.1625 | Val Loss: 0.1542 | Val Acc: 0.9579 | Time: 31.37s\n",
      "Training with hyperparameters: {'lr': 0.001, 'num_layers': 12, 'heads': 16, 'dropout': 0.2, 'emb_size': 128, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 1e-05}\n",
      "Epoch 1/5 | Train Loss: 0.2461 | Val Loss: 0.1587 | Val Acc: 0.9579 | Time: 24.32s\n",
      "Epoch 2/5 | Train Loss: 0.1706 | Val Loss: 0.1543 | Val Acc: 0.9579 | Time: 25.10s\n",
      "Epoch 3/5 | Train Loss: 0.1668 | Val Loss: 0.1547 | Val Acc: 0.9579 | Time: 25.14s\n",
      "Epoch 4/5 | Train Loss: 0.1665 | Val Loss: 0.1541 | Val Acc: 0.9579 | Time: 26.18s\n",
      "Epoch 5/5 | Train Loss: 0.1637 | Val Loss: 0.1540 | Val Acc: 0.9579 | Time: 23.35s\n",
      "Training with hyperparameters: {'lr': 0.001, 'num_layers': 12, 'heads': 16, 'dropout': 0.2, 'emb_size': 256, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 1e-05}\n",
      "Epoch 1/5 | Train Loss: 0.2075 | Val Loss: 0.1575 | Val Acc: 0.9579 | Time: 25.55s\n",
      "Epoch 2/5 | Train Loss: 0.1665 | Val Loss: 0.1553 | Val Acc: 0.9579 | Time: 25.99s\n",
      "Epoch 3/5 | Train Loss: 0.1628 | Val Loss: 0.1557 | Val Acc: 0.9579 | Time: 26.58s\n",
      "Epoch 4/5 | Train Loss: 0.1625 | Val Loss: 0.1542 | Val Acc: 0.9579 | Time: 24.74s\n",
      "Epoch 5/5 | Train Loss: 0.1613 | Val Loss: 0.1542 | Val Acc: 0.9579 | Time: 25.44s\n",
      "Training with hyperparameters: {'lr': 0.001, 'num_layers': 12, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 32, 'weight_decay': 1e-05}\n",
      "Epoch 1/5 | Train Loss: 0.1856 | Val Loss: 0.1572 | Val Acc: 0.9581 | Time: 51.98s\n",
      "Epoch 2/5 | Train Loss: 0.1641 | Val Loss: 0.1556 | Val Acc: 0.9581 | Time: 55.19s\n",
      "Epoch 3/5 | Train Loss: 0.1606 | Val Loss: 0.1541 | Val Acc: 0.9581 | Time: 55.31s\n",
      "Epoch 4/5 | Train Loss: 0.1598 | Val Loss: 0.1540 | Val Acc: 0.9581 | Time: 54.80s\n",
      "Epoch 5/5 | Train Loss: 0.1591 | Val Loss: 0.1539 | Val Acc: 0.9581 | Time: 52.67s\n",
      "Training with hyperparameters: {'lr': 0.001, 'num_layers': 12, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 128, 'weight_decay': 1e-05}\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 8.00 GiB total capacity; 7.04 GiB already allocated; 0 bytes free; 7.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m search_results \u001b[39m=\u001b[39m hyperparameter_research_ft(list_params, results_hyperparam, device)\n",
      "Cell \u001b[1;32mIn[25], line 19\u001b[0m, in \u001b[0;36mhyperparameter_research_ft\u001b[1;34m(params_list, results, device)\u001b[0m\n\u001b[0;32m      5\u001b[0m model \u001b[39m=\u001b[39m MultilabelSequenceClassificationTransformer(\n\u001b[0;32m      6\u001b[0m     src_vocab_size\u001b[39m=\u001b[39mvocab_size,\n\u001b[0;32m      7\u001b[0m     num_classes\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(emotions),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     max_len\u001b[39m=\u001b[39mMAX_LEN\n\u001b[0;32m     16\u001b[0m )\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     18\u001b[0m \u001b[39m# Train the model and get the validation loss\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m val_loss, val_acc \u001b[39m=\u001b[39m train_model(\n\u001b[0;32m     20\u001b[0m     model, train_dataset, test_dataset, epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, batch_size \u001b[39m=\u001b[39;49m params[\u001b[39m\"\u001b[39;49m\u001b[39mbatch_size\u001b[39;49m\u001b[39m\"\u001b[39;49m], device \u001b[39m=\u001b[39;49m device, lr\u001b[39m=\u001b[39;49mparams[\u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m], weight_decay\u001b[39m=\u001b[39;49mparams[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m], warmup_steps\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     23\u001b[0m \u001b[39m# Save the results\u001b[39;00m\n\u001b[0;32m     24\u001b[0m results\u001b[39m.\u001b[39mappend({\u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m: params, \u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m: val_loss, \u001b[39m\"\u001b[39m\u001b[39mval_acc\u001b[39m\u001b[39m\"\u001b[39m: val_acc})\n",
      "Cell \u001b[1;32mIn[17], line 26\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_dataset, val_dataset, epochs, batch_size, device, lr, weight_decay, warmup_steps)\u001b[0m\n\u001b[0;32m     24\u001b[0m input_ids \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     25\u001b[0m labels \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 26\u001b[0m loss, _ \u001b[39m=\u001b[39m model(input_ids, labels\u001b[39m=\u001b[39;49mlabels)\n\u001b[0;32m     28\u001b[0m \u001b[39m# Backward pass to compute gradients\u001b[39;00m\n\u001b[0;32m     29\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\reidp\\Documents\\MEIA-PROJ3\\SentimentAnalysis\\Model\\transformer.py:383\u001b[0m, in \u001b[0;36mMultilabelSequenceClassificationTransformer.forward\u001b[1;34m(self, src, labels)\u001b[0m\n\u001b[0;32m    379\u001b[0m src_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_src_mask(src)\n\u001b[0;32m    380\u001b[0m \u001b[39m#print(\"Src Mask:\", has_nan_or_inf(src_mask))\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \n\u001b[0;32m    382\u001b[0m \u001b[39m# Pass the input sequence through the Encoder\u001b[39;00m\n\u001b[1;32m--> 383\u001b[0m enc_src \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(src, src_mask)\n\u001b[0;32m    384\u001b[0m \u001b[39m#print(\"Encoder output:\", has_nan_or_inf(enc_src))\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[39m# Calculate the mean of the Encoder output along the sequence dimension\u001b[39;00m\n\u001b[0;32m    387\u001b[0m enc_src_mean \u001b[39m=\u001b[39m enc_src\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\reidp\\Documents\\MEIA-PROJ3\\SentimentAnalysis\\Model\\transformer.py:155\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[39m# Pass the input through the TransformerBlock layers\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m--> 155\u001b[0m     out \u001b[39m=\u001b[39m layer(out, out, out, mask)\n\u001b[0;32m    157\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\reidp\\Documents\\MEIA-PROJ3\\SentimentAnalysis\\Model\\transformer.py:102\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, value, key, query, mask)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, value, key, query, mask):\n\u001b[0;32m    101\u001b[0m     \u001b[39m# Calculate self-attention scores and apply them to the input values\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m     attention \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(value, key, query, mask)\n\u001b[0;32m    104\u001b[0m     \u001b[39m# Perform residual connection (add original input to the self-attention output)\u001b[39;00m\n\u001b[0;32m    105\u001b[0m     \u001b[39m# and apply LayerNorm to stabilize the input for the next layer\u001b[39;00m\n\u001b[0;32m    106\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(attention \u001b[39m+\u001b[39m query))\n",
      "File \u001b[1;32mc:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\reidp\\Documents\\MEIA-PROJ3\\SentimentAnalysis\\Model\\transformer.py:53\u001b[0m, in \u001b[0;36mSelfAttention.forward\u001b[1;34m(self, values, keys, queries, mask)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39m#print(\"Energy before mask output:\", has_nan_or_inf(energy))\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \n\u001b[0;32m     50\u001b[0m \u001b[39m# If a mask is provided, apply it to the energy\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39mif\u001b[39;00m mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m     \u001b[39m# Upper left triangle is being changed to 0s in order to prevent insight of the following words / tokens and try to predict them\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     energy \u001b[39m=\u001b[39m energy\u001b[39m.\u001b[39;49mmasked_fill(mask \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m, \u001b[39mfloat\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39m-1e20\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m     54\u001b[0m     \u001b[39m#print(\"Energy after mask output:\", has_nan_or_inf(energy))\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[39m# Normalize the energy by the square root of emb_size and apply softmax\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# Basically (QK.T(energy) / emb_size(dk) ** 0.5 )\u001b[39;00m\n\u001b[0;32m     58\u001b[0m attention \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msoftmax(energy \u001b[39m/\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb_size \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m)), dim\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m) \u001b[39m# Normalizing across key_len\u001b[39;00m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 8.00 GiB total capacity; 7.04 GiB already allocated; 0 bytes free; 7.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "search_results = hyperparameter_research_ft(list_params, results_hyperparam, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bfd8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save File\n",
    "\n",
    "with open('dict_results_hyperparams.pkl', 'wb') as fp:\n",
    "    pickle.dump(search_results, fp)\n",
    "    print('dictionary saved successfully to file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ce2766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load File\n",
    "\n",
    "with open('dict_results_hyperparams.pkl', 'rb') as fp:\n",
    "    results_hyperparam = pickle.load(fp)\n",
    "    print('Results hyper parameters dictionary')\n",
    "    print(results_hyperparam)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dde27b3b",
   "metadata": {},
   "source": [
    "# Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0af36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "BERT_PATH = 'training_data/bert/checkpoint-15816/trainer_state.json'\n",
    "DISTILBERT_PATH = 'training_data/distilbert/checkpoint-15876/trainer_state.json'\n",
    "ROBERTA_PATH = 'training_data/roberta/checkpoint-15816/trainer_state.json'\n",
    "XLNET_PATH = 'training_data/xlnet/checkpoint-31710/trainer_state.json'\n",
    "\n",
    "\n",
    "model_files = [BERT_PATH, DISTILBERT_PATH, ROBERTA_PATH, XLNET_PATH]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_file in model_files:\n",
    "    with open(model_file) as f:\n",
    "        data = json.load(f)\n",
    "        for item in data['log_history']:\n",
    "            if 'epoch' in item and item['epoch'] == 3.0:\n",
    "                results[model_file.split('/')[1]] = item\n",
    "                break\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c77f834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# assuming the 'results' dictionary has been created as in the previous example\n",
    "\n",
    "metrics = ['eval_Accuracy', 'eval_loss', 'eval_runtime']\n",
    "\n",
    "# create a bar chart for accuracy and loss\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(len(model_files))\n",
    "\n",
    "for i, metric in enumerate(metrics[:-1]):\n",
    "    values = [results[model_file.split('/')[1]].get(metric, np.nan) for model_file in model_files]\n",
    "    ax.bar(x + i*0.25 - 0.25, values, width=0.25, label=metric)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([model_file.split('/')[1] for model_file in model_files], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# create a bar chart for runtime (logarithmic scale)\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(len(model_files))\n",
    "\n",
    "values = [results[model_file.split('/')[1]].get('eval_runtime', np.nan) for model_file in model_files]\n",
    "ax.bar(x, values, log=True, color='orange')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([model_file.split('/')[1] for model_file in model_files], rotation=45, ha='right')\n",
    "ax.set_ylabel('eval_runtime (seconds)')\n",
    "ax.set_xlabel('model')\n",
    "ax.set_title('Evaluation Runtime at Epoch 3.0')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7fe95972",
   "metadata": {},
   "source": [
    "# Implementation with own Dataset and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545b40d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "# Import model\n",
    "\n",
    "DISTILBERT_PATH = 'training_data/distilbert/checkpoint-15876'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = DistilBertForMultilabelSequenceClassification.from_pretrained(DISTILBERT_PATH).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(DISTILBERT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f20fcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0021c97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consumer = pd.read_csv('data/consumer_data_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def33bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consumer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eae0193",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consumer['clean_text'] = df_consumer['consumer_complaint'].progress_apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3131ab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consumer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0696ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(text, threshold=0.5):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    outputs= model(**inputs)\n",
    "    logits = outputs.logits.detach().cpu().numpy()[0] # assuming batch size of 1\n",
    "    probs = softmax(logits)\n",
    "    probs_scaled = (probs - np.min(probs)) / (np.max(probs) - np.min(probs))\n",
    "    binary_preds = np.where(probs_scaled >= threshold, 1, 0)\n",
    "    return binary_preds\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec150ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consumer['predicted_label'] = df_consumer['clean_text'].apply(predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b981f69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consumer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d9c410",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = df_consumer['predicted_label'].head(10).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e73170",
   "metadata": {},
   "outputs": [],
   "source": [
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d0bd6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "37f16e38780d10c35648b3bd09ed40a738946f51fc07e16d845fbaf5897e263f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
