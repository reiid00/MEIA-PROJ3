{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d72859ab",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87742597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Dataset\n",
    "#Kaggle: https://www.kaggle.com/datasets/shivamb/go-emotions-google-emotions-dataset\n",
    "\n",
    "!wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_1.csv\n",
    "!wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_2.csv\n",
    "!wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_3.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b83dc85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr 14 20:19:45 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 531.14                 Driver Version: 531.14       CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 Ti    WDDM | 00000000:29:00.0  On |                  N/A |\n",
      "| 30%   33C    P0               42W / 200W|    631MiB /  8192MiB |      1%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      5016    C+G   ...Programs\\Microsoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A      9548    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A     11024    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A     11048    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     11524    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     12996    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     13316    C+G   ...23.0_x86__zpdnekdrzrea0\\Spotify.exe    N/A      |\n",
      "|    0   N/A  N/A     13540    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     14440    C+G   ...oogle\\Chrome\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     15388    C+G   ...siveControlPanel\\SystemSettings.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "630a3245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import preprocessor\n",
    "import contractions\n",
    "import json\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "914f4fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>rater_id</th>\n",
       "      <th>example_very_unclear</th>\n",
       "      <th>admiration</th>\n",
       "      <th>...</th>\n",
       "      <th>love</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That game hurt.</td>\n",
       "      <td>eew5j0j</td>\n",
       "      <td>Brdd9</td>\n",
       "      <td>nrl</td>\n",
       "      <td>t3_ajis4z</td>\n",
       "      <td>t1_eew18eq</td>\n",
       "      <td>1.548381e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&gt;sexuality shouldn’t be a grouping category I...</td>\n",
       "      <td>eemcysk</td>\n",
       "      <td>TheGreen888</td>\n",
       "      <td>unpopularopinion</td>\n",
       "      <td>t3_ai4q37</td>\n",
       "      <td>t3_ai4q37</td>\n",
       "      <td>1.548084e+09</td>\n",
       "      <td>37</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You do right, if you don't care then fuck 'em!</td>\n",
       "      <td>ed2mah1</td>\n",
       "      <td>Labalool</td>\n",
       "      <td>confessions</td>\n",
       "      <td>t3_abru74</td>\n",
       "      <td>t1_ed2m7g7</td>\n",
       "      <td>1.546428e+09</td>\n",
       "      <td>37</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Man I love reddit.</td>\n",
       "      <td>eeibobj</td>\n",
       "      <td>MrsRobertshaw</td>\n",
       "      <td>facepalm</td>\n",
       "      <td>t3_ahulml</td>\n",
       "      <td>t3_ahulml</td>\n",
       "      <td>1.547965e+09</td>\n",
       "      <td>18</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[NAME] was nowhere near them, he was by the Fa...</td>\n",
       "      <td>eda6yn6</td>\n",
       "      <td>American_Fascist713</td>\n",
       "      <td>starwarsspeculation</td>\n",
       "      <td>t3_ackt2f</td>\n",
       "      <td>t1_eda65q2</td>\n",
       "      <td>1.546669e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       id  \\\n",
       "0                                    That game hurt.  eew5j0j   \n",
       "1   >sexuality shouldn’t be a grouping category I...  eemcysk   \n",
       "2     You do right, if you don't care then fuck 'em!  ed2mah1   \n",
       "3                                 Man I love reddit.  eeibobj   \n",
       "4  [NAME] was nowhere near them, he was by the Fa...  eda6yn6   \n",
       "\n",
       "                author            subreddit    link_id   parent_id  \\\n",
       "0                Brdd9                  nrl  t3_ajis4z  t1_eew18eq   \n",
       "1          TheGreen888     unpopularopinion  t3_ai4q37   t3_ai4q37   \n",
       "2             Labalool          confessions  t3_abru74  t1_ed2m7g7   \n",
       "3        MrsRobertshaw             facepalm  t3_ahulml   t3_ahulml   \n",
       "4  American_Fascist713  starwarsspeculation  t3_ackt2f  t1_eda65q2   \n",
       "\n",
       "    created_utc  rater_id  example_very_unclear  admiration  ...  love  \\\n",
       "0  1.548381e+09         1                 False           0  ...     0   \n",
       "1  1.548084e+09        37                  True           0  ...     0   \n",
       "2  1.546428e+09        37                 False           0  ...     0   \n",
       "3  1.547965e+09        18                 False           0  ...     1   \n",
       "4  1.546669e+09         2                 False           0  ...     0   \n",
       "\n",
       "   nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
       "0            0         0      0            0       0        0        1   \n",
       "1            0         0      0            0       0        0        0   \n",
       "2            0         0      0            0       0        0        0   \n",
       "3            0         0      0            0       0        0        0   \n",
       "4            0         0      0            0       0        0        0   \n",
       "\n",
       "   surprise  neutral  \n",
       "0         0        0  \n",
       "1         0        0  \n",
       "2         0        1  \n",
       "3         0        0  \n",
       "4         0        1  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = 'data/full_dataset/goemotions_'\n",
    "OUTPUT_DIR = 'training_data'\n",
    "\n",
    "df1 = pd.read_csv(f'{DATA_PATH}1.csv')\n",
    "df2 = pd.read_csv(f'{DATA_PATH}2.csv')\n",
    "df3 = pd.read_csv(f'{DATA_PATH}3.csv')\n",
    "\n",
    "frames = [df1,df2,df3]\n",
    "\n",
    "df = pd.concat(frames)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ecff46d",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87fb69ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM: https://www.kaggle.com/code/esknight/emotion-classification-final\n",
    "# Function for cleaning text\n",
    "def clean_text(text):\n",
    "    re_number = re.compile('[0-9]+')\n",
    "    re_url = re.compile(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\")\n",
    "    re_tag = re.compile('\\[[A-Z]+\\]')\n",
    "    re_char = re.compile('[^0-9a-zA-Z\\s?!.,:\\'\\\"//]+')\n",
    "    re_char_clean = re.compile('[^0-9a-zA-Z\\s?!.,\\[\\]]')\n",
    "    re_punc = re.compile('[?!,.\\'\\\"]')\n",
    "  \n",
    "    text = re.sub(re_char, \"\", text) # Remove unknown character \n",
    "    text = contractions.fix(text) # Expand contraction\n",
    "    text = re.sub(re_url, ' [url] ', text) # Replace URL with number\n",
    "    text = re.sub(re_char_clean, \"\", text) # Only alphanumeric and punctuations.\n",
    "    #text = re.sub(re_punc, \"\", text) # Remove punctuation.\n",
    "    text = text.lower() # Lower text\n",
    "    text = \" \".join([w for w in text.split(' ') if w != \" \"]) # Remove whitespace\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cce4567c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042db8191343417e9cb6a3d2c55b48de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/211225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clean text\n",
    "df['clean_text'] = df['text'].progress_apply(clean_text)\n",
    "\n",
    "# Drop Useless Columns\n",
    "df = df.drop(columns=['id','example_very_unclear','author','subreddit','link_id','parent_id','created_utc','rater_id'])\n",
    "\n",
    "# Reorganize Columns\n",
    "df = df[['clean_text'] + [col for col in df.columns if col not in ['text','clean_text']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98a7282b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise',\n",
       " 'neutral']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View emotions easier\n",
    "emotions = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "482895ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {str(i):label for i, label in enumerate(emotions)}\n",
    "label2id = {label:str(i) for i, label in enumerate(emotions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f2819ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'admiration', '1': 'amusement', '2': 'anger', '3': 'annoyance', '4': 'approval', '5': 'caring', '6': 'confusion', '7': 'curiosity', '8': 'desire', '9': 'disappointment', '10': 'disapproval', '11': 'disgust', '12': 'embarrassment', '13': 'excitement', '14': 'fear', '15': 'gratitude', '16': 'grief', '17': 'joy', '18': 'love', '19': 'nervousness', '20': 'optimism', '21': 'pride', '22': 'realization', '23': 'relief', '24': 'remorse', '25': 'sadness', '26': 'surprise', '27': 'neutral'}\n",
      "{'admiration': '0', 'amusement': '1', 'anger': '2', 'annoyance': '3', 'approval': '4', 'caring': '5', 'confusion': '6', 'curiosity': '7', 'desire': '8', 'disappointment': '9', 'disapproval': '10', 'disgust': '11', 'embarrassment': '12', 'excitement': '13', 'fear': '14', 'gratitude': '15', 'grief': '16', 'joy': '17', 'love': '18', 'nervousness': '19', 'optimism': '20', 'pride': '21', 'realization': '22', 'relief': '23', 'remorse': '24', 'sadness': '25', 'surprise': '26', 'neutral': '27'}\n"
     ]
    }
   ],
   "source": [
    "print(id2label)\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dda44d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>admiration</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>approval</th>\n",
       "      <th>caring</th>\n",
       "      <th>confusion</th>\n",
       "      <th>curiosity</th>\n",
       "      <th>desire</th>\n",
       "      <th>...</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>that game hurt.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sexuality should not be a grouping category i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you do right, if you do not care then fuck them!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>man i love reddit.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>name was nowhere near them, he was by the falc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  admiration  amusement  \\\n",
       "0                                    that game hurt.           0          0   \n",
       "1   sexuality should not be a grouping category i...           0          0   \n",
       "2   you do right, if you do not care then fuck them!           0          0   \n",
       "3                                 man i love reddit.           0          0   \n",
       "4  name was nowhere near them, he was by the falc...           0          0   \n",
       "\n",
       "   anger  annoyance  approval  caring  confusion  curiosity  desire  ...  \\\n",
       "0      0          0         0       0          0          0       0  ...   \n",
       "1      0          0         0       0          0          0       0  ...   \n",
       "2      0          0         0       0          0          0       0  ...   \n",
       "3      0          0         0       0          0          0       0  ...   \n",
       "4      0          0         0       0          0          0       0  ...   \n",
       "\n",
       "   nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
       "0            0         0      0            0       0        0        1   \n",
       "1            0         0      0            0       0        0        0   \n",
       "2            0         0      0            0       0        0        0   \n",
       "3            0         0      0            0       0        0        0   \n",
       "4            0         0      0            0       0        0        0   \n",
       "\n",
       "   surprise  neutral                                             labels  \n",
       "0         0        0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1         0        0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2         0        1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3         0        0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4         0        1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-Hot Encoding all Emotions\n",
    "df[\"labels\"] = df[emotions].values.tolist()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86a5f182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((168971, 30), (42254, 30))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create train / test splits\n",
    "mask = np.random.rand(len(df)) < 0.8\n",
    "df_train = df[mask]\n",
    "df_test = df[~mask]\n",
    "\n",
    "(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6cfca4",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a9c9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotions Visualization by number of cases\n",
    "\n",
    "temp = df[list(emotions)].sum(axis=0) \\\n",
    "    .reset_index() \\\n",
    "    .rename(columns={'index': 'emotion', 0: 'n'}) \\\n",
    "    .sort_values('n', ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.tick_params(axis='x', rotation=90)\n",
    "sns.barplot(data=temp, x='n', \n",
    "            y='emotion',\n",
    "            dodge=False,\n",
    "            ax=ax).set_title('Emotions by number of appearances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3428cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating emotions\n",
    "\n",
    "pos = {'admiration','amusement','approval','caring','desire','excitement','gratitude','joy','love',\n",
    "       'optimism','pride','relief'}\n",
    "neg = {'sadness','fear','embarrassment','disapproval','disappointment','annoyance','anger','nervousness',\n",
    "       'remorse','grief','disgust'}\n",
    "amb= {'realization','surprise','curiosity','confusion','neutral'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0cbe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotions and data vis\n",
    "\n",
    "print(\"Length of data: \", len(df))\n",
    "print(\"Number of emotions: \", len(emotions))\n",
    "print(\"Number of positive emotions: \", len(pos))\n",
    "print(\"Number of negative emotions: \", len(neg))\n",
    "print(\"Number of ambiguous emotions: \", len(amb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3aed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotions dataframe to later on aggregate\n",
    "\n",
    "df_emotion = pd.DataFrame()\n",
    "df_emotion['emotion'] = list(emotions)\n",
    "df_emotion['group'] = ''\n",
    "df_emotion['group'].loc[df_emotion['emotion'].isin(pos)] = 'positive'\n",
    "df_emotion['group'].loc[df_emotion['emotion'].isin(neg)] = 'negative'\n",
    "df_emotion['group'].loc[df_emotion['emotion'].isin(amb)] = 'ambiguous'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bfe9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emotion.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7952ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotions by number of appearences but by group\n",
    "\n",
    "temp = pd.DataFrame()\n",
    "temp['true positive rate'] = df.iloc[:, 3:-1].mean(0)\n",
    "temp['emotion'] = df.columns[3:-1]\n",
    "temp = temp.merge(df_emotion, how='left', on='emotion')\n",
    "temp = temp.sort_values('true positive rate')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "ax.tick_params(axis='x', rotation=90)\n",
    "\n",
    "sns.barplot(x=temp['emotion'], \n",
    "            y=temp['true positive rate'], \n",
    "            hue=temp['group'], \n",
    "            dodge=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d51f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def represent_train_test_balance(train_df,test_df):\n",
    "    # Class representation for train/test DS\n",
    "    train_GO = (train_df.loc[:,list(emotions)].sum(axis=0) / len(train_df)) * 100\n",
    "    test_GO = (test_df.loc[:,list(emotions)].sum(axis=0) / len(test_df)) * 100\n",
    "    \n",
    "    # Unique dataset for visualization purposes\n",
    "    \n",
    "    ds_GO = pd.DataFrame(data=[train_GO, test_GO]).T.reset_index(drop=False)\n",
    "    ds_GO.columns = ['Emotion', 'Train','Test']\n",
    "    ds_GO = ds_GO.sort_values('Train',ascending=False)\n",
    "    ds_GO = ds_GO.melt(id_vars='Emotion', var_name='Dataset', value_vars=['Train','Test'],\n",
    "                      value_name='Percentage')\n",
    "    \n",
    "    # Display dataset\n",
    "    \n",
    "    display(ds_GO.head(10))\n",
    "    \n",
    "    print(\"Graph Visualization\")\n",
    "    \n",
    "    plt.figure(figsize=(20,15))\n",
    "    sns.barplot(x='Percentage', y='Emotion', data=ds_GO, orient='h', hue='Dataset')\n",
    "    plt.title('Percentage of samples per emotion in train and test datasets', fontweight='bold', fontsize=20)\n",
    "    plt.xlabel('Percentage of all samples', fontweight='bold', fontsize=16)\n",
    "    plt.ylabel('Emotions', fontweight='bold', fontsize= 16)\n",
    "    plt.show()\n",
    "represent_train_test_balance(df_train, df_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8818d771",
   "metadata": {},
   "source": [
    "# Tokenization / Encoding / Method Structuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea478700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, DistilBertForSequenceClassification, BertForSequenceClassification, RobertaForSequenceClassification, XLNetForSequenceClassification\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers.models.xlnet.modeling_xlnet import XLNetForSequenceClassificationOutput\n",
    "from torch import nn\n",
    "import random\n",
    "import torch\n",
    "import platform\n",
    "import sys\n",
    "import sklearn as sk\n",
    "from typing import Optional, Union, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3799bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43432890",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoEmotionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: self.encodings[key][idx].clone().detach() for key, val in self.encodings}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edbe207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(df_train, df_test, tokenizer): \n",
    "  # Encodings\n",
    "\n",
    "  train_encodings = tokenizer(df_train[\"clean_text\"].values.tolist(), truncation=True)\n",
    "  test_encodings = tokenizer(df_test[\"clean_text\"].values.tolist(), truncation=True)\n",
    "\n",
    "  # labels / output\n",
    "  train_emotions = df_train[\"labels\"].values.tolist()\n",
    "  test_emotions = df_test[\"labels\"].values.tolist()\n",
    "\n",
    "  train_dataset = GoEmotionDataset(train_encodings, train_emotions)\n",
    "  test_dataset = GoEmotionDataset(test_encodings, test_emotions)\n",
    "  return train_dataset, test_dataset\n",
    "  \n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    y_pred = torch.from_numpy(logits)\n",
    "    y_true = torch.from_numpy(labels)\n",
    "    y_pred = y_pred.sigmoid()\n",
    "    y_pred = y_pred>0.5\n",
    "    y_true = y_true.bool()\n",
    "    acc = (y_pred==y_true).float().mean().item()\n",
    "\n",
    "    return {       \n",
    "      'Accuracy': acc\n",
    "    }\n",
    "    \n",
    "def set_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic=False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "def device_to_use():\n",
    "    has_gpu = torch.cuda.is_available()\n",
    "    has_mps = getattr(torch,'has_mps',False)\n",
    "    device = \"mps\" if getattr(torch,'has_mps',False) \\\n",
    "        else \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    print(f\"Python Platform: {platform.platform()}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    print()\n",
    "    print(f\"Python {sys.version}\")\n",
    "    print(f\"Pandas {pd.__version__}\")\n",
    "    print(f\"Scikit-Learn {sk.__version__}\")\n",
    "    print(\"GPU is\", \"available\" if has_gpu else \"NOT AVAILABLE\")\n",
    "    print(\"MPS (Apple Metal) is\", \"AVAILABLE\" if has_mps else \"NOT AVAILABLE\")\n",
    "    print(f\"Target device is {device}\")\n",
    "    return device\n",
    "\n",
    "def model_train(train_dataset, test_dataset, model, tokenizer, NUM_EPOCHS = 10,batch_size = 16, adam_epsilon_arg = 1e-8, learning_rate_arg = 2e-5, use_mps_device_arg = False, model_name = \"default\"):\n",
    "  training_args = TrainingArguments( \n",
    "    output_dir= OUTPUT_DIR+\"/\"+model_name,    \n",
    "    adam_epsilon = adam_epsilon_arg,\n",
    "    learning_rate = learning_rate_arg,\n",
    "    use_mps_device = use_mps_device_arg, # Mac Sylicon GPU\n",
    "    per_device_train_batch_size = batch_size, \n",
    "    per_device_eval_batch_size = batch_size*4,\n",
    "    gradient_accumulation_steps = 2, # scale batch size without needing more memory\n",
    "    num_train_epochs= NUM_EPOCHS,\n",
    "    do_eval = True,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    save_strategy = 'epoch',\n",
    "    load_best_model_at_end = True, # this allows to automatically get the best model at the end based on whatever metric we want\n",
    "    metric_for_best_model = 'Accuracy',\n",
    "    greater_is_better = True,\n",
    "    weight_decay=0.01,\n",
    "    seed = 25,\n",
    "    report_to=\"none\"\n",
    "  )\n",
    "  set_seed(training_args.seed)\n",
    "  trainer = Trainer(\n",
    "      model = model,\n",
    "      args = training_args,\n",
    "      train_dataset = train_dataset,\n",
    "      eval_dataset=test_dataset,\n",
    "      compute_metrics=compute_metrics,\n",
    "      tokenizer=tokenizer\n",
    "  )\n",
    "  return training_args, trainer\n",
    "                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093bf286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes to Each Model\n",
    "\n",
    "class DistilBertForMultilabelSequenceClassification(DistilBertForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "      super().__init__(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[SequenceClassifierOutput, Tuple[torch.Tensor, ...]]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        distilbert_output = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n",
    "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
    "        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n",
    "        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)\n",
    "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
    "        logits = self.classifier(pooled_output)  # (bs, num_labels)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), \n",
    "                            labels.float().view(-1, self.num_labels))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + distilbert_output[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=distilbert_output.hidden_states,\n",
    "            attentions=distilbert_output.attentions)\n",
    "\n",
    "class BertForMultilabelSequenceClassification(BertForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "      super().__init__(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        bert_output = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = bert_output[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), \n",
    "                            labels.float().view(-1, self.num_labels))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + bert_output[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=bert_output.hidden_states,\n",
    "            attentions=bert_output.attentions)\n",
    "\n",
    "class RoBertaForMultilabelSequenceClassification(RobertaForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "      super().__init__(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        roberta_output = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = roberta_output[0]\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), \n",
    "                            labels.float().view(-1, self.num_labels))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + roberta_output[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=roberta_output.hidden_states,\n",
    "            attentions=roberta_output.attentions)\n",
    "\n",
    "class XLNetForMultilabelSequenceClassification(XLNetForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "      super().__init__(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        mems: Optional[torch.Tensor] = None,\n",
    "        perm_mask: Optional[torch.Tensor] = None,\n",
    "        target_mapping: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        input_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        use_mems: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs,  # delete when `use_cache` is removed in XLNetModel\n",
    "    ) -> Union[Tuple, XLNetForSequenceClassificationOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        xlnet_output = self.transformer(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            mems=mems,\n",
    "            perm_mask=perm_mask,\n",
    "            target_mapping=target_mapping,\n",
    "            token_type_ids=token_type_ids,\n",
    "            input_mask=input_mask,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_mems=use_mems,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            **kwargs)\n",
    "        output = xlnet_output[0]\n",
    "        output = self.sequence_summary(output)\n",
    "        logits = self.logits_proj(output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), \n",
    "                            labels.float().view(-1, self.num_labels))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + xlnet_output[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=xlnet_output.hidden_states,\n",
    "            attentions=xlnet_output.attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5de0b7",
   "metadata": {},
   "source": [
    "# Pre-Trained Model - DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e24f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_or_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)\n",
    "num_labels=len(emotions)\n",
    "device = device_to_use()\n",
    "if device == 'gpu': device = 'cuda'\n",
    "model = DistilBertForMultilabelSequenceClassification.from_pretrained(model_path_or_name, num_labels=num_labels).to(device)\n",
    "model = model_config_ids(model, id2label, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a93aa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test(df_train, df_test, tokenizer)\n",
    "training_args, trainer = model_train(train_dataset, test_dataset, model, tokenizer, NUM_EPOCHS = 3,batch_size = 16, adam_epsilon_arg = 1e-8, learning_rate_arg = 2e-5, use_mps_device_arg = False, model_name = \"distilbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed43f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c506dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fbac40d",
   "metadata": {},
   "source": [
    "# Pre-Trained Model - BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b7cabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_or_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)\n",
    "num_labels=len(emotions)\n",
    "device = device_to_use()\n",
    "if device == 'gpu': device = 'cuda'\n",
    "model = BertForMultilabelSequenceClassification.from_pretrained(model_path_or_name, num_labels=num_labels).to(device)\n",
    "model = model_config_ids(model, id2label, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c934fc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test(df_train, df_test, tokenizer)\n",
    "training_args, trainer = model_train(train_dataset, test_dataset, model, tokenizer, NUM_EPOCHS = 3,batch_size = 16, adam_epsilon_arg = 1e-8, learning_rate_arg = 2e-5, use_mps_device_arg = False, model_name = \"bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9a9d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deb8a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1c51500",
   "metadata": {},
   "source": [
    "# Pre-Trained Model - RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640c5d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_or_name = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)\n",
    "num_labels=len(emotions)\n",
    "device = device_to_use()\n",
    "if device == 'gpu': device = 'cuda'\n",
    "model = RoBertaForMultilabelSequenceClassification.from_pretrained(model_path_or_name, num_labels=num_labels).to(device)\n",
    "model = model_config_ids(model, id2label, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e8c30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test(df_train, df_test, tokenizer)\n",
    "training_args, trainer = model_train(train_dataset, test_dataset, model, tokenizer, NUM_EPOCHS = 3,batch_size = 16, adam_epsilon_arg = 1e-8, learning_rate_arg = 2e-5, use_mps_device_arg = False, model_name = \"roberta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3fd5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881319ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "562ca376",
   "metadata": {},
   "source": [
    "# Pre-Trained Model - XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30c042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_or_name = \"xlnet-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)\n",
    "num_labels=len(emotions)\n",
    "device = device_to_use()\n",
    "if device == 'gpu': device = 'cuda'\n",
    "model = XLNetForMultilabelSequenceClassification.from_pretrained(model_path_or_name, num_labels=num_labels).to(device)\n",
    "model = model_config_ids(model, id2label, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905b8aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test(df_train, df_test, tokenizer)\n",
    "training_args, trainer = model_train(train_dataset, test_dataset, model, tokenizer, NUM_EPOCHS = 3,batch_size = 8, adam_epsilon_arg = 1e-8, learning_rate_arg = 2e-5, use_mps_device_arg = False, model_name = \"xlnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6849aa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b41285f4",
   "metadata": {},
   "source": [
    "# Transformer from Scratch - All you need!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71838691",
   "metadata": {},
   "source": [
    "Tokenizer - Still want to try some new ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f80cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from transformer import MultilabelSequenceClassificationTransformer, MultilabelLocalAttentionSequenceClassificationTransformer\n",
    "# Imports\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, DistilBertForSequenceClassification, BertForSequenceClassification, RobertaForSequenceClassification, XLNetForSequenceClassification\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers.models.xlnet.modeling_xlnet import XLNetForSequenceClassificationOutput\n",
    "from torch import nn\n",
    "import random\n",
    "import torch\n",
    "import platform\n",
    "import sys\n",
    "import sklearn as sk\n",
    "from typing import Optional, Union, Tuple\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47e0e4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98a2cea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoEmotionDatasetScratch(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: self.encodings[key][idx].clone().detach() for key in self.encodings}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7cb923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_to_ix(df_train, df_test):\n",
    "    word_to_ix = {'<pad>': 0, '<unk>': 1}\n",
    "    for text in pd.concat([df_train[\"clean_text\"], df_test[\"clean_text\"]]):\n",
    "        for token in spacy_tokenizer(text):\n",
    "            if token not in word_to_ix:\n",
    "                word_to_ix[token] = len(word_to_ix)\n",
    "    return word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af7dd7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text, word_to_ix, max_length=128):\n",
    "    tokens = [t for t in spacy_tokenizer(text)]\n",
    "    input_ids = [word_to_ix.get(token, word_to_ix['<unk>']) for token in tokens][:max_length]\n",
    "    input_ids = input_ids + [0] * (max_length - len(input_ids))\n",
    "    attention_mask = [1 if token_id != 0 else 0 for token_id in input_ids]\n",
    "\n",
    "    return {\n",
    "        'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "        'attention_mask': torch.tensor(attention_mask, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c93436f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(df_train, df_test, tokenizer, word_to_ix):\n",
    "    # Encodings\n",
    "    train_encoded_texts = [encode_text(text, word_to_ix) for text in df_train[\"clean_text\"].values.tolist()]\n",
    "    test_encoded_texts = [encode_text(text, word_to_ix) for text in df_test[\"clean_text\"].values.tolist()]\n",
    "\n",
    "    train_encodings = {\n",
    "        'input_ids': [text_encoding['input_ids'] for text_encoding in train_encoded_texts],\n",
    "        'attention_mask': [text_encoding['attention_mask'] for text_encoding in train_encoded_texts]\n",
    "    }\n",
    "\n",
    "    test_encodings = {\n",
    "        'input_ids': [text_encoding['input_ids'] for text_encoding in test_encoded_texts],\n",
    "        'attention_mask': [text_encoding['attention_mask'] for text_encoding in test_encoded_texts]\n",
    "    }\n",
    "\n",
    "    # labels / output\n",
    "    train_emotions = df_train[\"labels\"].values.tolist()\n",
    "    test_emotions = df_test[\"labels\"].values.tolist()\n",
    "\n",
    "    train_dataset = GoEmotionDatasetScratch(train_encodings, train_emotions)\n",
    "    test_dataset = GoEmotionDatasetScratch(test_encodings, test_emotions)\n",
    "\n",
    "    return train_dataset, test_dataset, len(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17465856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, val_dataset, epochs, batch_size, device, lr=0.001, weight_decay=0.01, warmup_steps=0, patience = 5):\n",
    "    \n",
    "    # Create DataLoaders for the training and validation datasets\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,pin_memory=True)\n",
    "\n",
    "    # Initialize the optimizer with model parameters, learning rate, and weight decay\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    model.to(device)\n",
    "\n",
    "    # Calculate the total number of training steps and create the learning rate scheduler\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for idx, batch in enumerate(train_loader):\n",
    "\n",
    "            # Reset the gradients for the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            loss, _ = model(input_ids, labels=labels)\n",
    "\n",
    "            # Backward pass to compute gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip gradients to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            # Update model parameters using the optimizer\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the learning rate using the scheduler\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            if (idx + 1) % 500 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{epochs} | Batch {idx + 1}/{len(train_loader)} | Train Loss: {loss.item():.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        val_f1 = 0\n",
    "        num_val_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                loss, logits = model(input_ids, labels=labels)\n",
    "                val_loss += loss.item()\n",
    "                y_pred = logits.cpu()\n",
    "                y_true = labels.cpu()\n",
    "                y_pred = y_pred.sigmoid()\n",
    "                y_pred = y_pred>0.5\n",
    "                y_true = y_true.bool()\n",
    "                f1 = f1_score(y_true, y_pred, average='micro')\n",
    "                acc = (y_pred==y_true).float().mean().item()\n",
    "                val_f1 += f1\n",
    "                val_acc += acc\n",
    "                num_val_batches += 1\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc /= num_val_batches\n",
    "        val_f1 /= num_val_batches\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f} | Val Acc: {val_acc:.4f} | Time: {elapsed_time:.2f}s\")\n",
    "        # Early stopping logic\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            print(f\"EarlyStopping counter: {counter} out of {patience}\")\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    return best_val_loss, val_acc, val_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ff8d15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = create_word_to_ix(df_train, df_test)\n",
    "train_dataset, test_dataset, vocab_size = train_test(df_train, df_test, spacy_tokenizer, word_to_ix)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "src_pad_idx = word_to_ix['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e63f6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal \n",
    "model2 = MultilabelSequenceClassificationTransformer(\n",
    "    src_vocab_size= vocab_size,\n",
    "    num_classes= len(emotions),\n",
    "    src_pad_idx= src_pad_idx,\n",
    "    emb_size = 128,\n",
    "    max_len=128\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a518fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results hyper parameters dictionary\n",
      "[{'params': {'lr': 1e-05, 'num_layers': 6, 'heads': 8, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 4, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.1694728625672204, 'val_acc': 0.9588509287152972}, {'params': {'lr': 1e-05, 'num_layers': 6, 'heads': 8, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 4, 'batch_size': 64, 'weight_decay': 0.001}, 'val_loss': 0.16154434851237706, 'val_acc': 0.9588509287152972}, {'params': {'lr': 1e-05, 'num_layers': 6, 'heads': 8, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.17052964653287614, 'val_acc': 0.9588509287152972}, {'params': {'lr': 1e-05, 'num_layers': 6, 'heads': 8, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 0.001}, 'val_loss': 0.1628226169518062, 'val_acc': 0.9588509287152972}, {'params': {'lr': 1e-05, 'num_layers': 6, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 4, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.1733415573835373, 'val_acc': 0.9588509287152972}, {'params': {'lr': 1e-05, 'num_layers': 6, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 4, 'batch_size': 64, 'weight_decay': 0.001}, 'val_loss': 0.1589777788945607, 'val_acc': 0.9588509287152972}, {'params': {'lr': 1e-05, 'num_layers': 6, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.16529120717729842, 'val_acc': 0.9588509287152972}, {'params': {'lr': 1e-05, 'num_layers': 6, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 0.001}, 'val_loss': 0.1553161612578801, 'val_acc': 0.9588509287152972}, {'params': {'lr': 1e-05, 'num_layers': 12, 'heads': 8, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 4, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.1671474426984787, 'val_acc': 0.9588509287152972}, {'params': {'lr': 1e-05, 'num_layers': 12, 'heads': 8, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 4, 'batch_size': 64, 'weight_decay': 0.001}, 'val_loss': 0.16708412340709142, 'val_acc': 0.9588509287152972}, {'params': {'lr': 1e-05, 'num_layers': 12, 'heads': 8, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.16354919331414358, 'val_acc': 0.9588509287152972}, {'params': {'lr': 1e-05, 'num_layers': 12, 'heads': 8, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 0.001}, 'val_loss': 0.15895125269889832, 'val_acc': 0.9588509287152972}, {'params': {'lr': 1e-05, 'num_layers': 12, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 4, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.16620116787297384, 'val_acc': 0.9588509287152972}, {'params': {'lr': 1e-05, 'num_layers': 12, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 4, 'batch_size': 64, 'weight_decay': 0.001}, 'val_loss': 0.16665744355746678, 'val_acc': 0.9588509287152972}, {'params': {'lr': 1e-05, 'num_layers': 12, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.15971017735345022, 'val_acc': 0.9588509287152972}, {'params': {'lr': 1e-05, 'num_layers': 12, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 0.001}, 'val_loss': 0.16398029242243087, 'val_acc': 0.9588509287152972}, {'params': {'lr': 0.001, 'num_layers': 6, 'heads': 8, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 4, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.15341281252247946, 'val_acc': 0.9588509287152972}, {'params': {'lr': 0.001, 'num_layers': 6, 'heads': 8, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 4, 'batch_size': 64, 'weight_decay': 0.001}, 'val_loss': 0.15339339630944387, 'val_acc': 0.9588509287152972}, {'params': {'lr': 0.001, 'num_layers': 6, 'heads': 8, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.15346688245024, 'val_acc': 0.9588509287152972}, {'params': {'lr': 0.001, 'num_layers': 6, 'heads': 8, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 0.001}, 'val_loss': 0.1532680413552693, 'val_acc': 0.9588509287152972}, {'params': {'lr': 0.001, 'num_layers': 6, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 4, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.15341971814632416, 'val_acc': 0.9588509287152972}, {'params': {'lr': 0.001, 'num_layers': 6, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 4, 'batch_size': 64, 'weight_decay': 0.001}, 'val_loss': 0.15329302421637944, 'val_acc': 0.9588509287152972}, {'params': {'lr': 0.001, 'num_layers': 6, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.1537885240146092, 'val_acc': 0.9588509287152972}, {'params': {'lr': 0.001, 'num_layers': 6, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 0.001}, 'val_loss': 0.15356084917272841, 'val_acc': 0.9588509287152972}, {'params': {'lr': 0.001, 'num_layers': 12, 'heads': 8, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 4, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.1532393800360816, 'val_acc': 0.9588509287152972}, {'params': {'lr': 0.001, 'num_layers': 12, 'heads': 8, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 4, 'batch_size': 64, 'weight_decay': 0.001}, 'val_loss': 0.15336079469748906, 'val_acc': 0.9588509287152972}, {'params': {'lr': 0.001, 'num_layers': 12, 'heads': 8, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.15367268664496286, 'val_acc': 0.9588509287152972}, {'params': {'lr': 0.001, 'num_layers': 12, 'heads': 8, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 0.001}, 'val_loss': 0.15342403522559575, 'val_acc': 0.9588509287152972}, {'params': {'lr': 0.001, 'num_layers': 12, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 4, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.1532111189195088, 'val_acc': 0.9588509287152972}, {'params': {'lr': 0.001, 'num_layers': 12, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 4, 'batch_size': 64, 'weight_decay': 0.001}, 'val_loss': 0.15333122866494314, 'val_acc': 0.9588509287152972}, {'params': {'lr': 0.001, 'num_layers': 12, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.15317635451044356, 'val_acc': 0.9588509287152972}, {'params': {'lr': 0.001, 'num_layers': 12, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 0.001}, 'val_loss': 0.15327459147998265, 'val_acc': 0.9588509287152972}, {'params': {'lr': 0.001, 'num_layers': 12, 'heads': 16, 'dropout': 0.1, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.1547988419021879, 'val_acc': 0.9583226953233991}, {'params': {'lr': 0.001, 'num_layers': 12, 'heads': 16, 'dropout': 0.4, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.1547997828040804, 'val_acc': 0.9583226953233991}, {'params': {'lr': 0.001, 'num_layers': 12, 'heads': 16, 'dropout': 0.2, 'emb_size': 128, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.15478673577308655, 'val_acc': 0.9583226953233991}, {'params': {'lr': 0.001, 'num_layers': 12, 'heads': 16, 'dropout': 0.2, 'emb_size': 256, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 1e-05}, 'val_loss': 0.15495342016220093, 'val_acc': 0.9583226953233991}, {'params': {'lr': 0.001, 'num_layers': 12, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 32, 'weight_decay': 1e-05}, 'val_loss': 0.15221967654568808, 'val_acc': 0.9583067553383964}, {'params': {'lr': 0.001, 'num_layers': 12, 'heads': 16, 'dropout': 0.2, 'emb_size': 512, 'forward_expansion': 8, 'batch_size': 64, 'weight_decay': 0}, 'val_loss': 0.15206867456436157, 'val_acc': 0.9585937602179391}]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('dict_results_hyperparams.pkl', 'rb') as fp:\n",
    "    results_hyperparam = pickle.load(fp)\n",
    "    print('Results hyper parameters dictionary')\n",
    "    print(results_hyperparam)\n",
    "best_result = min(results_hyperparam, key=lambda x: x[\"val_loss\"])\n",
    "\n",
    "best_result = best_result['params']\n",
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be4ef5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultilabelLocalAttentionSequenceClassificationTransformer(\n",
    "            src_vocab_size=vocab_size,\n",
    "            num_classes=len(emotions),\n",
    "            src_pad_idx=src_pad_idx,\n",
    "            emb_size=best_result[\"emb_size\"],\n",
    "            num_layers=best_result[\"num_layers\"],\n",
    "            forward_expansion=best_result[\"forward_expansion\"],\n",
    "            heads=best_result[\"heads\"],\n",
    "            device=device,\n",
    "            max_len=MAX_LEN\n",
    "        ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1a2cef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "train_model(model, train_dataset, test_dataset, epochs=50, batch_size=best_result['batch_size'], device=device, lr=best_result['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9af4fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "torch.manual_seed(42)\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "train_model(model2, train_dataset, test_dataset, epochs, batch_size, device, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229338e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_debug(model, train_dataset, val_dataset, epochs, batch_size, device, lr=0.001, weight_decay=0.01, warmup_steps=0):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,pin_memory=True)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    model.to(device)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        for _, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            loss, logits = model(input_ids, labels=labels)\n",
    "            return loss, logits, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f3d9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To verify everything was correct, which is\n",
    "loss, logits, labels = train_model_debug(model, train_dataset, test_dataset, epochs, batch_size, device, lr)\n",
    "print(logits[0]) # Which will then be normalized\n",
    "print(labels[0])\n",
    "loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "loss = loss_fct(logits[0], labels[0].float())\n",
    "print(loss)\n",
    "loss = loss_fct(logits, labels.float())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15709e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_dataset(dataset):\n",
    "    label_counts = np.zeros(28)\n",
    "    sequence_lengths = []\n",
    "\n",
    "    for idx in range(len(dataset)):\n",
    "        sample = dataset[idx]\n",
    "        input_ids = sample['input_ids']\n",
    "        labels = sample['labels']\n",
    "\n",
    "        sequence_lengths.append(len(input_ids))\n",
    "        label_counts += labels.numpy()\n",
    "\n",
    "        # Check if there are NaN or infinite values in input data\n",
    "    if torch.isnan(input_ids).any() or torch.isinf(input_ids).any():\n",
    "        print(f\"NaN or infinite values found in input_ids at batch {idx + 1}\")\n",
    "\n",
    "    if torch.isnan(labels).any() or torch.isinf(labels).any():\n",
    "        print(f\"NaN or infinite values found in labels at batch {idx + 1}\")\n",
    "\n",
    "    # Normalize label counts\n",
    "    label_counts /= len(dataset)\n",
    "\n",
    "    # Analyze the distribution of sequence lengths\n",
    "    plt.hist(sequence_lengths, bins=50)\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Sequence Lengths')\n",
    "    plt.show()\n",
    "\n",
    "    # Analyze the distribution of labels\n",
    "    plt.bar(range(28), label_counts)\n",
    "    plt.xlabel('Label')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Labels')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Average sequence length: {np.mean(sequence_lengths):.2f}\")\n",
    "    print(f\"Standard deviation of sequence length: {np.std(sequence_lengths):.2f}\")\n",
    "    print(f\"Label frequencies: {label_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c80a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_dataset(train_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3adc3446",
   "metadata": {},
   "source": [
    "# Transformers - Hyper Parameters Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe3ed69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dfadba",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333db071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_search(data, hyperparameters, device):\n",
    "    results = []\n",
    "\n",
    "    for values in itertools.product(*hyperparameters.values()):\n",
    "        params = dict(zip(hyperparameters.keys(), values))\n",
    "        print(f\"Training with hyperparameters: {params}\")\n",
    "\n",
    "        # Create a new model with the current hyperparameters\n",
    "        model = MultilabelSequenceClassificationTransformer(\n",
    "            src_vocab_size=vocab_size,\n",
    "            num_classes=len(emotions),\n",
    "            src_pad_idx=src_pad_idx,\n",
    "            emb_size=params[\"emb_size\"],\n",
    "            num_layers=params[\"num_layers\"],\n",
    "            forward_expansion=params[\"forward_expansion\"],\n",
    "            heads=params[\"heads\"],\n",
    "            dropout=params[\"dropout\"],\n",
    "            device=device,\n",
    "            max_len=MAX_LEN\n",
    "        ).to(device)\n",
    "\n",
    "        # Train the model and get the validation loss\n",
    "        val_loss, val_acc = train_model(\n",
    "            model, train_dataset, test_dataset, epochs=5, batch_size = params[\"batch_size\"], device = device, lr=params[\"lr\"], weight_decay=params['weight_decay'], warmup_steps=0\n",
    "        )\n",
    "\n",
    "        # Save the results\n",
    "        results.append({\"params\": params, \"val_loss\": val_loss, \"val_acc\": val_acc})\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9db5fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample = df.sample(frac=0.01, random_state=42)\n",
    "mask = np.random.rand(len(data_sample)) < 0.8\n",
    "df_train_sample = data_sample[mask]\n",
    "df_test_sample = data_sample[~mask]\n",
    "\n",
    "word_to_ix = create_word_to_ix(df_train, df_test)\n",
    "train_dataset, test_dataset, vocab_size = train_test(df_train_sample, df_test_sample, spacy_tokenizer, word_to_ix)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "src_pad_idx = word_to_ix['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede9c326",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"lr\": [1e-5, 1e-3],\n",
    "    \"num_layers\": [6, 12],\n",
    "    \"heads\": [8, 16],\n",
    "    \"dropout\": [0.2],\n",
    "    \"emb_size\": [512],\n",
    "    \"forward_expansion\": [4, 8],\n",
    "    \"batch_size\": [64],\n",
    "    \"weight_decay\": [1e-5,1e-3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a5b23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the hyperparameter search\n",
    "search_results = hyperparameter_search(data_sample, hyperparameters, device)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "best_result = min(search_results, key=lambda x: x[\"val_loss\"])\n",
    "print(f\"Best hyperparameters: {best_result['params']} with validation loss: {best_result['val_loss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1ab9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = min(results_hyperparam, key=lambda x: x[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd8486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some new options just to verify some hyper params that weren't searched. ´\n",
    "# This is because they're not so relevant and were causing problems running by the fact of GPU VRAM limitations\n",
    "# Separated in two dicts in order to work with the 8GB VRAM\n",
    "params_hp_1 = {\n",
    "    \"dropout\": [0.1, 0.4],\n",
    "    \"emb_size\": [128,256]\n",
    "    }\n",
    "params_hp_2 = {\n",
    "    \"batch_size\": [32, 128],\n",
    "    \"weight_decay\":[0]\n",
    "\n",
    "}\n",
    "list_params = []\n",
    "for key, value in params_hp_1.items():\n",
    "    for v in value:\n",
    "        new_params = dict(best_result[\"params\"])\n",
    "        new_params.update({key:v})\n",
    "        list_params.append(new_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765ea0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_research_ft(params_list, results,device):\n",
    "    for params in params_list:\n",
    "        print(f\"Training with hyperparameters: {params}\")\n",
    "        # Create a new model with the current hyperparameters\n",
    "        model = MultilabelSequenceClassificationTransformer(\n",
    "            src_vocab_size=vocab_size,\n",
    "            num_classes=len(emotions),\n",
    "            src_pad_idx=src_pad_idx,\n",
    "            emb_size=params[\"emb_size\"],\n",
    "            num_layers=params[\"num_layers\"],\n",
    "            forward_expansion=params[\"forward_expansion\"],\n",
    "            heads=params[\"heads\"],\n",
    "            dropout=params[\"dropout\"],\n",
    "            device=device,\n",
    "            max_len=MAX_LEN\n",
    "        ).to(device)\n",
    "\n",
    "        # Train the model and get the validation loss\n",
    "        val_loss, val_acc = train_model(\n",
    "            model, train_dataset, test_dataset, epochs=5, batch_size = params[\"batch_size\"], device = device, lr=params[\"lr\"], weight_decay=params['weight_decay'], warmup_steps=0\n",
    "        )\n",
    "\n",
    "        # Save the results\n",
    "        results.append({\"params\": params, \"val_loss\": val_loss, \"val_acc\": val_acc})\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d745b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4cd848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load File\n",
    "\n",
    "with open('dict_results_hyperparams.pkl', 'rb') as fp:\n",
    "    results_hyperparam = pickle.load(fp)\n",
    "    print('Results hyper parameters dictionary')\n",
    "    print(results_hyperparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595aa0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = hyperparameter_research_ft(list_params, results_hyperparam, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a1bfd8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary saved successfully to file\n"
     ]
    }
   ],
   "source": [
    "# Save File\n",
    "\n",
    "with open('dict_results_hyperparams.pkl', 'wb') as fp:\n",
    "    pickle.dump(results_hyperparam, fp)\n",
    "    print('dictionary saved successfully to file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ce2766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load File\n",
    "\n",
    "with open('dict_results_hyperparams.pkl', 'rb') as fp:\n",
    "    results_hyperparam = pickle.load(fp)\n",
    "    print('Results hyper parameters dictionary')\n",
    "    print(results_hyperparam)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dde27b3b",
   "metadata": {},
   "source": [
    "# Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0af36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "BERT_PATH = 'training_data/bert/checkpoint-15816/trainer_state.json'\n",
    "DISTILBERT_PATH = 'training_data/distilbert/checkpoint-15876/trainer_state.json'\n",
    "ROBERTA_PATH = 'training_data/roberta/checkpoint-15816/trainer_state.json'\n",
    "XLNET_PATH = 'training_data/xlnet/checkpoint-31710/trainer_state.json'\n",
    "\n",
    "\n",
    "model_files = [BERT_PATH, DISTILBERT_PATH, ROBERTA_PATH, XLNET_PATH]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_file in model_files:\n",
    "    with open(model_file) as f:\n",
    "        data = json.load(f)\n",
    "        for item in data['log_history']:\n",
    "            if 'epoch' in item and item['epoch'] == 3.0:\n",
    "                results[model_file.split('/')[1]] = item\n",
    "                break\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c77f834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# assuming the 'results' dictionary has been created as in the previous example\n",
    "\n",
    "metrics = ['eval_Accuracy', 'eval_loss', 'eval_runtime']\n",
    "\n",
    "# create a bar chart for accuracy and loss\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(len(model_files))\n",
    "\n",
    "for i, metric in enumerate(metrics[:-1]):\n",
    "    values = [results[model_file.split('/')[1]].get(metric, np.nan) for model_file in model_files]\n",
    "    ax.bar(x + i*0.25 - 0.25, values, width=0.25, label=metric)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([model_file.split('/')[1] for model_file in model_files], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# create a bar chart for runtime (logarithmic scale)\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(len(model_files))\n",
    "\n",
    "values = [results[model_file.split('/')[1]].get('eval_runtime', np.nan) for model_file in model_files]\n",
    "ax.bar(x, values, log=True, color='orange')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([model_file.split('/')[1] for model_file in model_files], rotation=45, ha='right')\n",
    "ax.set_ylabel('eval_runtime (seconds)')\n",
    "ax.set_xlabel('model')\n",
    "ax.set_title('Evaluation Runtime at Epoch 3.0')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7fe95972",
   "metadata": {},
   "source": [
    "# Implementation Pre-Trained DistilBert with own Dataset and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545b40d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "# Import model\n",
    "\n",
    "DISTILBERT_PATH = 'training_data/distilbert/checkpoint-15876'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = DistilBertForMultilabelSequenceClassification.from_pretrained(DISTILBERT_PATH).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(DISTILBERT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f20fcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0021c97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consumer = pd.read_csv('data/consumer_data_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def33bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consumer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eae0193",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consumer['clean_text'] = df_consumer['consumer_complaint'].progress_apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3131ab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consumer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0696ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(text, threshold=0.5):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    outputs= model(**inputs)\n",
    "    logits = outputs.logits.detach().cpu().numpy()[0] # assuming batch size of 1\n",
    "    probs = softmax(logits)\n",
    "    probs_scaled = (probs - np.min(probs)) / (np.max(probs) - np.min(probs))\n",
    "    binary_preds = np.where(probs_scaled >= threshold, 1, 0)\n",
    "    return binary_preds\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec150ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consumer['predicted_label'] = df_consumer['clean_text'].apply(predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b981f69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consumer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d9c410",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = df_consumer['predicted_label'].head(10).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e73170",
   "metadata": {},
   "outputs": [],
   "source": [
    "array"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30c49c74",
   "metadata": {},
   "source": [
    "# Implementing with own Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55c6f243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from transformer import MultilabelSequenceClassificationTransformer, MultilabelLocalAttentionSequenceClassificationTransformer\n",
    "# Imports\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, DistilBertForSequenceClassification, BertForSequenceClassification, RobertaForSequenceClassification, XLNetForSequenceClassification\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers.models.xlnet.modeling_xlnet import XLNetForSequenceClassificationOutput\n",
    "from torch import nn\n",
    "import random\n",
    "import torch\n",
    "import platform\n",
    "import sys\n",
    "import sklearn as sk\n",
    "from typing import Optional, Union, Tuple\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e975ae05",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04394bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_to_ix(df_train, df_test):\n",
    "    word_to_ix = {'<pad>': 0, '<unk>': 1}\n",
    "    for text in pd.concat([df_train[\"clean_text\"], df_test[\"clean_text\"]]):\n",
    "        for token in spacy_tokenizer(text):\n",
    "            if token not in word_to_ix:\n",
    "                word_to_ix[token] = len(word_to_ix)\n",
    "    return word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3eba6df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text, word_to_ix, max_length=128):\n",
    "    tokens = [t for t in spacy_tokenizer(text)]\n",
    "    input_ids = [word_to_ix.get(token, word_to_ix['<unk>']) for token in tokens][:max_length]\n",
    "    input_ids = input_ids + [0] * (max_length - len(input_ids))\n",
    "    attention_mask = [1 if token_id != 0 else 0 for token_id in input_ids]\n",
    "\n",
    "    return {\n",
    "        'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "        'attention_mask': torch.tensor(attention_mask, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "278ad9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = create_word_to_ix(df_train, df_test)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "src_pad_idx = word_to_ix['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "86cef991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultilabelLocalAttentionSequenceClassificationTransformer(\n",
       "  (encoder): EncoderLocalAttention(\n",
       "    (word_embedding): Embedding(32477, 512)\n",
       "    (position_embedding): Embedding(512, 512)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x TransformerLocalAttentionBlock(\n",
       "        (attention): LocalSelfAttention(\n",
       "          (values): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (keys): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (queries): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=512, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (emotion_extractor): EmotionFeatureExtractor(\n",
       "    (sentiment_fc): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=513, out_features=28, bias=True)\n",
       "  (pre_classifier): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (dropout): Dropout(p=0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(word_to_ix)\n",
    "MAX_LEN = 512\n",
    "model = MultilabelLocalAttentionSequenceClassificationTransformer(\n",
    "            src_vocab_size=vocab_size,\n",
    "            num_classes=len(emotions),\n",
    "            src_pad_idx=src_pad_idx,\n",
    "            emb_size=best_result[\"emb_size\"],\n",
    "            num_layers=best_result[\"num_layers\"],\n",
    "            forward_expansion=best_result[\"forward_expansion\"],\n",
    "            heads=best_result[\"heads\"],\n",
    "            device=device,\n",
    "            max_len=MAX_LEN\n",
    "        ).to(device)\n",
    "PATH = \"training_data\\\\transformer_scratch\\\\tr_la_save_dict.pth\"\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bddb0602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(text, word_to_ix, model, threshold=0.5):\n",
    "    inputs = encode_text(text,word_to_ix)\n",
    "    input_ids = inputs['input_ids'].unsqueeze(0).to(device)\n",
    "    outputs= model(input_ids)\n",
    "    print(outputs)\n",
    "    logits = outputs[1].detach().cpu() # assuming batch size of 1\n",
    "    probs = logits.sigmoid()\n",
    "    probs = softmax(probs)\n",
    "    probs_scaled = (probs - np.min(probs)) / (np.max(probs) - np.min(probs))\n",
    "    print(probs_scaled)\n",
    "    binary_preds = np.where(probs_scaled >= threshold, 1, 0)\n",
    "    return binary_preds\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2ae9b26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, tensor([[-2.3949, -3.1236, -3.1996, -2.6819, -2.3831, -3.5813, -3.2771, -3.0328,\n",
      "         -4.0401, -3.1618, -2.8680, -3.6086, -4.4933, -3.6042, -4.1691, -2.8212,\n",
      "         -5.8767, -3.2586, -3.2068, -4.7127, -3.1683, -5.0598, -3.1115, -5.1245,\n",
      "         -4.3853, -3.4118, -3.6269, -1.0069]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "max() received an invalid combination of arguments - got (axis=NoneType, out=NoneType, ), but expected one of:\n * ()\n * (Tensor other)\n * (int dim, bool keepdim)\n      didn't match because some of the keywords were incorrect: axis, out\n * (name dim, bool keepdim)\n      didn't match because some of the keywords were incorrect: axis, out\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[104], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mMy life sucks\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m predict_label(text, word_to_ix, model)\n",
      "Cell \u001b[1;32mIn[103], line 8\u001b[0m, in \u001b[0;36mpredict_label\u001b[1;34m(text, word_to_ix, model, threshold)\u001b[0m\n\u001b[0;32m      6\u001b[0m logits \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu() \u001b[39m# assuming batch size of 1\u001b[39;00m\n\u001b[0;32m      7\u001b[0m probs \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39msigmoid()\n\u001b[1;32m----> 8\u001b[0m probs \u001b[39m=\u001b[39m softmax(probs)\n\u001b[0;32m      9\u001b[0m probs_scaled \u001b[39m=\u001b[39m (probs \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39mmin(probs)) \u001b[39m/\u001b[39m (np\u001b[39m.\u001b[39mmax(probs) \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39mmin(probs))\n\u001b[0;32m     10\u001b[0m \u001b[39mprint\u001b[39m(probs_scaled)\n",
      "Cell \u001b[1;32mIn[103], line 15\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msoftmax\u001b[39m(x):\n\u001b[1;32m---> 15\u001b[0m     e_x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexp(x \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39;49mmax(x))\n\u001b[0;32m     16\u001b[0m     \u001b[39mreturn\u001b[39;00m e_x \u001b[39m/\u001b[39m e_x\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mamax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2793\u001b[0m, in \u001b[0;36mamax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_amax_dispatcher)\n\u001b[0;32m   2678\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mamax\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue, initial\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue,\n\u001b[0;32m   2679\u001b[0m          where\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue):\n\u001b[0;32m   2680\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2681\u001b[0m \u001b[39m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[0;32m   2682\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2791\u001b[0m \u001b[39m    5\u001b[39;00m\n\u001b[0;32m   2792\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2793\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapreduction(a, np\u001b[39m.\u001b[39;49mmaximum, \u001b[39m'\u001b[39;49m\u001b[39mmax\u001b[39;49m\u001b[39m'\u001b[39;49m, axis, \u001b[39mNone\u001b[39;49;00m, out,\n\u001b[0;32m   2794\u001b[0m                           keepdims\u001b[39m=\u001b[39;49mkeepdims, initial\u001b[39m=\u001b[39;49minitial, where\u001b[39m=\u001b[39;49mwhere)\n",
      "File \u001b[1;32mc:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:84\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[39mreturn\u001b[39;00m reduction(axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mdtype, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n\u001b[0;32m     83\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m             \u001b[39mreturn\u001b[39;00m reduction(axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n\u001b[0;32m     86\u001b[0m \u001b[39mreturn\u001b[39;00m ufunc\u001b[39m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: max() received an invalid combination of arguments - got (axis=NoneType, out=NoneType, ), but expected one of:\n * ()\n * (Tensor other)\n * (int dim, bool keepdim)\n      didn't match because some of the keywords were incorrect: axis, out\n * (name dim, bool keepdim)\n      didn't match because some of the keywords were incorrect: axis, out\n"
     ]
    }
   ],
   "source": [
    "text = 'My life sucks'\n",
    "\n",
    "predict_label(text, word_to_ix, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b974b17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "37f16e38780d10c35648b3bd09ed40a738946f51fc07e16d845fbaf5897e263f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
