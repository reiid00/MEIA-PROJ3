{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d72859ab",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87742597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Dataset\n",
    "#Kaggle: https://www.kaggle.com/datasets/shivamb/go-emotions-google-emotions-dataset\n",
    "\n",
    "!wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_1.csv\n",
    "!wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_2.csv\n",
    "!wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_3.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b83dc85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 11 00:47:16 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 531.14                 Driver Version: 531.14       CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 Ti    WDDM | 00000000:29:00.0  On |                  N/A |\n",
      "| 33%   41C    P0               43W / 200W|    276MiB /  8192MiB |      2%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2060    C+G   ...71.0_x64__8wekyb3d8bbwe\\GameBar.exe    N/A      |\n",
      "|    0   N/A  N/A      6988    C+G   ...siveControlPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A      7324    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A      7460    C+G   ...on\\112.0.1722.34\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A      9388    C+G   ...Programs\\Microsoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A      9684    C+G   ...23.0_x86__zpdnekdrzrea0\\Spotify.exe    N/A      |\n",
      "|    0   N/A  N/A     10692    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     10784    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A     10808    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     12660    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     13112    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "630a3245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import preprocessor\n",
    "import contractions\n",
    "import json\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "914f4fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>rater_id</th>\n",
       "      <th>example_very_unclear</th>\n",
       "      <th>admiration</th>\n",
       "      <th>...</th>\n",
       "      <th>love</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That game hurt.</td>\n",
       "      <td>eew5j0j</td>\n",
       "      <td>Brdd9</td>\n",
       "      <td>nrl</td>\n",
       "      <td>t3_ajis4z</td>\n",
       "      <td>t1_eew18eq</td>\n",
       "      <td>1.548381e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&gt;sexuality shouldnâ€™t be a grouping category I...</td>\n",
       "      <td>eemcysk</td>\n",
       "      <td>TheGreen888</td>\n",
       "      <td>unpopularopinion</td>\n",
       "      <td>t3_ai4q37</td>\n",
       "      <td>t3_ai4q37</td>\n",
       "      <td>1.548084e+09</td>\n",
       "      <td>37</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You do right, if you don't care then fuck 'em!</td>\n",
       "      <td>ed2mah1</td>\n",
       "      <td>Labalool</td>\n",
       "      <td>confessions</td>\n",
       "      <td>t3_abru74</td>\n",
       "      <td>t1_ed2m7g7</td>\n",
       "      <td>1.546428e+09</td>\n",
       "      <td>37</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Man I love reddit.</td>\n",
       "      <td>eeibobj</td>\n",
       "      <td>MrsRobertshaw</td>\n",
       "      <td>facepalm</td>\n",
       "      <td>t3_ahulml</td>\n",
       "      <td>t3_ahulml</td>\n",
       "      <td>1.547965e+09</td>\n",
       "      <td>18</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[NAME] was nowhere near them, he was by the Fa...</td>\n",
       "      <td>eda6yn6</td>\n",
       "      <td>American_Fascist713</td>\n",
       "      <td>starwarsspeculation</td>\n",
       "      <td>t3_ackt2f</td>\n",
       "      <td>t1_eda65q2</td>\n",
       "      <td>1.546669e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       id  \\\n",
       "0                                    That game hurt.  eew5j0j   \n",
       "1   >sexuality shouldnâ€™t be a grouping category I...  eemcysk   \n",
       "2     You do right, if you don't care then fuck 'em!  ed2mah1   \n",
       "3                                 Man I love reddit.  eeibobj   \n",
       "4  [NAME] was nowhere near them, he was by the Fa...  eda6yn6   \n",
       "\n",
       "                author            subreddit    link_id   parent_id  \\\n",
       "0                Brdd9                  nrl  t3_ajis4z  t1_eew18eq   \n",
       "1          TheGreen888     unpopularopinion  t3_ai4q37   t3_ai4q37   \n",
       "2             Labalool          confessions  t3_abru74  t1_ed2m7g7   \n",
       "3        MrsRobertshaw             facepalm  t3_ahulml   t3_ahulml   \n",
       "4  American_Fascist713  starwarsspeculation  t3_ackt2f  t1_eda65q2   \n",
       "\n",
       "    created_utc  rater_id  example_very_unclear  admiration  ...  love  \\\n",
       "0  1.548381e+09         1                 False           0  ...     0   \n",
       "1  1.548084e+09        37                  True           0  ...     0   \n",
       "2  1.546428e+09        37                 False           0  ...     0   \n",
       "3  1.547965e+09        18                 False           0  ...     1   \n",
       "4  1.546669e+09         2                 False           0  ...     0   \n",
       "\n",
       "   nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
       "0            0         0      0            0       0        0        1   \n",
       "1            0         0      0            0       0        0        0   \n",
       "2            0         0      0            0       0        0        0   \n",
       "3            0         0      0            0       0        0        0   \n",
       "4            0         0      0            0       0        0        0   \n",
       "\n",
       "   surprise  neutral  \n",
       "0         0        0  \n",
       "1         0        0  \n",
       "2         0        1  \n",
       "3         0        0  \n",
       "4         0        1  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = 'data/full_dataset/goemotions_'\n",
    "OUTPUT_DIR = 'training_data'\n",
    "\n",
    "df1 = pd.read_csv(f'{DATA_PATH}1.csv')\n",
    "df2 = pd.read_csv(f'{DATA_PATH}2.csv')\n",
    "df3 = pd.read_csv(f'{DATA_PATH}3.csv')\n",
    "\n",
    "frames = [df1,df2,df3]\n",
    "\n",
    "df = pd.concat(frames)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ecff46d",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87fb69ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM: https://www.kaggle.com/code/esknight/emotion-classification-final\n",
    "# Function for cleaning text\n",
    "def clean_text(text):\n",
    "    re_number = re.compile('[0-9]+')\n",
    "    re_url = re.compile(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\")\n",
    "    re_tag = re.compile('\\[[A-Z]+\\]')\n",
    "    re_char = re.compile('[^0-9a-zA-Z\\s?!.,:\\'\\\"//]+')\n",
    "    re_char_clean = re.compile('[^0-9a-zA-Z\\s?!.,\\[\\]]')\n",
    "    re_punc = re.compile('[?!,.\\'\\\"]')\n",
    "  \n",
    "    text = re.sub(re_char, \"\", text) # Remove unknown character \n",
    "    text = contractions.fix(text) # Expand contraction\n",
    "    text = re.sub(re_url, ' [url] ', text) # Replace URL with number\n",
    "    text = re.sub(re_char_clean, \"\", text) # Only alphanumeric and punctuations.\n",
    "    #text = re.sub(re_punc, \"\", text) # Remove punctuation.\n",
    "    text = text.lower() # Lower text\n",
    "    text = \" \".join([w for w in text.split(' ') if w != \" \"]) # Remove whitespace\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cce4567c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe2a54066f3a419f9faf4c9735aced37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/211225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clean text\n",
    "df['clean_text'] = df['text'].progress_apply(clean_text)\n",
    "\n",
    "# Drop Useless Columns\n",
    "df = df.drop(columns=['id','example_very_unclear','author','subreddit','link_id','parent_id','created_utc','rater_id'])\n",
    "\n",
    "# Reorganize Columns\n",
    "df = df[['clean_text'] + [col for col in df.columns if col not in ['text','clean_text']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98a7282b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise',\n",
       " 'neutral']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View emotions easier\n",
    "emotions = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "482895ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {str(i):label for i, label in enumerate(emotions)}\n",
    "label2id = {label:str(i) for i, label in enumerate(emotions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f2819ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'admiration', '1': 'amusement', '2': 'anger', '3': 'annoyance', '4': 'approval', '5': 'caring', '6': 'confusion', '7': 'curiosity', '8': 'desire', '9': 'disappointment', '10': 'disapproval', '11': 'disgust', '12': 'embarrassment', '13': 'excitement', '14': 'fear', '15': 'gratitude', '16': 'grief', '17': 'joy', '18': 'love', '19': 'nervousness', '20': 'optimism', '21': 'pride', '22': 'realization', '23': 'relief', '24': 'remorse', '25': 'sadness', '26': 'surprise', '27': 'neutral'}\n",
      "{'admiration': '0', 'amusement': '1', 'anger': '2', 'annoyance': '3', 'approval': '4', 'caring': '5', 'confusion': '6', 'curiosity': '7', 'desire': '8', 'disappointment': '9', 'disapproval': '10', 'disgust': '11', 'embarrassment': '12', 'excitement': '13', 'fear': '14', 'gratitude': '15', 'grief': '16', 'joy': '17', 'love': '18', 'nervousness': '19', 'optimism': '20', 'pride': '21', 'realization': '22', 'relief': '23', 'remorse': '24', 'sadness': '25', 'surprise': '26', 'neutral': '27'}\n"
     ]
    }
   ],
   "source": [
    "print(id2label)\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dda44d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>admiration</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>approval</th>\n",
       "      <th>caring</th>\n",
       "      <th>confusion</th>\n",
       "      <th>curiosity</th>\n",
       "      <th>desire</th>\n",
       "      <th>...</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>that game hurt.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sexuality should not be a grouping category i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you do right, if you do not care then fuck them!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>man i love reddit.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>name was nowhere near them, he was by the falc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  admiration  amusement  \\\n",
       "0                                    that game hurt.           0          0   \n",
       "1   sexuality should not be a grouping category i...           0          0   \n",
       "2   you do right, if you do not care then fuck them!           0          0   \n",
       "3                                 man i love reddit.           0          0   \n",
       "4  name was nowhere near them, he was by the falc...           0          0   \n",
       "\n",
       "   anger  annoyance  approval  caring  confusion  curiosity  desire  ...  \\\n",
       "0      0          0         0       0          0          0       0  ...   \n",
       "1      0          0         0       0          0          0       0  ...   \n",
       "2      0          0         0       0          0          0       0  ...   \n",
       "3      0          0         0       0          0          0       0  ...   \n",
       "4      0          0         0       0          0          0       0  ...   \n",
       "\n",
       "   nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
       "0            0         0      0            0       0        0        1   \n",
       "1            0         0      0            0       0        0        0   \n",
       "2            0         0      0            0       0        0        0   \n",
       "3            0         0      0            0       0        0        0   \n",
       "4            0         0      0            0       0        0        0   \n",
       "\n",
       "   surprise  neutral                                             labels  \n",
       "0         0        0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1         0        0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2         0        1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3         0        0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4         0        1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-Hot Encoding all Emotions\n",
    "df[\"labels\"] = df[emotions].values.tolist()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86a5f182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((168944, 30), (42281, 30))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create train / test splits\n",
    "mask = np.random.rand(len(df)) < 0.8\n",
    "df_train = df[mask]\n",
    "df_test = df[~mask]\n",
    "\n",
    "(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6cfca4",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a9c9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotions Visualization by number of cases\n",
    "\n",
    "temp = df[list(emotions)].sum(axis=0) \\\n",
    "    .reset_index() \\\n",
    "    .rename(columns={'index': 'emotion', 0: 'n'}) \\\n",
    "    .sort_values('n', ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.tick_params(axis='x', rotation=90)\n",
    "sns.barplot(data=temp, x='n', \n",
    "            y='emotion',\n",
    "            dodge=False,\n",
    "            ax=ax).set_title('Emotions by number of appearances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3428cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating emotions\n",
    "\n",
    "pos = {'admiration','amusement','approval','caring','desire','excitement','gratitude','joy','love',\n",
    "       'optimism','pride','relief'}\n",
    "neg = {'sadness','fear','embarrassment','disapproval','disappointment','annoyance','anger','nervousness',\n",
    "       'remorse','grief','disgust'}\n",
    "amb= {'realization','surprise','curiosity','confusion','neutral'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0cbe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotions and data vis\n",
    "\n",
    "print(\"Length of data: \", len(df))\n",
    "print(\"Number of emotions: \", len(emotions))\n",
    "print(\"Number of positive emotions: \", len(pos))\n",
    "print(\"Number of negative emotions: \", len(neg))\n",
    "print(\"Number of ambiguous emotions: \", len(amb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3aed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotions dataframe to later on aggregate\n",
    "\n",
    "df_emotion = pd.DataFrame()\n",
    "df_emotion['emotion'] = list(emotions)\n",
    "df_emotion['group'] = ''\n",
    "df_emotion['group'].loc[df_emotion['emotion'].isin(pos)] = 'positive'\n",
    "df_emotion['group'].loc[df_emotion['emotion'].isin(neg)] = 'negative'\n",
    "df_emotion['group'].loc[df_emotion['emotion'].isin(amb)] = 'ambiguous'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bfe9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emotion.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7952ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotions by number of appearences but by group\n",
    "\n",
    "temp = pd.DataFrame()\n",
    "temp['true positive rate'] = df.iloc[:, 3:-1].mean(0)\n",
    "temp['emotion'] = df.columns[3:-1]\n",
    "temp = temp.merge(df_emotion, how='left', on='emotion')\n",
    "temp = temp.sort_values('true positive rate')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "ax.tick_params(axis='x', rotation=90)\n",
    "\n",
    "sns.barplot(x=temp['emotion'], \n",
    "            y=temp['true positive rate'], \n",
    "            hue=temp['group'], \n",
    "            dodge=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d51f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def represent_train_test_balance(train_df,test_df):\n",
    "    # Class representation for train/test DS\n",
    "    train_GO = (train_df.loc[:,list(emotions)].sum(axis=0) / len(train_df)) * 100\n",
    "    test_GO = (test_df.loc[:,list(emotions)].sum(axis=0) / len(test_df)) * 100\n",
    "    \n",
    "    # Unique dataset for visualization purposes\n",
    "    \n",
    "    ds_GO = pd.DataFrame(data=[train_GO, test_GO]).T.reset_index(drop=False)\n",
    "    ds_GO.columns = ['Emotion', 'Train','Test']\n",
    "    ds_GO = ds_GO.sort_values('Train',ascending=False)\n",
    "    ds_GO = ds_GO.melt(id_vars='Emotion', var_name='Dataset', value_vars=['Train','Test'],\n",
    "                      value_name='Percentage')\n",
    "    \n",
    "    # Display dataset\n",
    "    \n",
    "    display(ds_GO.head(10))\n",
    "    \n",
    "    print(\"Graph Visualization\")\n",
    "    \n",
    "    plt.figure(figsize=(20,15))\n",
    "    sns.barplot(x='Percentage', y='Emotion', data=ds_GO, orient='h', hue='Dataset')\n",
    "    plt.title('Percentage of samples per emotion in train and test datasets', fontweight='bold', fontsize=20)\n",
    "    plt.xlabel('Percentage of all samples', fontweight='bold', fontsize=16)\n",
    "    plt.ylabel('Emotions', fontweight='bold', fontsize= 16)\n",
    "    plt.show()\n",
    "represent_train_test_balance(df_train, df_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8818d771",
   "metadata": {},
   "source": [
    "# Tokenization / Encoding / Method Structuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea478700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, DistilBertForSequenceClassification, BertForSequenceClassification, RobertaForSequenceClassification, XLNetForSequenceClassification\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers.models.xlnet.modeling_xlnet import XLNetForSequenceClassificationOutput\n",
    "from torch import nn\n",
    "import random\n",
    "import torch\n",
    "import platform\n",
    "import sys\n",
    "import sklearn as sk\n",
    "from typing import Optional, Union, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b3799bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43432890",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoEmotionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edbe207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(df_train, df_test, tokenizer): \n",
    "  # Encodings\n",
    "\n",
    "  train_encodings = tokenizer(df_train[\"clean_text\"].values.tolist(), truncation=True)\n",
    "  test_encodings = tokenizer(df_test[\"clean_text\"].values.tolist(), truncation=True)\n",
    "\n",
    "  # labels / output\n",
    "  train_emotions = df_train[\"labels\"].values.tolist()\n",
    "  test_emotions = df_test[\"labels\"].values.tolist()\n",
    "\n",
    "  train_dataset = GoEmotionDataset(train_encodings, train_emotions)\n",
    "  test_dataset = GoEmotionDataset(test_encodings, test_emotions)\n",
    "  return train_dataset, test_dataset\n",
    "  \n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    y_pred = torch.from_numpy(logits)\n",
    "    y_true = torch.from_numpy(labels)\n",
    "    y_pred = y_pred.sigmoid()\n",
    "    y_pred = y_pred>0.5\n",
    "    y_true = y_true.bool()\n",
    "    acc = (y_pred==y_true).float().mean().item()\n",
    "\n",
    "    return {       \n",
    "      'Accuracy': acc\n",
    "    }\n",
    "    \n",
    "def set_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic=False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "def device_to_use():\n",
    "    has_gpu = torch.cuda.is_available()\n",
    "    has_mps = getattr(torch,'has_mps',False)\n",
    "    device = \"mps\" if getattr(torch,'has_mps',False) \\\n",
    "        else \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    print(f\"Python Platform: {platform.platform()}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    print()\n",
    "    print(f\"Python {sys.version}\")\n",
    "    print(f\"Pandas {pd.__version__}\")\n",
    "    print(f\"Scikit-Learn {sk.__version__}\")\n",
    "    print(\"GPU is\", \"available\" if has_gpu else \"NOT AVAILABLE\")\n",
    "    print(\"MPS (Apple Metal) is\", \"AVAILABLE\" if has_mps else \"NOT AVAILABLE\")\n",
    "    print(f\"Target device is {device}\")\n",
    "    return device\n",
    "\n",
    "def model_train(train_dataset, test_dataset, model, tokenizer, NUM_EPOCHS = 10,batch_size = 16, adam_epsilon_arg = 1e-8, learning_rate_arg = 2e-5, use_mps_device_arg = False, model_name = \"default\"):\n",
    "  training_args = TrainingArguments( \n",
    "    output_dir= OUTPUT_DIR+\"/\"+model_name,    \n",
    "    adam_epsilon = adam_epsilon_arg,\n",
    "    learning_rate = learning_rate_arg,\n",
    "    use_mps_device = use_mps_device_arg, # Mac Sylicon GPU\n",
    "    per_device_train_batch_size = batch_size, \n",
    "    per_device_eval_batch_size = batch_size*4,\n",
    "    gradient_accumulation_steps = 2, # scale batch size without needing more memory\n",
    "    num_train_epochs= NUM_EPOCHS,\n",
    "    do_eval = True,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    save_strategy = 'epoch',\n",
    "    load_best_model_at_end = True, # this allows to automatically get the best model at the end based on whatever metric we want\n",
    "    metric_for_best_model = 'Accuracy',\n",
    "    greater_is_better = True,\n",
    "    weight_decay=0.01,\n",
    "    seed = 25,\n",
    "    report_to=\"none\"\n",
    "  )\n",
    "  set_seed(training_args.seed)\n",
    "  trainer = Trainer(\n",
    "      model = model,\n",
    "      args = training_args,\n",
    "      train_dataset = train_dataset,\n",
    "      eval_dataset=test_dataset,\n",
    "      compute_metrics=compute_metrics,\n",
    "      tokenizer=tokenizer\n",
    "  )\n",
    "  return training_args, trainer\n",
    "                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "093bf286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes to Each Model\n",
    "\n",
    "class DistilBertForMultilabelSequenceClassification(DistilBertForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "      super().__init__(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[SequenceClassifierOutput, Tuple[torch.Tensor, ...]]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        distilbert_output = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n",
    "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
    "        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n",
    "        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)\n",
    "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
    "        logits = self.classifier(pooled_output)  # (bs, num_labels)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), \n",
    "                            labels.float().view(-1, self.num_labels))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + distilbert_output[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=distilbert_output.hidden_states,\n",
    "            attentions=distilbert_output.attentions)\n",
    "\n",
    "class BertForMultilabelSequenceClassification(BertForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "      super().__init__(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        bert_output = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = bert_output[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), \n",
    "                            labels.float().view(-1, self.num_labels))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + bert_output[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=bert_output.hidden_states,\n",
    "            attentions=bert_output.attentions)\n",
    "\n",
    "class RoBertaForMultilabelSequenceClassification(RobertaForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "      super().__init__(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        roberta_output = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = roberta_output[0]\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), \n",
    "                            labels.float().view(-1, self.num_labels))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + roberta_output[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=roberta_output.hidden_states,\n",
    "            attentions=roberta_output.attentions)\n",
    "\n",
    "class XLNetForMultilabelSequenceClassification(XLNetForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "      super().__init__(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        mems: Optional[torch.Tensor] = None,\n",
    "        perm_mask: Optional[torch.Tensor] = None,\n",
    "        target_mapping: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        input_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        use_mems: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs,  # delete when `use_cache` is removed in XLNetModel\n",
    "    ) -> Union[Tuple, XLNetForSequenceClassificationOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        xlnet_output = self.transformer(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            mems=mems,\n",
    "            perm_mask=perm_mask,\n",
    "            target_mapping=target_mapping,\n",
    "            token_type_ids=token_type_ids,\n",
    "            input_mask=input_mask,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_mems=use_mems,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            **kwargs)\n",
    "        output = xlnet_output[0]\n",
    "        output = self.sequence_summary(output)\n",
    "        logits = self.logits_proj(output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), \n",
    "                            labels.float().view(-1, self.num_labels))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + xlnet_output[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=xlnet_output.hidden_states,\n",
    "            attentions=xlnet_output.attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5de0b7",
   "metadata": {},
   "source": [
    "# Pre-Trained Model - DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e24f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_or_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)\n",
    "num_labels=len(emotions)\n",
    "device = device_to_use()\n",
    "if device == 'gpu': device = 'cuda'\n",
    "model = DistilBertForMultilabelSequenceClassification.from_pretrained(model_path_or_name, num_labels=num_labels).to(device)\n",
    "model = model_config_ids(model, id2label, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a93aa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test(df_train, df_test, tokenizer)\n",
    "training_args, trainer = model_train(train_dataset, test_dataset, model, tokenizer, NUM_EPOCHS = 3,batch_size = 16, adam_epsilon_arg = 1e-8, learning_rate_arg = 2e-5, use_mps_device_arg = False, model_name = \"distilbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed43f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c506dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fbac40d",
   "metadata": {},
   "source": [
    "# Pre-Trained Model - BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b7cabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_or_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)\n",
    "num_labels=len(emotions)\n",
    "device = device_to_use()\n",
    "if device == 'gpu': device = 'cuda'\n",
    "model = BertForMultilabelSequenceClassification.from_pretrained(model_path_or_name, num_labels=num_labels).to(device)\n",
    "model = model_config_ids(model, id2label, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c934fc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test(df_train, df_test, tokenizer)\n",
    "training_args, trainer = model_train(train_dataset, test_dataset, model, tokenizer, NUM_EPOCHS = 3,batch_size = 16, adam_epsilon_arg = 1e-8, learning_rate_arg = 2e-5, use_mps_device_arg = False, model_name = \"bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9a9d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deb8a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1c51500",
   "metadata": {},
   "source": [
    "# Pre-Trained Model - RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640c5d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_or_name = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)\n",
    "num_labels=len(emotions)\n",
    "device = device_to_use()\n",
    "if device == 'gpu': device = 'cuda'\n",
    "model = RoBertaForMultilabelSequenceClassification.from_pretrained(model_path_or_name, num_labels=num_labels).to(device)\n",
    "model = model_config_ids(model, id2label, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e8c30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test(df_train, df_test, tokenizer)\n",
    "training_args, trainer = model_train(train_dataset, test_dataset, model, tokenizer, NUM_EPOCHS = 3,batch_size = 16, adam_epsilon_arg = 1e-8, learning_rate_arg = 2e-5, use_mps_device_arg = False, model_name = \"roberta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3fd5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881319ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "562ca376",
   "metadata": {},
   "source": [
    "# Pre-Trained Model - XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30c042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_or_name = \"xlnet-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)\n",
    "num_labels=len(emotions)\n",
    "device = device_to_use()\n",
    "if device == 'gpu': device = 'cuda'\n",
    "model = XLNetForMultilabelSequenceClassification.from_pretrained(model_path_or_name, num_labels=num_labels).to(device)\n",
    "model = model_config_ids(model, id2label, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905b8aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test(df_train, df_test, tokenizer)\n",
    "training_args, trainer = model_train(train_dataset, test_dataset, model, tokenizer, NUM_EPOCHS = 3,batch_size = 8, adam_epsilon_arg = 1e-8, learning_rate_arg = 2e-5, use_mps_device_arg = False, model_name = \"xlnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6849aa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b41285f4",
   "metadata": {},
   "source": [
    "# Transformer from Scratch - All you need!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71838691",
   "metadata": {},
   "source": [
    "Tokenizer - Still want to try some new ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f80cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from transformer import MultilabelSequenceClassificationTransformer\n",
    "# Imports\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, DistilBertForSequenceClassification, BertForSequenceClassification, RobertaForSequenceClassification, XLNetForSequenceClassification\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers.models.xlnet.modeling_xlnet import XLNetForSequenceClassificationOutput\n",
    "from torch import nn\n",
    "import random\n",
    "import torch\n",
    "import platform\n",
    "import sys\n",
    "import sklearn as sk\n",
    "from typing import Optional, Union, Tuple\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47e0e4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98a2cea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoEmotionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(self.encodings[key][idx]) for key in self.encodings}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7cb923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_to_ix(df_train, df_test):\n",
    "    word_to_ix = {'<pad>': 0, '<unk>': 1}\n",
    "    for text in pd.concat([df_train[\"clean_text\"], df_test[\"clean_text\"]]):\n",
    "        for token in spacy_tokenizer(text):\n",
    "            if token not in word_to_ix:\n",
    "                word_to_ix[token] = len(word_to_ix)\n",
    "    return word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af7dd7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text, word_to_ix, max_length=128):\n",
    "    tokens = [t for t in spacy_tokenizer(text)]\n",
    "    input_ids = [word_to_ix.get(token, word_to_ix['<unk>']) for token in tokens][:max_length]\n",
    "    input_ids = input_ids + [0] * (max_length - len(input_ids))\n",
    "    attention_mask = [1 if token_id != 0 else 0 for token_id in input_ids]\n",
    "\n",
    "    return {\n",
    "        'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "        'attention_mask': torch.tensor(attention_mask, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c93436f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(df_train, df_test, tokenizer, word_to_ix):\n",
    "    # Encodings\n",
    "    train_encoded_texts = [encode_text(text, word_to_ix) for text in df_train[\"clean_text\"].values.tolist()]\n",
    "    test_encoded_texts = [encode_text(text, word_to_ix) for text in df_test[\"clean_text\"].values.tolist()]\n",
    "\n",
    "    train_encodings = {\n",
    "        'input_ids': [text_encoding['input_ids'] for text_encoding in train_encoded_texts],\n",
    "        'attention_mask': [text_encoding['attention_mask'] for text_encoding in train_encoded_texts]\n",
    "    }\n",
    "\n",
    "    test_encodings = {\n",
    "        'input_ids': [text_encoding['input_ids'] for text_encoding in test_encoded_texts],\n",
    "        'attention_mask': [text_encoding['attention_mask'] for text_encoding in test_encoded_texts]\n",
    "    }\n",
    "\n",
    "    # labels / output\n",
    "    train_emotions = df_train[\"labels\"].values.tolist()\n",
    "    test_emotions = df_test[\"labels\"].values.tolist()\n",
    "\n",
    "    train_dataset = GoEmotionDataset(train_encodings, train_emotions)\n",
    "    test_dataset = GoEmotionDataset(test_encodings, test_emotions)\n",
    "\n",
    "    return train_dataset, test_dataset, len(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17465856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, val_dataset, epochs, batch_size, device, lr=0.001, weight_decay=0.01, warmup_steps=0):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,pin_memory=True)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    model.to(device)\n",
    "\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for idx, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            loss, _ = model(input_ids, labels=labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{epochs} | Batch {idx + 1}/{len(train_loader)} | Train Loss: {loss.item():.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_f1 = 0\n",
    "        val_acc = 0\n",
    "        num_val_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                loss, logits = model(input_ids, labels=labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                preds = (logits > 0).long()\n",
    "                f1 = f1_score(labels.cpu(), preds.cpu(), average='micro')\n",
    "                acc = accuracy_score(labels.cpu(), preds.cpu())\n",
    "\n",
    "                val_f1 += f1\n",
    "                val_acc += acc\n",
    "                num_val_batches += 1\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_f1 /= num_val_batches\n",
    "        val_acc /= num_val_batches\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f} | Val Acc: {val_acc:.4f} | Time: {elapsed_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e63f6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = create_word_to_ix(df_train, df_test)\n",
    "train_dataset, test_dataset, vocab_size = train_test(df_train, df_test, spacy_tokenizer, word_to_ix)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "src_pad_idx = word_to_ix['<pad>']\n",
    "model = MultilabelSequenceClassificationTransformer(\n",
    "    src_vocab_size= vocab_size,\n",
    "    num_classes= len(emotions),\n",
    "    src_pad_idx= src_pad_idx,\n",
    "    emb_size = 128,\n",
    "    max_len=128\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4b8b4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "loss, logits, labels = train_model(model, train_dataset, test_dataset, epochs, batch_size, device, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229338e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_debug(model, train_dataset, val_dataset, epochs, batch_size, device, lr=0.001, weight_decay=0.01, warmup_steps=0):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,pin_memory=True)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    model.to(device)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        for _, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            loss, logits = model(input_ids, labels=labels)\n",
    "            return loss, logits, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f3d9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To verify everything was correct, which is\n",
    "loss, logits, labels = train_model_debug(model, train_dataset, test_dataset, epochs, batch_size, device, lr)\n",
    "print(logits[0]) # Which will then be normalized\n",
    "print(labels[0])\n",
    "loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "loss = loss_fct(logits[0], labels[0].float())\n",
    "print(loss)\n",
    "loss = loss_fct(logits, labels.float())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15709e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_dataset(dataset):\n",
    "    label_counts = np.zeros(28)\n",
    "    sequence_lengths = []\n",
    "\n",
    "    for idx in range(len(dataset)):\n",
    "        sample = dataset[idx]\n",
    "        input_ids = sample['input_ids']\n",
    "        labels = sample['labels']\n",
    "\n",
    "        sequence_lengths.append(len(input_ids))\n",
    "        label_counts += labels.numpy()\n",
    "\n",
    "        # Check if there are NaN or infinite values in input data\n",
    "    if torch.isnan(input_ids).any() or torch.isinf(input_ids).any():\n",
    "        print(f\"NaN or infinite values found in input_ids at batch {idx + 1}\")\n",
    "\n",
    "    if torch.isnan(labels).any() or torch.isinf(labels).any():\n",
    "        print(f\"NaN or infinite values found in labels at batch {idx + 1}\")\n",
    "\n",
    "    # Normalize label counts\n",
    "    label_counts /= len(dataset)\n",
    "\n",
    "    # Analyze the distribution of sequence lengths\n",
    "    plt.hist(sequence_lengths, bins=50)\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Sequence Lengths')\n",
    "    plt.show()\n",
    "\n",
    "    # Analyze the distribution of labels\n",
    "    plt.bar(range(28), label_counts)\n",
    "    plt.xlabel('Label')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Labels')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Average sequence length: {np.mean(sequence_lengths):.2f}\")\n",
    "    print(f\"Standard deviation of sequence length: {np.std(sequence_lengths):.2f}\")\n",
    "    print(f\"Label frequencies: {label_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c80a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_dataset(train_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dde27b3b",
   "metadata": {},
   "source": [
    "# Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f0af36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bert': {'epoch': 3.0, 'eval_Accuracy': 0.9620088338851929, 'eval_loss': 0.11119233071804047, 'eval_runtime': 42.837, 'eval_samples_per_second': 992.693, 'eval_steps_per_second': 15.524, 'step': 15816}, 'distilbert': {'epoch': 3.0, 'eval_Accuracy': 0.9619091749191284, 'eval_loss': 0.11119987070560455, 'eval_runtime': 22.9527, 'eval_samples_per_second': 1824.103, 'eval_steps_per_second': 28.537, 'step': 15876}, 'roberta': {'epoch': 3.0, 'eval_Accuracy': 0.9616267681121826, 'eval_loss': 0.11095210909843445, 'eval_runtime': 41.1004, 'eval_samples_per_second': 1034.661, 'eval_steps_per_second': 16.18, 'step': 15816}, 'xlnet': {'epoch': 3.0, 'eval_Accuracy': 0.9615607857704163, 'eval_loss': 0.11138853430747986, 'eval_runtime': 58.8364, 'eval_samples_per_second': 715.509, 'eval_steps_per_second': 22.367, 'step': 31710}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "BERT_PATH = 'training_data/bert/checkpoint-15816/trainer_state.json'\n",
    "DISTILBERT_PATH = 'training_data/distilbert/checkpoint-15876/trainer_state.json'\n",
    "ROBERTA_PATH = 'training_data/roberta/checkpoint-15816/trainer_state.json'\n",
    "XLNET_PATH = 'training_data/xlnet/checkpoint-31710/trainer_state.json'\n",
    "\n",
    "\n",
    "model_files = [BERT_PATH, DISTILBERT_PATH, ROBERTA_PATH, XLNET_PATH]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_file in model_files:\n",
    "    with open(model_file) as f:\n",
    "        data = json.load(f)\n",
    "        for item in data['log_history']:\n",
    "            if 'epoch' in item and item['epoch'] == 3.0:\n",
    "                results[model_file.split('/')[1]] = item\n",
    "                break\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c77f834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# assuming the 'results' dictionary has been created as in the previous example\n",
    "\n",
    "metrics = ['eval_Accuracy', 'eval_loss', 'eval_runtime']\n",
    "\n",
    "# create a bar chart for accuracy and loss\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(len(model_files))\n",
    "\n",
    "for i, metric in enumerate(metrics[:-1]):\n",
    "    values = [results[model_file.split('/')[1]].get(metric, np.nan) for model_file in model_files]\n",
    "    ax.bar(x + i*0.25 - 0.25, values, width=0.25, label=metric)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([model_file.split('/')[1] for model_file in model_files], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# create a bar chart for runtime (logarithmic scale)\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(len(model_files))\n",
    "\n",
    "values = [results[model_file.split('/')[1]].get('eval_runtime', np.nan) for model_file in model_files]\n",
    "ax.bar(x, values, log=True, color='orange')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([model_file.split('/')[1] for model_file in model_files], rotation=45, ha='right')\n",
    "ax.set_ylabel('eval_runtime (seconds)')\n",
    "ax.set_xlabel('model')\n",
    "ax.set_title('Evaluation Runtime at Epoch 3.0')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7fe95972",
   "metadata": {},
   "source": [
    "# Implementation with own Dataset and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "545b40d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "# Import model\n",
    "\n",
    "DISTILBERT_PATH = 'training_data/distilbert/checkpoint-15876'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = DistilBertForMultilabelSequenceClassification.from_pretrained(DISTILBERT_PATH).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(DISTILBERT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f20fcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0021c97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consumer = pd.read_csv('data/consumer_data_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "def33bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>consumer_complaint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>dissatisfied current outcome dispute initiated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>item showed credit report previously removed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>saw credit report collection  dont know since ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>capital one auto finacing  unknown inquiring c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>small trucking company one trucks needed repai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                 consumer_complaint\n",
       "0           0  dissatisfied current outcome dispute initiated...\n",
       "1           1   item showed credit report previously removed ...\n",
       "2           2  saw credit report collection  dont know since ...\n",
       "3           3  capital one auto finacing  unknown inquiring c...\n",
       "4           4  small trucking company one trucks needed repai..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_consumer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eae0193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3aa5e7b49874f7a8149f7c6442da571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_consumer['clean_text'] = df_consumer['consumer_complaint'].progress_apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3131ab53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>consumer_complaint</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>dissatisfied current outcome dispute initiated...</td>\n",
       "      <td>dissatisfied current outcome dispute initiated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>item showed credit report previously removed ...</td>\n",
       "      <td>item showed credit report previously removed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>saw credit report collection  dont know since ...</td>\n",
       "      <td>saw credit report collection  do not know sinc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>capital one auto finacing  unknown inquiring c...</td>\n",
       "      <td>capital one auto finacing  unknown inquiring c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>small trucking company one trucks needed repai...</td>\n",
       "      <td>small trucking company one trucks needed repai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                 consumer_complaint  \\\n",
       "0           0  dissatisfied current outcome dispute initiated...   \n",
       "1           1   item showed credit report previously removed ...   \n",
       "2           2  saw credit report collection  dont know since ...   \n",
       "3           3  capital one auto finacing  unknown inquiring c...   \n",
       "4           4  small trucking company one trucks needed repai...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  dissatisfied current outcome dispute initiated...  \n",
       "1   item showed credit report previously removed ...  \n",
       "2  saw credit report collection  do not know sinc...  \n",
       "3  capital one auto finacing  unknown inquiring c...  \n",
       "4  small trucking company one trucks needed repai...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_consumer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0696ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    outputs= model(**inputs)\n",
    "    predicted_label = outputs.logits.argmax().item()\n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec150ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.1746, -6.9245, -5.2887, -3.6512, -3.3251, -5.3302, -4.0238, -4.8172,\n",
      "         -5.2583, -3.5854, -3.0456, -5.6179, -5.7017, -6.8682, -7.1072, -6.6426,\n",
      "         -6.9257, -7.2554, -7.3581, -6.2068, -4.6177, -6.5652, -3.5004, -6.9968,\n",
      "         -5.8415, -5.1619, -5.6564,  1.0718]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.7892, -6.7238, -4.8080, -3.2872, -3.2373, -6.3954, -4.5742, -5.8128,\n",
      "         -7.0240, -4.1600, -2.5682, -4.9369, -5.7419, -7.3031, -6.6545, -8.2348,\n",
      "         -7.5582, -7.5453, -8.5864, -6.1729, -5.3611, -7.6035, -3.4635, -7.4463,\n",
      "         -6.9241, -5.6089, -5.9134,  1.1575]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.2978, -6.1950, -6.0454, -4.8534, -3.9838, -6.9224, -1.6365, -3.9325,\n",
      "         -6.6930, -5.1279, -4.3711, -6.6057, -6.4305, -6.4736, -7.7638, -7.5328,\n",
      "         -8.1474, -6.4935, -7.6892, -6.9121, -5.5726, -7.5763, -3.4352, -7.6656,\n",
      "         -7.1945, -6.2089, -4.6616,  0.8898]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.0317, -5.8874, -6.1344, -5.2492, -4.1622, -6.6525, -2.8597, -2.8102,\n",
      "         -5.9921, -6.3045, -5.9808, -7.2216, -7.2012, -5.5723, -6.8118, -7.3265,\n",
      "         -8.3779, -6.5959, -8.0159, -6.7365, -4.8853, -7.6464, -4.6846, -8.0012,\n",
      "         -7.6033, -6.8543, -3.8478,  1.0337]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.0880, -6.8108, -4.8994, -3.8089, -4.1239, -6.2028, -3.2263, -4.0118,\n",
      "         -5.8869, -4.0667, -3.8116, -5.9523, -5.8143, -6.5396, -7.1319, -7.6237,\n",
      "         -7.2605, -7.5028, -7.7246, -6.3041, -5.7303, -7.5734, -3.5208, -7.6157,\n",
      "         -5.9322, -5.3909, -5.2416,  1.3932]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.9868, -7.1345, -5.6327, -4.4490, -4.1046, -5.6755, -2.9837, -3.4893,\n",
      "         -5.3282, -5.1300, -4.6538, -6.7851, -6.7116, -6.4457, -7.4406, -7.4524,\n",
      "         -7.8466, -7.3071, -7.5185, -6.6112, -4.5799, -7.6077, -3.8742, -7.7045,\n",
      "         -6.6941, -6.0696, -5.7127,  1.4121]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.6469, -6.8667, -5.8924, -4.7700, -3.5874, -5.1741, -3.3451, -3.8755,\n",
      "         -5.4199, -5.7027, -4.9354, -6.9198, -6.8559, -6.6209, -7.3452, -7.0956,\n",
      "         -7.8727, -7.0375, -7.1969, -6.6417, -4.7353, -7.4749, -4.0742, -7.3021,\n",
      "         -6.9698, -6.4255, -6.2365,  1.4832]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.9342, -6.4450, -6.1998, -5.2045, -3.3944, -5.8988, -3.6699, -4.2160,\n",
      "         -6.1084, -6.1442, -5.6505, -7.2779, -6.8996, -6.1519, -7.6459, -7.0078,\n",
      "         -8.1811, -6.5549, -7.2831, -6.8525, -5.3580, -7.6039, -4.1865, -7.5137,\n",
      "         -7.0312, -6.7139, -6.1111,  1.5104]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-4.3120, -5.7111, -2.8911, -1.6115, -2.1015, -4.0628, -5.3946, -6.0093,\n",
      "         -6.0649, -3.3535, -1.2022, -2.8154, -4.1694, -6.5899, -5.0339, -4.9452,\n",
      "         -6.2524, -6.4396, -7.3662, -5.0400, -4.0584, -5.6738, -3.3736, -5.5647,\n",
      "         -5.7373, -4.8628, -5.7504, -0.8087]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.5255, -7.3722, -5.9087, -4.6558, -3.6012, -5.9117, -3.6018, -4.1237,\n",
      "         -6.2753, -5.6722, -4.9643, -6.8913, -6.6279, -6.5570, -7.7185, -7.0059,\n",
      "         -8.1933, -7.2232, -7.3613, -6.8677, -5.3377, -7.4197, -3.6476, -7.4618,\n",
      "         -6.8609, -6.5353, -5.9192,  1.4951]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.5296, -7.0145, -3.8314, -2.6676, -4.3699, -4.7208, -4.9073, -5.6814,\n",
      "         -6.1816, -2.5659, -2.8499, -4.0435, -4.6489, -7.6168, -5.5001, -6.9408,\n",
      "         -5.4175, -7.9783, -7.7907, -5.1899, -5.6868, -7.5369, -3.8659, -7.3017,\n",
      "         -4.4275, -3.0309, -6.1312,  0.5384]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.2167, -6.5958, -6.0957, -4.2505, -3.3857, -4.0511, -3.3937, -4.1144,\n",
      "         -2.8194, -3.2591, -3.7179, -6.2308, -5.9988, -5.8197, -5.7262, -6.7167,\n",
      "         -6.3621, -6.7041, -6.4890, -5.1418, -1.5286, -6.2005, -3.0932, -6.2974,\n",
      "         -5.4692, -4.3065, -5.3094,  0.4899]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.8853, -7.0615, -5.8424, -4.2362, -3.6125, -6.5060, -4.1146, -4.9919,\n",
      "         -6.9670, -4.3927, -3.9651, -6.3991, -6.2521, -6.8108, -7.4708, -7.2226,\n",
      "         -7.7088, -7.5044, -8.4115, -6.6275, -6.0021, -7.1389, -3.6232, -7.3483,\n",
      "         -6.5790, -5.8447, -5.2362,  1.4972]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.6702, -7.0372, -5.3478, -4.1054, -4.0735, -6.8859, -4.0210, -5.1182,\n",
      "         -7.1243, -4.8616, -3.8251, -5.8779, -6.4568, -6.5438, -7.3166, -7.5822,\n",
      "         -7.9380, -6.9088, -8.2050, -6.7402, -6.4908, -7.8830, -4.3579, -7.8236,\n",
      "         -7.4348, -6.0701, -5.6122,  1.5867]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.7707, -6.4641, -5.2119, -3.7406, -3.2692, -6.0487, -5.8661, -7.1100,\n",
      "         -7.2763, -3.7674, -2.2191, -5.5633, -5.7607, -7.0097, -6.8371, -6.6391,\n",
      "         -6.7512, -6.7516, -7.6361, -6.3081, -6.2207, -6.5653, -3.1093, -6.3852,\n",
      "         -6.1238, -4.6526, -5.4490,  0.9758]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.3524, -7.0038, -5.9401, -4.8216, -4.0429, -4.1414, -4.1508, -3.4715,\n",
      "         -4.4494, -5.5072, -4.8866, -6.8549, -6.4759, -5.7309, -5.9044,  0.2333,\n",
      "         -6.5732, -6.2642, -6.8620, -5.4348, -4.0308, -6.8883, -3.6569, -5.0649,\n",
      "         -4.9170, -5.5564, -5.4383, -0.8503]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.9265, -6.7299, -5.5680, -4.0372, -3.8618, -4.8093, -4.0584, -4.1792,\n",
      "         -4.9373, -5.5826, -4.7433, -6.6554, -6.7017, -6.6791, -7.4733, -6.8342,\n",
      "         -7.7968, -7.0936, -7.1437, -6.6177, -4.6251, -7.6430, -4.2423, -7.3675,\n",
      "         -6.6911, -6.3007, -6.3084,  1.6304]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.4158, -6.6616, -6.4158, -4.7520, -3.4286, -5.0852, -3.4414, -3.3644,\n",
      "         -4.6520, -5.5096, -5.5363, -6.9782, -6.7982, -6.0847, -6.8541, -7.6597,\n",
      "         -7.9786, -6.9541, -7.2233, -6.2278, -3.8839, -7.3806, -3.5348, -7.2911,\n",
      "         -7.1048, -6.3016, -5.5965,  1.3863]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.4336, -7.0624, -5.1909, -3.8325, -3.6821, -4.0022, -4.6693, -5.6882,\n",
      "         -5.0069, -2.9019, -3.2762, -5.1568, -5.3400, -7.2918, -4.9602, -6.9131,\n",
      "         -4.9622, -7.1986, -7.8434, -4.5593, -3.4972, -6.9071, -3.2393, -6.0533,\n",
      "         -4.5296, -2.7648, -6.0721,  0.4651]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.0740, -6.9559, -5.8011, -4.5150, -3.0837, -5.7837, -3.4090, -4.3339,\n",
      "         -5.6761, -4.9697, -4.1465, -6.3175, -6.1145, -6.5535, -6.9129, -7.2308,\n",
      "         -7.4772, -6.9777, -7.6939, -6.2782, -4.4421, -6.7505, -3.0101, -6.7772,\n",
      "         -6.4618, -5.9888, -5.4146,  1.4419]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.1733, -6.8362, -4.3602, -2.9868, -3.5035, -3.8793, -5.0916, -5.7444,\n",
      "         -4.7027, -3.6957, -3.1928, -5.1437, -5.4072, -7.2247, -6.2560, -7.0076,\n",
      "         -6.1514, -7.6617, -7.3320, -5.7289, -4.0846, -6.7186, -3.1390, -6.4583,\n",
      "         -5.2571, -4.4729, -6.4653,  0.8335]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.4014, -7.4662, -6.2490, -4.9547, -2.7218, -5.6638, -4.5208, -4.7424,\n",
      "         -5.4371, -5.6393, -4.8034, -7.1053, -6.9269, -6.5411, -7.7532, -6.8303,\n",
      "         -8.1672, -7.6580, -7.3658, -7.1108, -5.0736, -6.8671, -3.7262, -7.2916,\n",
      "         -7.0726, -6.5331, -6.1656,  1.4194]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.3999, -6.0230, -4.4938, -3.0943, -2.6093, -5.3881, -4.5980, -4.6889,\n",
      "         -4.1211, -4.2888, -3.3492, -5.3527, -5.0663, -6.3314, -6.6620, -6.8227,\n",
      "         -6.8891, -6.8922, -7.6435, -5.9160, -5.2288, -6.7154, -3.6705, -6.9208,\n",
      "         -5.3932, -5.4038, -6.3862,  1.0123]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.2407, -7.2122, -5.2372, -4.4559, -3.9649, -6.1720, -4.5112, -4.8495,\n",
      "         -6.1663, -5.4817, -4.9130, -6.2670, -6.8353, -6.4185, -7.4184, -6.8805,\n",
      "         -7.7226, -6.8659, -7.7377, -6.7801, -6.2992, -7.8942, -4.5799, -7.5392,\n",
      "         -7.1933, -6.0667, -6.1886,  1.7279]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.7300, -7.0802, -5.5601, -4.4304, -4.0356, -5.6671, -3.8410, -4.3284,\n",
      "         -6.3745, -5.1665, -4.9590, -6.4553, -6.5244, -6.1125, -6.7164, -7.3887,\n",
      "         -7.5129, -6.7236, -7.2436, -6.2671, -5.3740, -7.4420, -3.9883, -7.2611,\n",
      "         -6.8355, -5.9200, -5.5299,  1.6798]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.6684, -6.3421, -5.0993, -3.4151, -2.4661, -4.8101, -5.1248, -5.2932,\n",
      "         -5.4308, -3.7336, -3.6884, -6.0831, -5.6715, -6.1682, -6.7916, -5.5814,\n",
      "         -6.0629, -5.7148, -8.3824, -5.5291, -4.5990, -5.4149, -2.9915, -4.8629,\n",
      "         -5.6108, -4.7695, -5.9654,  0.9025]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.9731, -6.4110, -6.1429, -5.2795, -3.5894, -5.7136, -6.3439, -6.5802,\n",
      "         -6.9246, -6.3583, -6.2371, -7.2460, -7.4692, -5.8142, -7.9145, -5.8733,\n",
      "         -7.8933, -5.7517, -7.1542, -7.3257, -6.0587, -6.9941, -5.0105, -7.0626,\n",
      "         -7.8477, -6.4883, -6.6545,  1.9578]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.8352, -6.7957, -5.9818, -4.0334, -3.1793, -5.6540, -3.8306, -5.0475,\n",
      "         -5.4350, -4.1728, -3.0547, -6.4418, -6.1543, -6.6880, -7.2279, -7.0193,\n",
      "         -7.4934, -7.1020, -7.7342, -6.4845, -4.3378, -6.6354, -2.8715, -6.6683,\n",
      "         -6.3905, -5.8347, -5.5962,  1.2289]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.9666, -7.3510, -5.4118, -3.4419, -3.5411, -4.7894, -3.5414, -4.4451,\n",
      "         -4.9828, -2.5329, -2.8343, -5.6409, -5.2530, -6.9413, -6.0135, -4.9399,\n",
      "         -5.8350, -7.0946, -7.5986, -4.9264, -4.4560, -6.7782, -3.3726, -6.0544,\n",
      "         -4.8941, -3.9937, -6.0318,  0.4218]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.0925, -6.0429, -3.2571, -2.1243, -2.5378, -4.3339, -5.5118, -5.5885,\n",
      "         -5.9728, -4.6845, -3.7062, -3.9239, -4.3783, -6.7916, -5.4976, -6.7619,\n",
      "         -6.5645, -6.9726, -7.7964, -5.2381, -5.0886, -6.5787, -3.3031, -6.3447,\n",
      "         -5.5656, -5.3054, -6.0546,  0.4540]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.7414, -6.7080, -6.1170, -4.7331, -3.8249, -5.9637, -4.1290, -4.5019,\n",
      "         -6.4248, -5.6798, -4.9982, -6.7002, -6.7237, -6.7630, -7.9503, -7.0327,\n",
      "         -8.2012, -7.0583, -7.1967, -7.2674, -5.5546, -7.2816, -4.2390, -7.6613,\n",
      "         -7.2143, -6.7506, -5.8992,  1.7546]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.5243, -5.4256, -2.9125, -1.7885, -3.2592, -5.9571, -5.6196, -6.7591,\n",
      "         -6.7496, -2.2684, -1.6171, -2.1391, -3.0096, -6.4498, -5.7690, -6.8670,\n",
      "         -6.1098, -6.2023, -7.0020, -5.2990, -5.8928, -6.6086, -3.3880, -6.8713,\n",
      "         -5.3058, -3.7961, -6.0139, -0.8283]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.3292, -6.6231, -5.1185, -3.4294, -4.0297, -5.6649, -2.7724, -3.2857,\n",
      "         -5.2910, -4.1628, -4.1784, -6.2436, -5.9109, -6.5344, -6.8039, -7.9480,\n",
      "         -7.5038, -7.3282, -7.9192, -6.0218, -4.9175, -7.9373, -3.8606, -7.7921,\n",
      "         -6.2975, -5.4600, -5.5498,  1.1761]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.1231, -6.1089, -4.9668, -4.0563, -4.6161, -5.7499, -4.8088, -5.2943,\n",
      "         -6.4446, -5.2934, -5.2668, -6.3723, -6.6153, -6.5398, -6.9172, -7.4336,\n",
      "         -7.3865, -6.9332, -7.6638, -6.5696, -6.0918, -8.0547, -4.8952, -7.8962,\n",
      "         -7.0973, -5.7288, -5.6919,  1.9313]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.9489, -6.7335, -5.2149, -3.5087, -3.4154, -1.9962, -4.8589, -4.9222,\n",
      "         -4.6003, -2.9760, -3.4939, -5.7118, -5.5677, -6.6056, -4.6736, -5.5612,\n",
      "         -5.1154, -7.0740, -6.7065, -4.0736, -3.4267, -6.7847, -3.3210, -5.3355,\n",
      "         -4.3897, -3.1626, -5.5333,  0.2758]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.5889, -6.0664, -5.2630, -3.4928, -2.3410, -4.5516, -5.7549, -5.8409,\n",
      "         -3.5609, -4.5925, -4.5056, -6.3503, -6.0109, -6.3697, -6.7769, -6.6158,\n",
      "         -6.8653, -6.3503, -6.8110, -6.0172, -4.1384, -6.0140, -3.5010, -6.3446,\n",
      "         -6.5597, -5.3077, -6.1579,  1.2132]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.9936, -6.8945, -5.1405, -3.9151, -3.7888, -6.6712, -3.9602, -4.5139,\n",
      "         -6.5178, -4.6838, -3.9484, -5.5497, -5.7022, -5.9532, -7.2072, -7.1252,\n",
      "         -7.6959, -6.8363, -7.3765, -6.6287, -5.9305, -7.0144, -3.5441, -7.3040,\n",
      "         -6.4033, -6.1041, -5.2251,  1.4440]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.5281, -7.2545, -5.2050, -4.0299, -3.8137, -4.3215, -5.2549, -5.4258,\n",
      "         -5.0802, -4.5608, -4.6209, -6.4426, -6.8415, -6.9192, -6.9391, -6.8846,\n",
      "         -6.8607, -7.4003, -7.6402, -6.3431, -4.7484, -7.4992, -4.8156, -7.3005,\n",
      "         -6.2610, -5.0166, -6.9308,  1.4865]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.0562, -6.6295, -6.1066, -4.8699, -3.8472, -5.6878, -3.0809, -3.6197,\n",
      "         -5.4908, -5.6460, -5.1087, -7.2257, -6.8081, -6.2570, -7.4129, -7.1000,\n",
      "         -8.0156, -6.8909, -7.3650, -6.6305, -4.6780, -7.6215, -3.9228, -7.5131,\n",
      "         -6.8403, -6.4147, -5.6060,  1.5046]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.1793, -7.1266, -4.9790, -4.0238, -3.9787, -5.3135, -3.3461, -3.6785,\n",
      "         -5.5756, -5.3565, -4.5434, -5.9213, -6.5467, -6.8523, -6.3807, -7.9940,\n",
      "         -7.6913, -7.6373, -7.7313, -6.1845, -4.6875, -7.9114, -4.1449, -7.7139,\n",
      "         -7.0099, -6.1748, -5.7488,  1.4886]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.6349, -6.6042, -6.4017, -5.0843, -3.6560, -6.4954, -5.1487, -5.2926,\n",
      "         -6.6409, -5.2026, -5.5406, -7.0915, -6.7090, -5.7676, -7.5588, -6.8774,\n",
      "         -7.5343, -6.2225, -7.7209, -6.8675, -5.9748, -6.6729, -3.9144, -6.9621,\n",
      "         -6.8896, -6.0458, -5.5475,  1.7545]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.4920, -6.8221, -5.5704, -4.4008, -4.2764, -5.8237, -3.6730, -4.5684,\n",
      "         -6.2221, -4.7779, -4.3831, -6.6485, -6.5992, -6.7763, -6.9351, -7.4101,\n",
      "         -7.3962, -7.2373, -7.8966, -6.2822, -5.4937, -7.9757, -4.0949, -7.6352,\n",
      "         -6.7158, -5.4291, -5.6398,  1.6581]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.5908, -6.2155, -5.7579, -4.4782, -3.6513, -5.9736, -4.2207, -3.7054,\n",
      "         -5.2409, -5.4080, -4.5059, -6.2832, -6.3053, -5.6386, -6.6754, -6.8001,\n",
      "         -7.6898, -6.6400, -6.8799, -6.1572, -5.4201, -7.5832, -4.0507, -7.2735,\n",
      "         -6.5335, -5.9733, -4.9078,  1.6431]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.4856, -6.8077, -6.1488, -5.0213, -3.7120, -5.7412, -4.1308, -4.7980,\n",
      "         -6.9716, -5.8065, -5.4309, -6.9434, -6.8753, -6.3393, -7.6230, -6.6001,\n",
      "         -7.8622, -6.4748, -7.4011, -6.8373, -6.0754, -7.2875, -4.0733, -6.9861,\n",
      "         -7.1132, -6.4541, -5.9135,  1.7003]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.9822, -7.4273, -4.6045, -3.5782, -3.7815, -5.3329, -3.6929, -4.3032,\n",
      "         -5.9332, -4.4356, -3.5137, -5.3231, -5.9636, -7.0921, -6.2608, -7.9024,\n",
      "         -7.2099, -7.8039, -7.8307, -5.8857, -5.3523, -7.8123, -4.0060, -7.4356,\n",
      "         -6.5249, -5.3250, -5.9952,  1.2943]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.0060, -6.9485, -4.6296, -3.1229, -3.1213, -5.0745, -5.5556, -6.6705,\n",
      "         -6.0843, -3.6048, -2.2985, -5.2359, -5.7547, -7.5509, -6.8487, -6.7455,\n",
      "         -6.7901, -7.4012, -8.4048, -6.1746, -4.8323, -6.7597, -3.6075, -6.8815,\n",
      "         -6.1620, -5.0090, -6.8363,  0.8913]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.4795, -7.1151, -5.4528, -4.1809, -3.8017, -6.1128, -4.1589, -5.2997,\n",
      "         -7.0375, -4.4602, -3.8924, -6.2888, -6.6120, -6.6647, -7.5534, -7.2961,\n",
      "         -7.5006, -6.9158, -7.6322, -6.6539, -5.8370, -7.0910, -3.6347, -7.1034,\n",
      "         -6.9088, -5.8187, -5.4842,  1.5298]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.0936, -6.7471, -4.3555, -2.7041, -3.4680, -5.6829, -4.9061, -6.3190,\n",
      "         -6.9359, -2.4094, -1.6746, -4.2689, -4.8432, -7.4623, -6.0353, -6.8738,\n",
      "         -6.2834, -7.4466, -8.2386, -5.3268, -5.5715, -7.0337, -3.2709, -6.8864,\n",
      "         -5.4842, -4.1606, -5.8327,  0.4493]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.4433, -5.9889, -5.2273, -3.5521, -3.2858, -5.4983, -4.0191, -4.2716,\n",
      "         -4.0775, -3.4926, -2.6639, -5.9564, -6.0705, -5.5182, -6.7802, -6.4996,\n",
      "         -6.6294, -5.9965, -7.4915, -5.9084, -4.2250, -6.2148, -3.9326, -6.5497,\n",
      "         -6.0139, -4.8658, -5.1972,  1.1347]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.2094, -6.6159, -4.9921, -3.8446, -4.2881, -5.7658, -3.0738, -3.5675,\n",
      "         -6.0462, -5.0318, -4.5022, -5.8336, -6.1017, -6.5343, -6.4858, -7.7823,\n",
      "         -7.5426, -7.1739, -8.1083, -6.0727, -5.3423, -7.9080, -4.0844, -7.6262,\n",
      "         -6.6471, -5.8919, -5.2773,  1.6138]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.5727, -6.7183, -6.1008, -5.3741, -3.6611, -5.6499, -4.6579, -4.6726,\n",
      "         -6.1470, -6.2575, -6.1648, -7.3787, -7.2901, -5.6458, -7.6281, -6.4540,\n",
      "         -7.8458, -6.1430, -7.2700, -6.8277, -5.8702, -7.1785, -4.2763, -6.9864,\n",
      "         -7.2180, -6.5552, -5.3643,  1.8069]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.1485, -6.0408, -3.8248, -3.3360, -4.2070, -3.9460, -5.5139, -5.5293,\n",
      "         -5.9549, -4.4506, -4.6626, -4.7683, -5.3829, -6.5549, -3.6512, -6.3797,\n",
      "         -5.0604, -6.4709, -7.4630, -4.1878, -5.6457, -7.4507, -4.4915, -6.2332,\n",
      "         -5.4736, -3.5823, -5.7840,  1.2038]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-4.8144, -6.6622, -6.2704, -4.8104, -2.6047, -5.4436, -5.0899, -4.5838,\n",
      "         -5.7098, -6.5284, -6.2719, -7.1609, -7.3365, -6.0823, -8.1623, -6.9156,\n",
      "         -8.8106, -6.4755, -6.5612, -7.5820, -4.9216, -6.5152, -3.9817, -7.2749,\n",
      "         -8.0540, -7.7282, -6.5933,  1.4940]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.3390, -6.6827, -5.0273, -4.0516, -3.8819, -6.0429, -5.2539, -5.9257,\n",
      "         -6.7799, -4.7524, -3.7328, -5.6340, -6.2264, -6.5286, -6.8627, -7.4701,\n",
      "         -7.2244, -6.8467, -7.3662, -6.5938, -6.1556, -7.3520, -4.4172, -7.3388,\n",
      "         -6.7347, -5.5299, -6.1068,  1.6016]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.7337, -6.6480, -5.1856, -3.8843, -4.4301, -4.3784, -3.9737, -3.4546,\n",
      "         -5.1189, -5.4731, -5.1739, -6.4563, -6.7165, -6.2162, -7.0819, -6.8628,\n",
      "         -7.4903, -6.9886, -6.9412, -6.4020, -4.9553, -7.6682, -4.6259, -7.4204,\n",
      "         -6.5520, -6.1585, -5.7200,  1.6594]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.6818, -6.7051, -5.1887, -3.6563, -3.4215, -6.7688, -3.8788, -5.4568,\n",
      "         -6.7337, -3.3118, -2.9313, -5.2429, -5.4240, -7.3442, -6.8770, -8.3106,\n",
      "         -7.1326, -7.6075, -8.2092, -6.1257, -6.2525, -7.6383, -3.3237, -7.7984,\n",
      "         -6.1369, -4.7118, -5.7685,  0.8582]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.6775, -6.5262, -3.4968, -2.2667, -4.0837, -6.1783, -3.7689, -4.8997,\n",
      "         -6.6038, -3.5952, -1.6879, -3.9124, -5.1517, -6.7939, -6.7203, -7.6890,\n",
      "         -7.4035, -7.1076, -7.6153, -6.1958, -5.7349, -7.6440, -3.7311, -7.6552,\n",
      "         -6.4053, -5.4512, -5.5608,  0.5106]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-4.6005, -6.8279, -6.9423, -5.8584, -2.8923, -4.4906, -3.5442, -3.2679,\n",
      "         -4.8504, -6.5188, -5.7876, -7.5331, -7.2137, -5.3155, -6.9113, -5.1593,\n",
      "         -8.1397, -6.3083, -6.0076, -6.6524, -3.5899, -6.5827, -3.8246, -6.3879,\n",
      "         -6.9729, -7.0508, -5.3133,  0.8887]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.8508, -6.5940, -4.0213, -3.2335, -3.4564, -4.4574, -5.4954, -5.9681,\n",
      "         -5.6235, -0.7900, -2.5959, -4.0821, -4.6504, -6.9182, -4.8780, -4.9887,\n",
      "         -3.9881, -6.9574, -8.1866, -4.7839, -4.3913, -5.6964, -3.3329, -6.2133,\n",
      "         -2.9515, -1.3937, -5.6615, -1.2186]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.1931, -6.5450, -6.3543, -5.1978, -2.0315, -6.1184, -3.5727, -3.9574,\n",
      "         -5.7303, -6.4754, -5.1984, -7.4487, -7.4603, -6.0215, -7.8763, -7.1442,\n",
      "         -8.7843, -6.3440, -7.4358, -7.0826, -4.8368, -7.2538, -4.1853, -7.3269,\n",
      "         -7.8653, -7.2604, -6.3331,  1.2187]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.9338, -5.9589, -2.9889, -2.1143, -3.9748, -2.6568, -5.5572, -4.5552,\n",
      "         -3.6799, -3.5456, -3.2681, -3.7953, -4.5318, -6.2850, -4.1084, -5.0863,\n",
      "         -4.8128, -6.5455, -6.5294, -4.1474, -5.2043, -7.4453, -4.6888, -5.8112,\n",
      "         -4.2081, -3.1181, -5.7529, -0.1579]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.7445, -6.8495, -5.6148, -4.3519, -3.5555, -6.3131, -3.2985, -4.3297,\n",
      "         -6.4154, -5.1293, -4.2212, -6.3329, -6.2480, -6.5078, -7.2908, -7.6522,\n",
      "         -7.9741, -7.0696, -7.7373, -6.5615, -5.6027, -7.4317, -3.6841, -7.3899,\n",
      "         -6.8585, -6.2786, -5.4768,  1.5453]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.5934, -6.1734, -5.3296, -4.4820, -1.3396, -5.7838, -5.2716, -6.2881,\n",
      "         -6.7148, -5.6578, -4.6956, -6.2529, -6.6913, -7.0201, -7.2672, -6.7765,\n",
      "         -7.6571, -6.3726, -8.3086, -6.6602, -5.1875, -6.9083, -3.7966, -6.6218,\n",
      "         -7.2005, -6.2623, -6.8640,  0.7966]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-4.9501, -7.3659, -6.3840, -5.1584, -2.8046, -3.8596, -6.9519, -6.2326,\n",
      "         -5.7104, -6.3896, -5.9915, -7.5194, -7.9154, -6.1542, -7.6365,  0.2219,\n",
      "         -7.6928, -5.9046, -7.1124, -6.9980, -4.7901, -6.1302, -4.6332, -4.9458,\n",
      "         -6.8381, -6.8204, -7.1220, -0.7095]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.2440, -6.5286, -3.5145, -2.4414, -4.6412, -5.2149, -3.2568, -4.3371,\n",
      "         -6.9173, -3.8136, -2.5875, -4.1559, -4.4743, -7.4407, -5.9066, -7.3148,\n",
      "         -6.7979, -7.8228, -8.0231, -5.3978, -6.3193, -8.1756, -4.1934, -7.7727,\n",
      "         -5.4132, -4.7111, -5.7324,  0.7415]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.4386, -6.4191, -5.8709, -4.5855, -3.5910, -6.2401, -4.6464, -5.1281,\n",
      "         -6.5910, -5.0971, -4.4558, -6.5537, -6.4919, -6.3094, -7.6261, -7.3815,\n",
      "         -7.6971, -6.4978, -7.2408, -6.8231, -5.9542, -7.0019, -3.4543, -7.2780,\n",
      "         -6.7790, -6.1403, -5.4939,  1.7712]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.3720, -6.5910, -5.0228, -3.6615, -3.2885, -6.2595, -5.4172, -6.0370,\n",
      "         -7.0583, -3.8157, -3.6856, -4.7677, -5.1100, -6.3680, -6.1200, -7.0704,\n",
      "         -6.7727, -6.6751, -7.5438, -5.4867, -6.6090, -7.1660, -3.6286, -6.8392,\n",
      "         -6.2637, -4.8306, -5.7416,  1.2893]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.6014, -6.4424, -5.2824, -4.3475, -4.3598, -6.1122, -3.9569, -4.2169,\n",
      "         -5.6520, -4.9828, -4.3412, -6.4919, -6.7441, -5.7930, -7.4763, -7.3576,\n",
      "         -7.5756, -6.4653, -7.1746, -6.8010, -5.4381, -7.3229, -4.1908, -7.4812,\n",
      "         -6.9028, -6.0072, -5.1056,  1.8938]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.1624, -6.4287, -4.5701, -3.6682, -3.7116, -5.6231, -4.7526, -5.1720,\n",
      "         -5.1120, -2.9547, -2.9057, -5.6351, -5.8014, -6.1883, -6.4038, -5.8600,\n",
      "         -6.0134, -6.9155, -7.3634, -5.6923, -6.2059, -6.6505, -3.7811, -6.7171,\n",
      "         -5.4798, -4.1314, -5.3064,  1.0165]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.3451, -6.4237, -6.0998, -4.1131, -2.9873, -5.9847, -3.2638, -4.4306,\n",
      "         -6.4444, -4.9128, -4.4346, -6.3715, -5.6793, -6.3884, -6.7051, -7.9296,\n",
      "         -8.0116, -6.7108, -7.5859, -6.1421, -4.6505, -6.9711, -2.0868, -6.8444,\n",
      "         -6.6960, -6.2266, -4.9253,  0.9543]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.6249, -6.1789, -3.4222, -2.4579, -2.7723, -4.7753, -5.6870, -6.5884,\n",
      "         -6.5828, -3.0160, -2.6824, -3.6696, -3.8706, -7.0723, -5.3099, -5.9145,\n",
      "         -5.2454, -7.0461, -8.1554, -4.9550, -5.4007, -5.9673, -2.8620, -5.7748,\n",
      "         -4.3370, -3.6886, -5.7245,  0.2322]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.2228, -6.9610, -6.0597, -5.0047, -3.0778, -5.1008, -5.1064, -5.1996,\n",
      "         -5.3117, -6.2517, -5.8352, -7.2968, -7.1508, -5.6027, -7.8754, -6.2610,\n",
      "         -7.8997, -6.0177, -6.2883, -6.9247, -4.8113, -6.8124, -4.0599, -6.8368,\n",
      "         -7.0818, -6.7409, -6.1898,  1.7325]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.6651, -7.1042, -3.8035, -2.7658, -3.8270, -6.0119, -4.1837, -4.8978,\n",
      "         -5.7837, -3.4984, -0.9929, -5.0572, -6.1027, -7.0841, -7.1026, -6.9497,\n",
      "         -7.5628, -7.6164, -7.8480, -6.5656, -5.8490, -7.7194, -4.4576, -7.8989,\n",
      "         -6.6363, -5.3676, -6.2229,  0.2920]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.6724, -6.6459, -6.0951, -4.8214, -3.8631, -4.9947, -4.5605, -4.6126,\n",
      "         -4.9316, -6.0512, -5.1181, -7.2113, -7.4208, -6.0823, -7.9470, -6.5012,\n",
      "         -8.0585, -6.2900, -6.0651, -7.0161, -5.2307, -7.6751, -4.7295, -7.2574,\n",
      "         -7.1651, -6.6201, -6.3113,  1.7525]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.6432, -6.3044, -6.3693, -4.7135, -3.1888, -4.5766, -4.6033, -5.0307,\n",
      "         -4.6681, -4.8099, -5.5058, -7.0786, -6.5178, -5.4953, -6.6293, -6.8864,\n",
      "         -6.8273, -5.7778, -7.5786, -5.6631, -3.3496, -6.5755, -3.4827, -5.8287,\n",
      "         -6.2588, -5.2080, -5.7402,  1.4144]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.9908, -6.9635, -5.1679, -4.2103, -4.1946, -5.6366, -4.3441, -4.5205,\n",
      "         -6.5468, -5.1512, -4.6101, -6.0093, -6.3678, -6.3680, -6.6696, -7.0269,\n",
      "         -7.3989, -6.9916, -7.6841, -6.2119, -6.1238, -7.5729, -4.0396, -7.1841,\n",
      "         -6.7607, -5.9115, -5.4692,  1.7773]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.9561, -6.9244, -5.2107, -4.0356, -4.3566, -6.3195, -3.2851, -3.6342,\n",
      "         -6.2493, -4.7146, -4.1946, -6.1372, -5.9514, -6.3560, -7.2156, -7.1341,\n",
      "         -7.6355, -7.2572, -7.5469, -6.6099, -6.0814, -7.8314, -3.9659, -7.6945,\n",
      "         -6.0505, -5.7162, -5.2874,  1.4030]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.0425, -7.6275, -5.6146, -4.6155, -2.9002, -4.0450, -4.0150, -3.7376,\n",
      "         -3.5075, -5.6781, -5.0299, -6.9159, -6.7117, -5.2360, -7.4214, -4.4547,\n",
      "         -7.3068, -6.1873, -6.1032, -6.5458, -2.3473, -5.8702, -3.9640, -6.1719,\n",
      "         -6.0124, -6.4025, -5.7868,  0.5839]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.1520, -6.8450, -4.0869, -2.9680, -3.9308, -4.1542, -1.8994, -2.9313,\n",
      "         -4.8932, -4.1198, -3.2588, -5.2045, -5.0245, -7.0523, -5.2977, -6.5244,\n",
      "         -6.4902, -7.3109, -7.4210, -4.9629, -4.6158, -7.7272, -3.8951, -6.8625,\n",
      "         -5.1048, -4.7651, -5.3596,  0.4880]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.1652, -6.2500, -3.7141, -2.2318, -4.2424, -6.0662, -4.3061, -5.5535,\n",
      "         -7.1316, -3.6860, -2.5727, -4.0619, -4.5549, -7.2878, -6.6689, -7.4330,\n",
      "         -7.2449, -7.4962, -8.5589, -5.8901, -5.9445, -7.2822, -3.4901, -7.4415,\n",
      "         -6.1666, -5.5992, -5.0685,  0.6311]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.3583, -6.6891, -5.1667, -3.9676, -3.1537, -4.1886, -4.1974, -5.2987,\n",
      "         -6.1475, -3.1156, -3.4681, -5.6838, -5.0045, -7.0567, -4.8606, -6.0645,\n",
      "         -4.9111, -6.9724, -8.5017, -4.1495, -4.7740, -6.4992, -2.6318, -5.2572,\n",
      "         -4.6389, -3.1970, -4.8281,  0.6985]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.6074, -6.8690, -5.8381, -4.7626, -4.1469, -5.0463, -4.4102, -4.7980,\n",
      "         -6.3222, -5.5549, -5.4046, -7.2606, -7.2973, -6.0552, -7.3878, -6.9345,\n",
      "         -7.8513, -6.6005, -7.8019, -6.7099, -5.3287, -7.3616, -4.3880, -7.1926,\n",
      "         -7.3781, -6.3431, -5.7888,  1.8309]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.9219, -6.6044, -5.0916, -3.3169, -3.3224, -5.5496, -3.8005, -5.3815,\n",
      "         -6.1347, -3.2246, -1.5944, -5.0182, -5.1841, -7.3274, -6.1260, -7.1965,\n",
      "         -6.6674, -7.2267, -7.6501, -5.4142, -4.9191, -7.4888, -3.2520, -6.8054,\n",
      "         -5.4840, -4.4360, -5.8883,  0.6274]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.7746, -7.1229, -5.1996, -3.9788, -4.0656, -5.5926, -3.6774, -4.3161,\n",
      "         -6.6790, -4.9558, -3.7979, -5.8150, -6.2318, -6.8148, -6.9074, -7.2890,\n",
      "         -7.7170, -7.3477, -7.3782, -6.4353, -5.9850, -7.8137, -4.1354, -7.4914,\n",
      "         -6.7889, -6.1152, -5.9659,  1.5363]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.7605, -6.6788, -5.4118, -4.3793, -4.2174, -6.5521, -4.4384, -5.2774,\n",
      "         -7.0919, -5.0019, -4.0712, -6.1504, -6.4972, -6.3924, -7.4709, -7.2798,\n",
      "         -7.8214, -6.9211, -7.6011, -6.8524, -6.8202, -7.6802, -4.1181, -7.5507,\n",
      "         -7.0930, -6.0758, -5.3067,  1.6873]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.2650, -6.5167, -5.4154, -3.7467, -2.5335, -4.1224, -5.3859, -5.8899,\n",
      "         -4.9930, -3.8783, -3.9112, -5.3533, -5.2701, -6.8672, -5.4468, -7.2797,\n",
      "         -5.9811, -6.8601, -8.1093, -4.9210, -3.7892, -6.5217, -2.9095, -5.9510,\n",
      "         -5.4531, -4.4592, -6.5038,  1.0232]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.1882, -6.5667, -3.2680, -2.4306, -4.2182, -5.7088, -5.7121, -6.6533,\n",
      "         -7.1847, -2.1535, -1.6715, -3.6581, -4.5891, -7.5089, -5.9013, -6.3940,\n",
      "         -5.7499, -7.8662, -8.4630, -5.5795, -6.3005, -7.2610, -3.9388, -7.2942,\n",
      "         -4.8082, -3.4731, -5.8268, -0.0739]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.2364, -7.3288, -3.9732, -2.7903, -3.4492, -4.2487, -5.0655, -5.9537,\n",
      "         -5.9060, -3.5143, -1.7341, -4.7355, -5.4897, -7.6047, -5.8876, -6.1742,\n",
      "         -6.3321, -7.6898, -7.7507, -5.5428, -5.0941, -7.0596, -3.7955, -6.5941,\n",
      "         -5.6163, -4.6346, -6.5156,  0.5604]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.7572, -6.5984, -6.1375, -4.8899, -3.5211, -5.3419, -5.7133, -6.0012,\n",
      "         -5.8351, -5.8712, -5.5293, -7.3602, -7.6768, -6.4180, -7.9886, -6.6987,\n",
      "         -8.1784, -6.9411, -7.7354, -7.4077, -4.6829, -7.0200, -4.6748, -7.5717,\n",
      "         -7.7690, -6.7370, -6.7927,  1.8636]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.3598, -6.4278, -4.6895, -2.8196, -3.6409, -5.4377, -4.4726, -5.5203,\n",
      "         -5.7244, -3.5448, -2.4066, -5.0282, -5.4914, -7.0357, -6.2513, -7.7706,\n",
      "         -6.8245, -7.0584, -7.8917, -5.7056, -4.7921, -7.0489, -3.3806, -7.0339,\n",
      "         -6.4012, -4.8190, -5.7233,  1.0551]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.8538, -7.0718, -3.8844, -2.8384, -4.1052, -5.9869, -4.2036, -4.9362,\n",
      "         -6.1649, -3.4849, -1.6665, -5.3897, -5.9924, -7.0037, -6.9956, -6.5376,\n",
      "         -7.5715, -7.7221, -8.0885, -6.3571, -6.0268, -7.8264, -4.6652, -8.0144,\n",
      "         -6.2676, -5.5058, -5.9179,  0.4624]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.2188, -6.4230, -6.0403, -4.8253, -2.8126, -5.9023, -2.7235, -3.5717,\n",
      "         -4.8027, -5.4609, -4.6822, -7.0164, -6.7453, -6.3224, -7.3113, -7.4631,\n",
      "         -8.1012, -7.0745, -7.8185, -6.4801, -4.0958, -7.5212, -3.3887, -7.3764,\n",
      "         -6.8999, -6.3810, -5.6965,  1.1124]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.0140, -5.7685, -5.8526, -4.2870, -3.2914, -4.6511, -5.4201, -6.6395,\n",
      "         -5.9006, -4.9030, -5.0861, -6.3729, -6.6364, -6.5670, -6.4809, -7.0569,\n",
      "         -6.6796, -5.8859, -7.9225, -5.9108, -4.6893, -6.9253, -3.6532, -6.0532,\n",
      "         -7.0499, -5.2662, -6.5265,  1.6925]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.6651, -6.4691, -4.9680, -3.8811, -3.3156, -4.6546, -5.2264, -5.6326,\n",
      "         -6.3677, -4.6328, -4.1106, -5.9215, -5.8235, -6.4218, -6.2667, -6.1449,\n",
      "         -6.3907, -6.3866, -7.4953, -5.5931, -5.6699, -6.4998, -3.2199, -5.8263,\n",
      "         -5.6875, -5.2580, -5.4204,  1.6421]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.4891, -6.6717, -4.7249, -3.1199, -3.8867, -6.0417, -1.9430, -3.2663,\n",
      "         -5.8568, -3.6935, -1.8695, -4.8710, -5.3480, -7.0682, -6.3704, -7.4789,\n",
      "         -7.3526, -7.4369, -7.9653, -5.9407, -5.3277, -7.6866, -3.1834, -7.4363,\n",
      "         -6.2327, -5.3630, -5.2328,  0.4443]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.1095, -6.9619, -4.3347, -2.9349, -3.7891, -5.7666, -3.9957, -5.0838,\n",
      "         -6.5294, -3.3259, -2.1773, -4.5234, -5.2902, -6.2275, -5.1004, -7.2053,\n",
      "         -6.5462, -6.8027, -7.8853, -5.0926, -5.4989, -7.2195, -3.8032, -6.7819,\n",
      "         -6.2262, -4.5249, -5.0463,  0.8837]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.8703, -7.1658, -5.3851, -4.4399, -3.8828, -5.7627, -3.9613, -4.2792,\n",
      "         -6.1775, -5.8621, -4.6104, -6.4964, -6.9161, -6.6942, -7.6764, -7.2607,\n",
      "         -8.1585, -7.2115, -7.2618, -7.0579, -5.9832, -7.9173, -4.4291, -7.7093,\n",
      "         -7.3402, -6.6885, -6.0251,  1.7554]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.1056, -6.5314, -3.3168, -2.2181, -4.5586, -5.8521, -5.2611, -6.6100,\n",
      "         -7.4792, -2.5213, -1.1148, -3.5676, -4.9309, -7.7035, -6.0184, -6.5460,\n",
      "         -6.5767, -7.6477, -8.4027, -5.7249, -6.6888, -8.0205, -4.3270, -7.5544,\n",
      "         -5.8025, -4.1084, -6.2489, -0.2450]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.8527, -7.1007, -5.5396, -4.3518, -3.9295, -5.9386, -3.5166, -3.8492,\n",
      "         -5.8541, -5.3295, -4.9700, -6.5536, -6.5443, -6.3260, -7.2328, -7.7260,\n",
      "         -7.9539, -7.2436, -7.3934, -6.5950, -5.1910, -7.6970, -4.1832, -7.8354,\n",
      "         -6.8253, -6.2352, -5.5728,  1.6251]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.3004, -7.2675, -4.7455, -3.6729, -4.2445, -4.8518, -3.2206, -3.7432,\n",
      "         -6.0792, -4.1330, -3.7176, -5.2133, -5.5057, -7.2605, -5.5731, -7.3278,\n",
      "         -6.4726, -7.6483, -7.9169, -5.2947, -5.2025, -7.8748, -3.7063, -7.1095,\n",
      "         -5.6203, -4.6214, -5.6452,  1.0843]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.3004, -6.6570, -6.4639, -5.1781, -3.1759, -2.0841, -3.8944, -3.7109,\n",
      "         -4.1162, -5.1978, -5.2138, -7.3315, -6.4156, -5.9710, -5.3529, -5.0964,\n",
      "         -6.0769, -6.0901, -5.7401, -4.6437, -3.0531, -6.6260, -3.4828, -5.2551,\n",
      "         -5.1127, -4.6493, -5.5822,  0.5309]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.6186, -5.9454, -3.4856, -1.9265, -3.7057, -2.1476, -5.3263, -5.0234,\n",
      "         -3.7297, -2.8978, -2.4517, -4.6184, -4.7263, -6.9350, -5.4551, -5.3144,\n",
      "         -5.3611, -6.8749, -6.9642, -4.8607, -4.0247, -7.0857, -3.9469, -6.0626,\n",
      "         -3.9518, -3.4417, -6.2269, -0.3772]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-4.8980, -6.4924, -6.3468, -5.1538, -2.8550, -4.7975, -3.6464, -3.9731,\n",
      "         -3.0208, -4.4786, -5.0639, -7.3524, -7.2927, -6.0359, -7.4280, -6.5770,\n",
      "         -7.1928, -6.0105, -7.1794, -6.5616, -2.0416, -6.0273, -4.0994, -6.7996,\n",
      "         -6.9468, -5.4581, -5.9098,  0.4403]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.3993, -6.9318, -3.8518, -3.0271, -4.3533, -5.9747, -3.9693, -4.5125,\n",
      "         -6.3198, -4.3713, -3.6933, -4.6423, -5.4323, -6.5487, -5.9850, -8.1356,\n",
      "         -7.1593, -7.2615, -7.4193, -5.7056, -6.2700, -8.0220, -4.2555, -7.8538,\n",
      "         -6.3568, -5.2125, -5.6400,  1.3449]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.7639, -6.4980, -3.7780, -2.9526, -4.3767, -5.7636, -3.9015, -4.2818,\n",
      "         -6.7306, -4.0872, -3.6075, -4.6949, -5.3080, -6.8455, -5.4740, -7.5712,\n",
      "         -6.6229, -7.2947, -8.2137, -5.3117, -6.2124, -8.0465, -4.0401, -7.4143,\n",
      "         -5.8361, -4.7409, -5.5762,  1.2024]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.7388, -7.2006, -5.5325, -4.0006, -3.7871, -5.9039, -4.2650, -5.1663,\n",
      "         -6.3659, -3.5587, -3.3758, -5.5077, -5.6353, -6.9036, -6.1214, -6.9747,\n",
      "         -6.6844, -7.3517, -8.0950, -5.5034, -5.6704, -7.5616, -3.6474, -7.0908,\n",
      "         -5.7767, -4.4571, -5.7927,  1.2722]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.1622, -6.1787, -4.8398, -3.4997, -1.7190, -1.8022, -4.9179, -5.4149,\n",
      "         -3.0667, -3.1330, -2.6240, -4.6158, -5.5501, -6.9730, -4.6428, -4.9323,\n",
      "         -5.4639, -6.6086, -5.6862, -4.5651, -2.8103, -6.0461, -2.9813, -4.9305,\n",
      "         -5.0214, -3.4620, -6.6204, -0.6381]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.1246, -6.7468, -4.0253, -2.4553, -3.6542, -5.1702, -4.1384, -5.0906,\n",
      "         -5.7293, -3.5474, -1.1003, -4.4347, -5.5218, -7.2401, -6.1309, -6.9016,\n",
      "         -7.2285, -7.5216, -7.7940, -5.9214, -4.8929, -7.4090, -4.1129, -7.3902,\n",
      "         -6.2560, -5.2870, -6.3361,  0.3635]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.8342, -7.1342, -4.8667, -3.4043, -3.8880, -5.0287, -2.9811, -3.8973,\n",
      "         -5.9720, -4.0957, -3.0565, -5.3799, -5.4014, -6.9105, -6.4880, -6.6892,\n",
      "         -6.9262, -7.2759, -7.4192, -5.6579, -5.2836, -7.5222, -3.4957, -6.7862,\n",
      "         -5.5802, -5.2630, -5.8997,  1.0224]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.3990, -7.2261, -5.2236, -3.8612, -4.1789, -6.0200, -3.8527, -5.0545,\n",
      "         -6.4311, -4.0820, -2.6779, -5.7493, -6.0986, -6.7646, -6.9947, -7.3256,\n",
      "         -7.3860, -7.3330, -7.5377, -6.4138, -5.5686, -7.3693, -3.5422, -7.2502,\n",
      "         -6.3907, -5.5741, -5.5248,  1.2248]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.8378, -7.1288, -5.3828, -4.2721, -4.1854, -5.6957, -3.1097, -3.7667,\n",
      "         -5.8379, -5.0562, -3.9658, -6.3542, -6.4663, -6.2830, -7.0512, -7.3229,\n",
      "         -7.7738, -7.0874, -7.2519, -6.4799, -5.1060, -7.6111, -3.8021, -7.4694,\n",
      "         -6.6942, -6.1165, -5.4219,  1.4216]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-4.8860, -7.0416, -5.7790, -3.7868, -2.5050, -6.1209, -3.3606, -4.2698,\n",
      "         -6.0010, -4.6160, -3.1409, -5.8714, -5.7478, -6.4935, -6.9433, -6.7518,\n",
      "         -7.9919, -7.1190, -8.2553, -6.4584, -4.6272, -6.6833, -3.3651, -6.7583,\n",
      "         -6.5441, -6.3732, -5.2602,  0.7831]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.3264, -6.1354, -4.5595, -3.2943, -4.2640, -6.3480, -3.4304, -4.5538,\n",
      "         -6.2853, -4.7983, -3.6345, -5.7922, -6.4642, -6.6704, -7.1423, -8.2193,\n",
      "         -8.0967, -7.1299, -8.0132, -6.6495, -5.7689, -8.3833, -4.6210, -8.2401,\n",
      "         -7.3313, -6.2544, -5.9786,  1.4854]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-4.8485, -5.9113, -5.3544, -3.9070, -1.3071, -5.8201, -5.6389, -6.2027,\n",
      "         -5.1756, -4.1811, -5.0441, -5.5673, -5.0068, -4.9411, -6.0732, -5.4152,\n",
      "         -6.3076, -5.1098, -7.6429, -5.4023, -3.8371, -4.6846, -2.1200, -4.7597,\n",
      "         -5.2804, -5.1284, -4.7883,  0.2892]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.1831, -7.3412, -5.7566, -4.2915, -3.4182, -5.8989, -2.9033, -3.9070,\n",
      "         -6.2594, -5.0382, -4.0092, -6.2919, -6.1796, -6.5013, -7.0974, -6.8044,\n",
      "         -7.8192, -7.2241, -7.5839, -6.4377, -4.9622, -7.2589, -3.4929, -7.0345,\n",
      "         -6.6276, -6.2250, -5.5526,  1.2245]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.3989, -5.8971, -3.2534, -2.7544, -3.7907, -5.2935, -5.2431, -4.9874,\n",
      "         -6.9950, -4.1086, -4.3493, -4.4926, -3.8994, -5.9204, -5.3906, -6.1147,\n",
      "         -5.7294, -6.4185, -8.0999, -5.0070, -6.6090, -6.4172, -3.3844, -6.3480,\n",
      "         -4.4090, -4.3714, -4.6251,  0.9041]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.7659, -6.5955, -5.5812, -3.9350, -3.7963, -6.5040, -3.2996, -4.6189,\n",
      "         -6.4987, -5.0855, -3.4394, -6.6677, -6.4946, -6.6776, -7.8938, -7.7417,\n",
      "         -8.4188, -6.9315, -7.9962, -7.0622, -5.7477, -7.4310, -3.4338, -7.4816,\n",
      "         -7.2781, -6.7076, -5.2611,  1.4963]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.8581, -7.1273, -5.9128, -4.6940, -3.9254, -5.7240, -3.4781, -4.1574,\n",
      "         -5.7511, -5.4069, -4.8049, -6.9945, -6.9029, -6.5896, -7.6845, -7.4218,\n",
      "         -7.9849, -7.0700, -7.3139, -6.8172, -4.9324, -7.5279, -4.0043, -7.6167,\n",
      "         -7.0103, -6.2749, -5.9805,  1.5825]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.0049, -6.1604, -4.0558, -2.2107, -3.4813, -5.3199, -2.7579, -3.5629,\n",
      "         -5.4213, -4.2996, -2.9195, -4.2833, -4.9512, -6.1948, -6.2600, -7.1854,\n",
      "         -7.5235, -6.4556, -6.8348, -5.8276, -3.9845, -6.9330, -3.3437, -6.8985,\n",
      "         -6.3314, -6.0839, -5.4712,  0.7172]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.6485, -7.2051, -5.2290, -3.9012, -3.8368, -6.2025, -4.0290, -4.7021,\n",
      "         -6.2293, -4.7493, -4.0629, -6.1181, -6.2093, -6.6002, -7.1756, -7.6248,\n",
      "         -7.7812, -7.4261, -7.9221, -6.6072, -5.2402, -7.3879, -3.7789, -7.6063,\n",
      "         -6.6278, -5.9825, -5.7095,  1.5882]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.9622, -3.9943, -2.3274, -2.0712, -4.2379, -4.2408, -4.8782, -4.6533,\n",
      "         -5.5233, -4.8013, -4.2777, -4.5628, -5.6230, -5.1010, -4.4849, -6.3238,\n",
      "         -6.3211, -5.5837, -7.6194, -5.3159, -4.3489, -7.0391, -5.1168, -6.6926,\n",
      "         -6.5323, -5.2235, -4.2776,  0.5780]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.5195, -6.7438, -4.5011, -3.3437, -3.7480, -6.1712, -5.2241, -6.5614,\n",
      "         -6.6002, -3.4520, -1.2076, -5.4534, -6.0601, -7.3676, -7.3751, -6.3240,\n",
      "         -7.4332, -7.3537, -8.0595, -6.6416, -6.2373, -7.6253, -4.3767, -7.8176,\n",
      "         -6.3514, -5.1254, -6.4112,  0.2636]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.6310, -7.0011, -6.0897, -4.2668, -3.4606, -3.6199, -3.4577, -4.4522,\n",
      "         -4.4011, -3.1725, -3.9099, -6.2063, -6.1247, -7.0487, -5.8232, -6.6350,\n",
      "         -5.6146, -6.9801, -7.0776, -5.0389, -2.9421, -6.6024, -3.3754, -6.0718,\n",
      "         -5.4907, -3.5157, -5.8279,  0.6762]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.5148, -6.2744, -3.9622, -2.3445, -2.5206, -5.8773, -5.1116, -5.6471,\n",
      "         -6.3916, -2.8219, -2.3545, -4.2031, -4.2587, -6.4423, -6.1321, -6.0549,\n",
      "         -6.5110, -6.9594, -7.8449, -5.3594, -5.5522, -5.9456, -2.2205, -6.1806,\n",
      "         -5.0528, -4.8566, -5.3154,  0.0657]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.8573, -6.0183, -3.8562, -2.4457, -3.9634, -4.3899, -3.5927, -4.3066,\n",
      "         -3.3631, -3.2320, -1.5812, -5.2433, -6.2307, -7.2220, -6.7490, -6.7715,\n",
      "         -6.9873, -7.0649, -7.4955, -6.1192, -3.4908, -7.5553, -4.1773, -7.5645,\n",
      "         -6.3715, -5.0024, -6.6863,  0.0094]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.6112, -6.5188, -5.2341, -3.8640, -3.2035, -0.4809, -5.3885, -5.7743,\n",
      "         -5.0912, -4.2901, -3.8964, -6.0036, -5.8752, -7.1469, -5.6951, -5.6947,\n",
      "         -5.5235, -6.2587, -5.6068, -4.9614, -4.0536, -6.4631, -3.7325, -4.8888,\n",
      "         -5.4872, -4.2479, -6.5578,  0.2178]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.4444, -6.6199, -5.4162, -4.0466, -2.9509, -5.1053, -6.2306, -6.7642,\n",
      "         -6.2607, -4.5827, -3.9856, -6.2477, -6.5602, -6.6231, -7.5135, -6.8258,\n",
      "         -7.1498, -6.4701, -7.1265, -6.6790, -5.1800, -6.3883, -3.8328, -6.6470,\n",
      "         -6.9213, -5.6957, -6.3470,  1.7485]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.5779, -7.0259, -5.4975, -4.0332, -2.6882, -4.4804, -5.2245, -5.7731,\n",
      "         -5.6012, -3.3549, -4.0897, -6.1376, -5.6342, -6.5254, -5.8137, -5.9005,\n",
      "         -5.6332, -6.7395, -7.8765, -5.1512, -4.2240, -5.9890, -2.6469, -5.5236,\n",
      "         -5.0770, -4.0031, -5.4565,  0.9097]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.3117, -6.1527, -5.9318, -5.0837, -3.6461, -5.3209, -5.4442, -4.9281,\n",
      "         -6.5746, -6.3968, -6.5227, -7.5185, -7.0339, -5.2862, -7.5329, -5.6989,\n",
      "         -7.7087, -5.7038, -7.1354, -6.7599, -5.8813, -6.6320, -3.7983, -6.5504,\n",
      "         -6.9354, -6.6919, -5.2788,  1.6238]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.3707, -7.1566, -5.1790, -3.6314, -3.6926, -6.1290, -3.2622, -4.5780,\n",
      "         -6.2379, -3.6744, -2.1832, -5.7721, -5.8233, -7.0485, -6.9853, -6.7973,\n",
      "         -7.2822, -7.6365, -8.0871, -6.1639, -5.3710, -7.3687, -3.5036, -7.3232,\n",
      "         -5.9222, -5.3386, -5.4487,  0.9300]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.8165, -6.3846, -3.4096, -1.9389, -4.0927, -6.2508, -4.8277, -5.8314,\n",
      "         -7.1578, -2.6120, -1.9712, -2.2598, -3.9977, -7.0812, -5.5572, -7.0628,\n",
      "         -6.5007, -7.3878, -7.7918, -5.4742, -5.9931, -7.5811, -3.8523, -7.5032,\n",
      "         -5.6743, -4.2590, -5.7323, -0.1352]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.5203, -7.0064, -4.9898, -3.3667, -3.2509, -5.2546, -4.1062, -4.4685,\n",
      "         -4.5556, -3.6989, -1.7629, -5.6002, -5.6007, -6.6881, -6.8002, -5.3031,\n",
      "         -6.7716, -7.1981, -7.6222, -5.9060, -5.3244, -7.1440, -3.5907, -6.6210,\n",
      "         -5.1260, -5.1668, -6.1855,  0.3982]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.3712, -7.0053, -4.7201, -3.8095, -4.1160, -6.0774, -4.7054, -5.1134,\n",
      "         -6.6350, -4.3988, -3.8176, -5.5384, -5.8186, -6.6406, -6.3037, -6.9753,\n",
      "         -6.9855, -7.4190, -8.2062, -5.9059, -6.7620, -7.8668, -4.0631, -7.4450,\n",
      "         -6.1430, -4.9942, -5.3877,  1.4567]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.2400, -7.0523, -6.0301, -4.8734, -3.5729, -4.3878, -4.4514, -4.8248,\n",
      "         -4.7637, -5.2410, -5.0155, -6.9928, -6.8442, -6.5768, -6.6644, -6.0654,\n",
      "         -6.9485, -7.2355, -7.7615, -6.1064, -4.2475, -6.9769, -3.9153, -6.4402,\n",
      "         -6.5117, -5.7280, -6.0395,  1.6376]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.0854, -6.3100, -5.8010, -4.7036, -3.9328, -6.7946, -4.3381, -4.6430,\n",
      "         -7.0078, -5.4405, -5.3226, -6.8380, -6.4055, -6.0040, -7.8292, -7.3003,\n",
      "         -7.9940, -6.5887, -8.0539, -7.0543, -6.6390, -7.4613, -3.7583, -7.6549,\n",
      "         -6.7474, -6.1949, -5.2138,  1.7421]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.4554, -6.4749, -5.5780, -4.6252, -3.1305, -5.0189, -6.1697, -6.1487,\n",
      "         -5.9654, -5.8682, -6.2127, -6.5446, -6.7627, -6.3041, -7.6396, -6.1158,\n",
      "         -7.2290, -5.8440, -7.2878, -6.9187, -5.4214, -6.1762, -4.4380, -6.4030,\n",
      "         -7.2373, -6.0238, -6.5426,  1.7238]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.1852, -7.0580, -4.7762, -3.6004, -3.7567, -5.7843, -2.8474, -3.4244,\n",
      "         -6.2644, -4.5048, -4.2473, -5.4229, -5.5515, -6.9405, -5.7945, -7.6492,\n",
      "         -7.1655, -7.5715, -8.5516, -5.4598, -5.0193, -7.5440, -3.2890, -7.2603,\n",
      "         -6.3138, -5.5244, -4.9281,  0.9049]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.5264, -6.5358, -4.4849, -3.2591, -3.9969, -5.2429, -4.2209, -4.9498,\n",
      "         -6.2121, -4.4994, -3.7961, -5.5781, -6.1043, -6.6411, -6.9559, -7.5855,\n",
      "         -7.3723, -6.9384, -7.3148, -6.3887, -5.1102, -7.0980, -3.8851, -7.2522,\n",
      "         -6.7588, -5.8587, -5.7531,  1.6124]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.1379, -6.6354, -5.4388, -4.2044, -4.6077, -5.4600, -3.6803, -3.6173,\n",
      "         -5.9470, -5.5644, -5.1685, -6.3588, -6.6448, -6.2927, -6.7100, -7.5502,\n",
      "         -7.9311, -7.0250, -7.4585, -6.3818, -5.4107, -8.1822, -4.6072, -7.8961,\n",
      "         -7.2018, -6.2634, -5.4019,  1.7176]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.3217, -6.2064, -5.7205, -4.4491, -2.6707, -5.5281, -4.4995, -3.9736,\n",
      "         -3.1303, -5.6432, -4.2587, -6.6942, -6.8189, -5.7537, -7.4922, -6.7769,\n",
      "         -7.9000, -6.4764, -6.1289, -6.7115, -4.1497, -7.2683, -4.1644, -7.4052,\n",
      "         -6.6763, -6.0884, -6.5769,  0.9736]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-4.1847, -6.4007, -1.7515, -1.8090, -2.8410, -3.3838, -6.4903, -6.1743,\n",
      "         -4.8935, -4.1786, -3.5944, -3.6154, -5.0195, -5.9593, -5.9525, -5.2069,\n",
      "         -5.8632, -6.1918, -5.2190, -5.8955, -5.1747, -5.1955, -4.2105, -6.0559,\n",
      "         -5.8760, -5.1195, -6.0443,  0.2525]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.4717, -7.2895, -5.6638, -4.1053, -3.2866, -5.5139, -3.3762, -4.3356,\n",
      "         -5.9293, -4.7995, -3.2467, -6.2272, -6.4330, -6.7691, -7.0854, -7.6826,\n",
      "         -7.8741, -7.3068, -7.5904, -6.3951, -4.7490, -7.3917, -3.6533, -7.1963,\n",
      "         -6.8173, -6.1741, -6.1409,  1.3477]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-4.5344, -6.8431, -6.4326, -5.1802, -2.8032, -5.8061, -5.2850, -5.6957,\n",
      "         -5.7503, -5.4020, -5.5040, -7.3617, -6.8761, -5.3412, -8.0377, -5.9736,\n",
      "         -7.6834, -5.8078, -7.0154, -7.0854, -4.5382, -5.2426, -3.4684, -6.2062,\n",
      "         -6.7258, -6.4728, -5.3074,  1.4639]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.8966, -6.2146, -4.7514, -3.4991, -1.3565, -5.4087, -4.5690, -4.7503,\n",
      "         -5.9525, -4.4189, -4.2369, -5.3170, -4.8832, -5.6399, -5.2701, -6.1260,\n",
      "         -6.6120, -6.3704, -8.1882, -4.9018, -4.0654, -5.8087, -2.5189, -5.3620,\n",
      "         -5.2901, -4.8473, -5.2638,  0.4246]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.8489, -6.7792, -6.2456, -4.8127, -3.6099, -5.9077, -4.1056, -4.6038,\n",
      "         -5.4562, -5.5345, -4.9685, -7.1341, -6.8338, -6.7520, -7.8415, -6.9823,\n",
      "         -8.0308, -7.0878, -7.7892, -6.9147, -5.2960, -7.7156, -4.2822, -7.6739,\n",
      "         -6.7919, -6.2678, -6.4168,  1.6179]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-4.2783, -5.9746, -6.3443, -5.3050, -3.0553, -3.9588, -5.9348, -5.3699,\n",
      "         -4.3792, -5.8835, -6.0425, -7.7136, -6.9701, -5.7629, -8.3995, -2.9320,\n",
      "         -6.9801, -5.5014, -4.3876, -7.3706, -4.4174, -5.4338, -3.9680, -6.0091,\n",
      "         -5.4661, -5.9033, -5.7868,  0.7637]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.9125, -6.7434, -5.0986, -3.4697, -3.0469, -5.3377, -4.2947, -5.1692,\n",
      "         -6.4340, -4.5232, -3.7491, -5.6139, -5.9366, -6.8521, -6.7577, -6.9662,\n",
      "         -7.3169, -6.8301, -7.8508, -6.0854, -4.6129, -6.5298, -3.0352, -6.4537,\n",
      "         -6.6032, -5.9994, -5.7123,  1.2764]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.6875, -7.3202, -5.3753, -4.3241, -4.0042, -5.7206, -3.7685, -4.1804,\n",
      "         -6.5013, -5.1857, -4.1681, -6.3064, -6.6246, -6.8415, -7.4812, -7.2684,\n",
      "         -7.9246, -7.4175, -7.1834, -6.7983, -5.9633, -7.7983, -4.2806, -7.7342,\n",
      "         -6.8252, -6.1508, -5.9178,  1.6349]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.7979, -4.3705, -3.1032, -2.3190, -4.4360, -6.6704, -4.9829, -5.3199,\n",
      "         -6.8191, -4.7329, -3.5213, -4.2659, -5.6191, -5.0100, -6.6381, -7.9407,\n",
      "         -7.8170, -5.3164, -7.1310, -6.5696, -6.4572, -7.7424, -4.9417, -8.1276,\n",
      "         -7.3530, -6.2645, -5.1823,  1.0673]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.9731, -6.9410, -5.0685, -4.1109, -4.1871, -6.3169, -3.7825, -4.1404,\n",
      "         -6.8384, -5.3520, -4.2168, -5.9507, -6.4042, -6.5664, -7.2013, -7.2564,\n",
      "         -8.1157, -7.3162, -7.7052, -6.7792, -6.4363, -8.0500, -4.3685, -7.8785,\n",
      "         -7.1210, -6.4456, -5.6099,  1.6333]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.6289, -6.8025, -3.7610, -3.6230, -4.7091, -6.5034, -5.6146, -5.6177,\n",
      "         -8.0482, -5.0418, -4.7356, -4.4809, -5.5494, -6.7955, -6.2120, -6.9419,\n",
      "         -7.1166, -7.6316, -8.1980, -6.4866, -8.0590, -8.0403, -5.0029, -7.9043,\n",
      "         -6.5926, -5.2734, -5.4687,  1.1863]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.3576, -7.0350, -6.4509, -4.9724, -3.3238, -4.9312, -2.9275, -3.8765,\n",
      "         -5.2128, -5.4637, -4.5233, -7.1395, -7.0276, -6.7633, -7.0457, -6.9451,\n",
      "         -7.9012, -7.2608, -7.3781, -6.3282, -3.6965, -7.3346, -3.8624, -7.1807,\n",
      "         -7.0442, -6.3869, -5.9270,  1.2100]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.0555, -6.6249, -6.2074, -4.8182, -3.2895, -5.4496, -5.9093, -6.7327,\n",
      "         -6.1027, -5.3247, -5.2979, -6.9717, -7.0534, -6.8834, -7.7973, -6.4248,\n",
      "         -7.5231, -6.5618, -7.4638, -7.0779, -5.0604, -6.8883, -4.0697, -7.0711,\n",
      "         -7.1345, -5.7064, -6.8558,  1.5723]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.4189, -6.6462, -5.6278, -3.9611, -3.4718, -5.4295, -5.4618, -6.0675,\n",
      "         -5.8031, -4.3535, -4.4962, -6.5125, -6.5282, -6.2439, -7.5609, -6.4982,\n",
      "         -7.3196, -6.6651, -7.0162, -6.7009, -4.8910, -6.4373, -4.0597, -7.0022,\n",
      "         -6.4285, -5.6412, -6.0968,  1.7195]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.4100, -6.1168, -5.1407, -3.3447, -3.6833, -4.7334, -1.0755, -3.3090,\n",
      "         -5.6879, -4.1522, -3.6079, -5.7843, -5.3976, -6.5686, -6.5760, -6.9690,\n",
      "         -7.1548, -5.9571, -6.4464, -5.6486, -4.2415, -7.1992, -3.2613, -6.7013,\n",
      "         -6.2726, -5.3729, -5.2347,  0.3966]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.7052, -6.9449, -4.9861, -3.9197, -4.3877, -6.2851, -3.5794, -4.0562,\n",
      "         -7.0964, -4.9467, -4.1868, -5.6642, -6.2066, -6.6393, -6.9343, -7.2444,\n",
      "         -7.7683, -7.2422, -7.7101, -6.5123, -6.2367, -7.7293, -4.0742, -7.6798,\n",
      "         -6.9265, -6.1660, -5.4154,  1.5443]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-4.8397, -5.9379, -4.1870, -1.9356, -3.6271, -5.1401, -4.6885, -4.7011,\n",
      "         -5.5186, -2.5120, -2.2791, -3.7606, -3.0351, -5.7245, -5.4363, -3.1438,\n",
      "         -5.6302, -5.9290, -6.8058, -4.7292, -5.3471, -5.7468, -2.7032, -4.9888,\n",
      "         -3.7157, -4.2519, -5.1371, -0.3589]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.2795, -6.4879, -5.3297, -4.3986, -3.8367, -5.5809, -4.0969, -4.9621,\n",
      "         -6.3166, -5.2682, -4.7426, -6.2513, -6.4957, -6.7246, -6.8875, -7.3115,\n",
      "         -7.2509, -6.8872, -7.1361, -6.3363, -5.5904, -7.8669, -4.3130, -7.4508,\n",
      "         -6.6394, -5.5515, -6.0386,  1.7675]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.5359, -6.7098, -5.5349, -4.2610, -3.7710, -5.7682, -3.8981, -3.8906,\n",
      "         -5.4444, -4.8570, -4.3854, -6.6936, -6.1176, -6.1897, -7.4745, -6.5511,\n",
      "         -7.3630, -6.9013, -7.5126, -6.6136, -5.3215, -7.1450, -3.8373, -7.1859,\n",
      "         -5.9340, -5.7531, -5.9132,  1.5598]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.5636, -6.9618, -6.5665, -5.4314, -3.2191, -4.7799, -6.3757, -5.0801,\n",
      "         -4.2544, -6.4303, -6.2642, -7.7649, -8.0189, -4.9164, -7.3903,  1.4910,\n",
      "         -7.9035, -5.4848, -6.9774, -6.8354, -3.7066, -6.5562, -4.3569, -5.3715,\n",
      "         -6.6714, -6.8757, -6.5058, -1.7671]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.5735, -7.0322, -4.7831, -3.4999, -3.9946, -3.6359, -4.8934, -5.1637,\n",
      "         -5.6119, -3.0786, -3.6400, -5.2309, -4.8784, -7.1275, -4.7933, -4.9972,\n",
      "         -4.7291, -6.9929, -7.1915, -4.3079, -5.0076, -6.8547, -3.1065, -5.5238,\n",
      "         -3.9628, -2.9380, -5.8904,  0.6535]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.7499, -6.8055, -5.9988, -4.6667, -3.5418, -5.3712, -4.5153, -4.9716,\n",
      "         -6.1117, -5.3375, -5.1221, -7.2824, -6.8221, -6.0332, -7.3918, -7.0550,\n",
      "         -7.6393, -6.5689, -7.6672, -6.4558, -5.2602, -7.2673, -3.8015, -7.0481,\n",
      "         -6.8605, -6.0190, -5.7232,  1.7532]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.3899, -6.5680, -5.7601, -4.4398, -3.4619, -5.8948, -3.7362, -4.2682,\n",
      "         -4.8907, -5.3029, -3.9060, -6.6429, -6.7007, -6.1338, -7.7817, -7.3997,\n",
      "         -8.0674, -6.4460, -5.8636, -6.9210, -4.8082, -7.3194, -3.8117, -7.5465,\n",
      "         -6.8923, -6.2573, -5.7889,  1.4433]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.9925, -5.9524, -5.5123, -3.7091, -3.4250, -7.1976, -1.4567, -4.0839,\n",
      "         -6.6496, -4.6878, -2.8433, -5.5275, -5.5235, -6.8556, -7.5895, -7.7115,\n",
      "         -8.3556, -6.6798, -8.0389, -6.9576, -5.6918, -7.7047, -3.5217, -7.7481,\n",
      "         -6.8299, -6.3801, -5.1231,  0.4816]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.9693, -7.2066, -4.6664, -3.5615, -4.1884, -5.7185, -4.2595, -4.3571,\n",
      "         -5.8111, -4.2498, -3.8449, -5.3613, -5.6826, -6.6140, -6.7882, -7.3492,\n",
      "         -6.9533, -7.4035, -7.8774, -5.9364, -6.2482, -7.5206, -3.9231, -7.3344,\n",
      "         -5.8694, -5.2394, -5.4831,  1.4261]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.2842, -6.6073, -4.0294, -2.8657, -4.3840, -5.8860, -3.7882, -4.6026,\n",
      "         -6.6563, -4.0905, -2.8560, -4.2698, -5.1795, -7.3270, -6.1304, -7.5642,\n",
      "         -7.1239, -7.6009, -8.2281, -5.8026, -5.7263, -7.9508, -3.7287, -7.5147,\n",
      "         -6.2235, -5.3594, -5.4083,  0.9712]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.4117, -6.8957, -4.5217, -3.7412, -4.3981, -5.2537, -3.4851, -3.8456,\n",
      "         -6.4683, -4.8245, -4.0622, -5.5395, -5.9238, -6.7672, -5.5954, -7.9228,\n",
      "         -7.0790, -7.5385, -8.0002, -5.5644, -5.5071, -8.0763, -4.2180, -7.6937,\n",
      "         -6.3640, -5.4070, -5.4210,  1.4513]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.8586, -6.3616, -6.6283, -5.9845, -4.4320, -6.2643, -6.2335, -5.3681,\n",
      "         -5.6817, -6.8541, -7.0828, -7.8499, -8.2522, -5.5572, -8.3399,  1.0161,\n",
      "         -8.5482, -5.3812, -6.1814, -7.6607, -6.2748, -7.8720, -5.3021, -6.7347,\n",
      "         -7.3356, -6.9322, -6.9777, -2.2948]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.0026, -6.3002, -6.6204, -5.9722, -3.5196, -6.6144, -5.7254, -5.5730,\n",
      "         -6.8476, -6.7689, -6.4684, -7.6310, -7.7549, -4.6366, -7.9479, -5.8779,\n",
      "         -8.5124, -5.3647, -6.9605, -7.5323, -5.8078, -6.5018, -4.6111, -7.0564,\n",
      "         -8.1862, -7.4946, -5.4004,  1.7596]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.2878, -7.1241, -4.7555, -3.2977, -3.6349, -5.6413, -4.3734, -4.9947,\n",
      "         -6.5789, -4.0551, -2.9178, -4.6987, -5.1726, -6.7662, -6.1766, -7.0740,\n",
      "         -7.0680, -7.3782, -7.5511, -5.8692, -5.7625, -7.3613, -3.7129, -6.9941,\n",
      "         -6.1061, -5.1800, -5.7660,  1.0766]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.5187, -5.9512, -4.6770, -2.5766, -2.7792, -5.2908, -4.3306, -5.0545,\n",
      "         -6.0432, -3.4767, -2.5861, -4.0158, -4.7441, -6.1580, -6.0628, -5.3498,\n",
      "         -6.5706, -5.9660, -6.9462, -5.7189, -4.6490, -5.6824, -3.5383, -5.7751,\n",
      "         -5.7623, -5.1251, -5.6567,  0.4616]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.1796, -6.6485, -4.5196, -3.0427, -3.8013, -5.4826, -3.3182, -4.3761,\n",
      "         -5.9687, -4.6380, -4.0184, -5.5483, -5.9397, -7.0472, -6.5530, -8.2269,\n",
      "         -7.4638, -7.2202, -7.9221, -6.0474, -4.7320, -7.5940, -3.9476, -7.5513,\n",
      "         -6.6604, -5.6717, -6.1637,  1.3908]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.2009, -6.2892, -4.4759, -3.6182, -3.9175, -5.9978, -4.6933, -4.9570,\n",
      "         -6.6385, -5.0395, -4.4863, -5.0672, -5.6420, -6.5418, -6.7226, -7.5015,\n",
      "         -7.3121, -6.9854, -7.8192, -6.2304, -6.0070, -7.6445, -4.1110, -7.5505,\n",
      "         -6.4879, -5.7913, -5.6528,  1.5890]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.9571, -5.7822, -3.6791, -2.6838, -4.6500, -7.4845, -4.6219, -4.7848,\n",
      "         -7.3260, -4.0199, -3.4818, -4.1985, -4.6051, -5.9099, -6.6333, -7.3020,\n",
      "         -7.4660, -6.8853, -8.6288, -6.3174, -7.3606, -7.3226, -4.3725, -7.8353,\n",
      "         -6.4550, -5.8487, -4.2286,  0.8837]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.4744, -6.3620, -5.0968, -3.8699, -3.6884, -5.7928, -4.7555, -5.5320,\n",
      "         -6.8645, -4.4088, -3.5460, -5.5246, -5.5121, -6.6873, -7.0571, -7.4169,\n",
      "         -7.1014, -6.8778, -7.1441, -6.3270, -6.5198, -7.2976, -3.6930, -7.2345,\n",
      "         -6.0804, -5.4132, -5.7657,  1.5705]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.2995, -7.0406, -5.1994, -4.3215, -4.1428, -5.2706, -3.1161, -3.5206,\n",
      "         -5.6389, -5.4623, -5.0166, -6.7177, -6.6799, -6.2450, -7.2325, -7.2491,\n",
      "         -7.7367, -6.9662, -7.2520, -6.4259, -4.8839, -7.6155, -4.1182, -7.5181,\n",
      "         -6.7353, -6.2441, -5.6652,  1.5211]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-4.4894, -6.3107, -5.5709, -3.9259, -3.7143, -5.9069, -3.9891, -4.9176,\n",
      "         -6.5324, -4.1035, -2.9841, -5.9840, -6.2208, -6.1020, -7.1626, -6.7216,\n",
      "         -7.5256, -6.4845, -7.1055, -6.7130, -5.0954, -6.4213, -3.5960, -6.8363,\n",
      "         -6.7368, -5.7999, -5.0973,  1.3144]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.9079, -6.5703, -4.8267, -2.8285, -3.4505, -3.0951, -4.1676, -5.0377,\n",
      "         -4.0796, -3.6563, -2.2394, -5.2892, -5.5461, -7.5681, -6.1069, -6.9203,\n",
      "         -6.5866, -7.4849, -7.4167, -5.3864, -3.7424, -7.2954, -3.4038, -6.3539,\n",
      "         -5.5047, -4.8954, -6.6786,  0.4582]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.0399, -6.6666, -4.4877, -3.0040, -3.8344, -6.3832, -3.5068, -5.0329,\n",
      "         -6.4515, -2.6324, -2.0203, -4.7901, -4.8193, -7.5225, -6.3669, -7.3093,\n",
      "         -6.5134, -7.9699, -8.9025, -5.6123, -5.4702, -7.6400, -3.1762, -7.4882,\n",
      "         -5.0051, -4.1496, -5.3459,  0.4261]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.5225, -7.1881, -5.3167, -4.2288, -3.9703, -5.4420, -3.0872, -3.5595,\n",
      "         -6.0574, -5.3381, -4.1467, -6.2614, -6.4840, -6.4129, -7.1741, -6.9483,\n",
      "         -7.9099, -7.0838, -6.7639, -6.5993, -5.4027, -7.6638, -4.1623, -7.4055,\n",
      "         -6.6535, -6.3431, -5.7781,  1.3996]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.1429, -6.5947, -5.9588, -5.0365, -3.5231, -5.7370, -4.3991, -4.6614,\n",
      "         -6.0669, -6.1878, -5.4926, -7.3363, -6.9796, -6.2253, -7.8300, -7.1449,\n",
      "         -8.1012, -6.6034, -7.4640, -6.8752, -5.6965, -7.7031, -4.3685, -7.5252,\n",
      "         -7.0563, -6.4885, -6.1861,  1.7589]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.3133, -6.2355, -5.9375, -4.6099, -3.0126, -4.2483, -4.9302, -5.0743,\n",
      "         -4.0943, -5.7207, -5.4559, -7.0443, -7.0586, -6.5535, -7.1709, -7.2398,\n",
      "         -7.5499, -6.8172, -7.7357, -6.5785, -3.5078, -7.1632, -4.6529, -7.0025,\n",
      "         -6.9205, -6.0241, -6.7965,  1.3607]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.6831, -6.8720, -3.6878, -2.5701, -2.8252, -5.0999, -5.1778, -5.9015,\n",
      "         -6.2142, -2.5478, -2.0965, -4.2669, -4.7336, -7.3262, -5.8250, -5.7266,\n",
      "         -5.8473, -7.8116, -8.5480, -5.1667, -5.1841, -6.3164, -3.1756, -6.4830,\n",
      "         -4.8202, -3.8203, -5.7548,  0.1869]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.4196, -6.4531, -4.5764, -3.4891, -3.1485, -6.4914, -5.3842, -5.6557,\n",
      "         -6.6953, -4.0155, -4.0723, -4.7603, -5.3278, -6.4134, -6.2817, -6.7203,\n",
      "         -6.6875, -6.8992, -7.8674, -6.0010, -6.3588, -6.4406, -3.4947, -6.7975,\n",
      "         -6.1627, -5.2646, -5.3319,  1.1868]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.1439, -6.8248, -3.4764, -2.5901, -4.3501, -5.3715, -3.8039, -4.1238,\n",
      "         -5.5842, -3.9507, -3.4342, -4.7109, -5.0520, -6.7500, -6.6003, -7.3964,\n",
      "         -6.9135, -7.7043, -7.7915, -5.9852, -5.4481, -7.6379, -4.1093, -7.7479,\n",
      "         -5.3347, -5.1224, -5.9713,  0.9908]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.6253, -6.8128, -5.8110, -4.5006, -3.9072, -5.4990, -2.8471, -3.3306,\n",
      "         -5.7854, -5.8537, -4.6204, -6.5623, -6.7000, -6.2593, -7.3874, -6.7122,\n",
      "         -8.2052, -6.6547, -6.3907, -6.7051, -5.3431, -7.7664, -4.0913, -7.2930,\n",
      "         -7.0378, -6.8009, -5.6568,  1.3941]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.9495, -6.8360, -5.0703, -3.9800, -4.2248, -5.7684, -3.5108, -4.4987,\n",
      "         -6.5691, -4.2484, -3.4173, -5.7701, -5.7850, -6.3794, -6.4777, -7.2425,\n",
      "         -6.9589, -7.0476, -7.3686, -5.7692, -5.7863, -7.5142, -3.7542, -7.0419,\n",
      "         -6.0661, -5.1560, -5.2516,  1.4366]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.2398, -6.2196, -4.4685, -3.0030, -3.2953, -6.1206, -5.6740, -6.5914,\n",
      "         -7.3383, -2.8437, -2.1424, -4.6071, -4.3463, -6.6105, -6.2131, -6.3825,\n",
      "         -6.2946, -6.8227, -7.9125, -5.4051, -6.5770, -6.6772, -2.8302, -6.4035,\n",
      "         -4.9979, -4.2718, -5.4724,  0.5075]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.1335, -6.8565, -4.9219, -3.5194, -2.3758, -2.4302, -4.8329, -3.3526,\n",
      "         -1.8082, -5.3696, -4.7776, -6.4178, -6.5559, -5.0014, -5.9804, -4.7605,\n",
      "         -7.0581, -6.0773, -6.7026, -5.4782, -2.2249, -6.5871, -4.0846, -5.6920,\n",
      "         -6.1778, -5.7420, -6.1259, -0.3359]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-4.1299, -6.7771, -6.0236, -4.4726, -1.7394, -2.2563, -5.1289, -4.5568,\n",
      "         -2.4864, -4.9008, -5.1046, -6.8630, -6.1826, -5.7284, -7.1129, -3.9621,\n",
      "         -6.2726, -5.7936, -4.6125, -5.9389, -3.0213, -5.4809, -3.6187, -5.2678,\n",
      "         -4.9036, -4.7421, -6.4788,  0.0796]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-4.9833, -6.1058, -5.0602, -3.3500, -2.9582, -5.0505, -3.4974, -3.2788,\n",
      "         -2.4220, -3.9211, -3.6414, -5.7926, -6.0956, -4.0502, -5.7805, -6.6649,\n",
      "         -7.5305, -5.6616, -6.2798, -5.6695, -0.7570, -5.9914, -3.9406, -6.9509,\n",
      "         -6.2559, -5.6096, -5.5117, -0.2866]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.6691, -6.8532, -3.2622, -2.3350, -4.1898, -5.5632, -5.1500, -5.8096,\n",
      "         -6.7337, -4.2191, -3.0980, -4.7376, -5.6170, -7.0479, -7.0134, -7.4530,\n",
      "         -7.3309, -7.4090, -8.2113, -6.4275, -6.0709, -7.4490, -4.4654, -7.7037,\n",
      "         -6.4483, -5.6308, -6.2624,  1.1617]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.1119, -6.7371, -4.5686, -3.5406, -3.4550, -4.2015, -2.7637, -3.0634,\n",
      "         -5.3444, -3.2368, -3.0631, -5.2357, -5.4734, -6.8208, -3.4917, -7.0566,\n",
      "         -5.5469, -7.4398, -8.0119, -3.6027, -4.2819, -7.2256, -2.8586, -6.2038,\n",
      "         -5.4332, -3.6828, -4.8258,  0.3511]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.4996, -6.5601, -5.2209, -3.9411, -3.4946, -6.3650, -5.1413, -5.6726,\n",
      "         -6.5070, -5.2714, -3.9472, -6.4005, -6.7675, -6.4259, -7.6418, -7.3890,\n",
      "         -8.1832, -6.8894, -7.6915, -7.0858, -6.0294, -7.3245, -4.2146, -7.5321,\n",
      "         -7.6744, -6.6981, -5.9991,  1.9419]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.0033, -5.7711, -4.8415, -3.9829, -4.2602, -4.8646, -4.4336, -4.5566,\n",
      "         -6.3541, -3.2323, -3.9637, -5.5612, -3.9394, -6.7009, -6.3590, -4.7050,\n",
      "         -5.0489, -6.9456, -7.5615, -5.2848, -6.5582, -7.5313, -3.4851, -6.6405,\n",
      "         -1.8246, -2.3666, -5.5888, -0.2449]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.3986, -6.3895, -4.9240, -3.0500, -2.7311, -5.3926, -2.0452, -3.9271,\n",
      "         -6.1460, -4.1623, -2.5909, -5.1153, -5.0809, -7.1373, -6.2420, -7.5721,\n",
      "         -7.3523, -7.1466, -7.8219, -5.5679, -4.4637, -7.2373, -2.9397, -6.7918,\n",
      "         -6.0748, -5.4659, -5.7044,  0.4308]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.0736, -7.1228, -4.2482, -3.2309, -4.3624, -5.4664, -4.2698, -4.6178,\n",
      "         -6.8327, -3.4379, -3.6058, -4.3215, -4.7974, -6.9229, -5.0084, -7.6645,\n",
      "         -5.8275, -7.4446, -7.8249, -4.6896, -6.6074, -7.7932, -3.8414, -7.1235,\n",
      "         -5.5242, -3.6260, -4.9968,  1.0557]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.6112, -5.6667, -2.9601, -2.5512, -3.4414, -5.6366, -5.9665, -6.0823,\n",
      "         -5.4712, -3.8893, -3.6137, -3.7614, -4.8133, -6.4373, -6.1108, -7.6070,\n",
      "         -6.3655, -6.7743, -7.9039, -5.8445, -5.8870, -7.1970, -4.2785, -7.3854,\n",
      "         -5.6039, -4.5671, -6.3436,  0.7200]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.2178, -6.8814, -4.8840, -3.4078, -3.9270, -5.2133, -3.9786, -4.6207,\n",
      "         -6.1814, -3.8682, -3.4331, -5.7429, -5.5483, -6.5313, -5.9164, -7.7993,\n",
      "         -6.6644, -7.1512, -8.1112, -5.1049, -5.0276, -7.4877, -3.4438, -7.0168,\n",
      "         -5.8701, -4.6518, -5.0061,  1.3865]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.7783, -6.5704, -6.3214, -4.7748, -3.7330, -5.7619, -3.8511, -4.8542,\n",
      "         -6.3647, -4.9692, -4.8877, -7.0529, -6.8430, -6.3107, -7.6396, -6.1261,\n",
      "         -7.3701, -6.0639, -7.6500, -6.8224, -4.8480, -6.4426, -3.1598, -6.3735,\n",
      "         -6.6997, -6.1303, -5.3292,  1.5791]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.8492, -6.7018, -4.6815, -3.2262, -3.7383, -4.7145, -3.9433, -4.8704,\n",
      "         -4.8394, -2.1942, -1.7009, -4.9235, -4.3874, -6.3853, -5.5835, -1.3803,\n",
      "         -5.0330, -6.3286, -6.9911, -4.7739, -4.8872, -6.3370, -2.4556, -4.8557,\n",
      "         -2.7908, -3.1552, -4.9607, -1.1458]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.1314, -7.0151, -5.4976, -4.0799, -3.0613, -5.6754, -3.6470, -4.1650,\n",
      "         -6.1522, -4.6412, -3.9450, -5.9078, -5.5754, -7.0131, -6.3059, -7.2219,\n",
      "         -7.1451, -7.5394, -8.2641, -5.7340, -5.5944, -7.7816, -3.6143, -7.2164,\n",
      "         -5.6433, -5.1931, -6.2751,  1.1758]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.4531, -5.6778, -5.3579, -3.4289, -3.3539, -5.8182, -3.3601, -4.8380,\n",
      "         -5.1775, -4.2623, -2.5186, -5.3796, -5.2201, -6.6329, -7.1528, -7.2759,\n",
      "         -7.4101, -6.5573, -6.4923, -6.2366, -4.5818, -7.1088, -3.1301, -6.9902,\n",
      "         -5.8888, -5.4834, -5.5818,  1.0322]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.7139, -6.6673, -5.0227, -3.9377, -4.3134, -6.6566, -3.7603, -4.7329,\n",
      "         -6.4879, -4.8074, -3.2313, -5.7768, -6.1581, -6.5353, -7.3805, -7.7216,\n",
      "         -7.9707, -7.1153, -7.7348, -6.8660, -6.3428, -7.8992, -4.5131, -8.0215,\n",
      "         -6.9423, -6.1588, -5.6085,  1.4669]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.5401, -6.9672, -5.9351, -4.8945, -3.7442, -5.2867, -4.5782, -4.9835,\n",
      "         -6.0593, -5.5410, -5.3057, -6.9117, -7.0145, -6.4362, -7.3860, -7.1689,\n",
      "         -7.4583, -6.9897, -7.1534, -6.6521, -5.2198, -7.3446, -4.1957, -7.1674,\n",
      "         -6.9049, -5.8853, -5.9538,  1.8426]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.8001, -6.6341, -5.1732, -3.6110, -3.7918, -5.6334, -2.0088, -3.4451,\n",
      "         -5.5126, -4.1863, -2.9529, -5.4676, -5.4845, -6.7952, -5.7154, -7.8549,\n",
      "         -7.2126, -7.2531, -7.5903, -5.4073, -4.6168, -7.6525, -3.2174, -7.1761,\n",
      "         -6.4017, -5.2855, -4.9512,  1.0093]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-7.1429, -6.5983, -4.5189, -3.2355, -4.0104, -6.1616, -4.3252, -5.3525,\n",
      "         -7.1781, -3.0827, -2.7739, -4.3835, -4.9124, -7.2619, -5.7693, -7.3805,\n",
      "         -6.3703, -7.5083, -8.3895, -5.2710, -6.5993, -8.0175, -3.8261, -7.3808,\n",
      "         -5.3950, -3.9881, -5.7211,  0.8439]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.7248, -7.1032, -2.8862, -1.9300, -3.2221, -5.0348, -5.0514, -5.6543,\n",
      "         -5.5829, -2.1733, -1.0087, -2.8858, -4.3478, -7.2812, -5.3007, -6.6695,\n",
      "         -6.0815, -7.8524, -7.7639, -5.0558, -5.2720, -7.4855, -3.6103, -7.2964,\n",
      "         -4.6921, -3.5354, -6.5308, -0.8167]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.5446, -7.2115, -5.9951, -4.8451, -3.6430, -5.4733, -4.3022, -4.7371,\n",
      "         -6.1622, -5.3987, -4.6331, -7.0995, -6.8748, -6.5974, -7.6688, -6.5809,\n",
      "         -7.7835, -7.0995, -7.7312, -6.8402, -5.2661, -7.2048, -3.9506, -7.1263,\n",
      "         -6.6279, -6.3783, -6.0286,  1.6325]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.4574, -6.8700, -5.6758, -4.2715, -4.0052, -5.6228, -3.7335, -4.2069,\n",
      "         -5.5331, -5.2343, -4.4912, -6.4850, -6.7374, -6.6178, -7.1424, -7.6410,\n",
      "         -7.9588, -7.2508, -7.7414, -6.6965, -4.6124, -7.5971, -4.1545, -7.6290,\n",
      "         -7.0801, -6.3866, -5.7733,  1.7526]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.4528, -5.8542, -4.6056, -2.9490, -3.8516, -5.7346, -3.1260, -3.3576,\n",
      "         -4.1882, -4.3349, -3.0839, -5.0721, -5.4927, -6.1252, -7.0525, -7.8645,\n",
      "         -7.6644, -6.5393, -6.5191, -6.3880, -4.3483, -7.4706, -3.8520, -7.7240,\n",
      "         -6.3797, -5.8711, -5.7719,  1.0215]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.2127, -6.8203, -6.0772, -4.3882, -3.1900, -5.3231, -3.6182, -4.2583,\n",
      "         -4.9395, -4.9397, -4.6354, -6.5868, -6.5371, -6.0760, -6.9229, -7.4208,\n",
      "         -7.6016, -6.4784, -7.0036, -6.1036, -3.9710, -7.0688, -3.6047, -6.8277,\n",
      "         -7.0602, -5.8980, -5.7564,  1.4776]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.7467, -6.7111, -6.1507, -5.2661, -3.5475, -5.4332, -3.2982, -3.5963,\n",
      "         -4.7861, -5.9166, -5.4953, -7.4715, -7.3724, -5.9460, -7.4791, -7.4969,\n",
      "         -8.1213, -6.6625, -6.8919, -6.7109, -3.9382, -7.3759, -4.1843, -7.6066,\n",
      "         -7.3528, -6.5486, -6.0080,  1.4239]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.6837, -6.8402, -6.0248, -4.7742, -3.6898, -3.8792, -3.0242, -3.4263,\n",
      "         -4.5064, -5.3294, -4.9927, -7.1228, -6.8636, -6.7226, -6.7250, -7.0574,\n",
      "         -7.3403, -7.2057, -6.8657, -5.9249, -3.5207, -7.4497, -3.9117, -7.0277,\n",
      "         -6.4710, -5.7832, -6.0022,  1.1917]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.9999, -7.0860, -5.9652, -4.5376, -3.7636, -5.6176, -4.1110, -4.6664,\n",
      "         -5.7887, -5.4084, -4.7277, -6.6762, -6.7104, -6.8186, -7.1394, -7.5329,\n",
      "         -7.8463, -7.3235, -7.8009, -6.5732, -5.0172, -7.6650, -3.7982, -7.3551,\n",
      "         -7.0152, -6.2099, -6.1041,  1.7524]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.6882, -6.4575, -2.6400, -1.9705, -4.0363, -3.8309, -5.4600, -6.0625,\n",
      "         -6.5959, -2.4628, -1.2014, -4.1714, -4.3467, -7.0071, -6.0689, -5.7827,\n",
      "         -5.5029, -6.9668, -7.2479, -5.1577, -5.8389, -6.9502, -3.6641, -6.3892,\n",
      "         -3.9943, -3.2708, -6.5129, -0.5125]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.8317, -6.4579, -5.9601, -5.1591, -3.7250, -6.7436, -5.9504, -6.4594,\n",
      "         -7.4225, -6.3725, -6.0687, -6.7286, -7.3943, -5.9491, -7.9001, -7.1181,\n",
      "         -8.5370, -6.1064, -7.0552, -7.6533, -6.4688, -7.3770, -5.2010, -7.7954,\n",
      "         -8.1799, -6.9383, -6.3761,  1.8235]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.0811, -6.3323, -6.4056, -5.1507, -3.7145, -7.2171, -5.0568, -5.4311,\n",
      "         -7.1818, -6.2190, -5.4671, -7.3025, -7.3739, -6.0330, -8.6639, -7.1788,\n",
      "         -9.0963, -6.6487, -7.9423, -8.0258, -6.5257, -7.6421, -4.5381, -8.2362,\n",
      "         -7.9589, -7.3057, -5.8941,  1.7699]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.2532, -6.9492, -5.8205, -4.9027, -3.5842, -6.3437, -4.4047, -4.9149,\n",
      "         -7.1041, -5.9479, -5.4982, -6.7667, -6.9176, -6.2634, -8.0696, -6.9160,\n",
      "         -8.2856, -6.7869, -7.6525, -7.2167, -6.1633, -7.2073, -4.0207, -7.4140,\n",
      "         -7.4544, -6.9566, -5.6219,  1.7750]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.8711, -6.4227, -6.4743, -4.8428, -3.2587, -5.3357, -3.7881, -4.3881,\n",
      "         -4.7839, -5.1542, -4.3154, -6.9817, -6.9051, -6.1502, -7.1136, -7.2677,\n",
      "         -7.7707, -6.7165, -7.4286, -6.4528, -3.6169, -6.6450, -3.3537, -6.7537,\n",
      "         -6.6837, -6.2333, -5.7599,  1.5642]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.2081, -6.1952, -6.1544, -4.5703, -2.8306, -2.9681, -4.4705, -5.1295,\n",
      "         -2.7718, -4.2591, -3.8673, -6.2911, -6.5065, -6.2162, -5.7474,  0.0625,\n",
      "         -6.6856, -6.0794, -6.5556, -5.5150,  0.5443, -6.3932, -3.7011, -5.5140,\n",
      "         -4.4782, -4.7458, -6.1527, -3.3126]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-4.0081, -6.8373, -6.3563, -5.0029, -3.3142, -5.5703, -4.2975, -4.2229,\n",
      "         -5.1477, -5.2702, -5.1841, -7.0703, -7.2242, -5.5990, -7.7692, -6.1034,\n",
      "         -7.9148, -6.4436, -6.7000, -7.2436, -4.4028, -6.0495, -4.0389, -7.0286,\n",
      "         -7.2154, -6.5768, -6.0181,  1.3896]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.9336, -7.1628, -4.3577, -3.0773, -2.9954, -4.9512, -4.1696, -4.7227,\n",
      "         -5.8542, -4.6308, -2.6088, -6.0219, -6.3483, -6.5577, -6.7332, -7.4340,\n",
      "         -7.6955, -7.2489, -8.1290, -6.2681, -4.6302, -6.7706, -3.2735, -6.7317,\n",
      "         -6.9882, -6.2050, -5.0528,  0.9799]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.9106, -7.0095, -5.7418, -4.4468, -3.7966, -4.9562, -4.7391, -4.7278,\n",
      "         -5.3086, -5.5187, -5.2102, -6.9761, -6.8385, -6.4683, -7.0006, -6.0289,\n",
      "         -7.4205, -7.0832, -7.8136, -6.4423, -5.2366, -7.5928, -4.2644, -6.8905,\n",
      "         -6.5923, -5.9619, -6.1494,  1.5719]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.5580, -7.3030, -5.8223, -4.6506, -3.6103, -5.8901, -4.1905, -4.7979,\n",
      "         -5.6984, -4.7495, -4.5055, -6.8574, -6.9480, -6.4986, -7.5213, -7.4538,\n",
      "         -7.5777, -7.0850, -7.3062, -6.5929, -5.1339, -7.3555, -4.0700, -7.3785,\n",
      "         -6.8639, -5.8594, -6.2126,  1.6303]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5.9894, -6.0606, -6.0066, -4.3382, -3.3434, -6.1996, -1.9049, -3.8096,\n",
      "         -6.0434, -4.4914, -3.5566, -6.2352, -5.8960, -6.0634, -6.5666, -7.4221,\n",
      "         -7.6832, -6.5182, -7.4598, -6.0201, -4.2065, -6.9306, -2.5330, -6.8472,\n",
      "         -6.5037, -6.0339, -3.7556,  0.8425]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 8.00 GiB total capacity; 7.29 GiB already allocated; 0 bytes free; 7.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_consumer[\u001b[39m'\u001b[39m\u001b[39mpredicted_label\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_consumer[\u001b[39m'\u001b[39;49m\u001b[39mclean_text\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(predict_label)\n",
      "File \u001b[1;32mc:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1175\u001b[0m             values,\n\u001b[0;32m   1176\u001b[0m             f,\n\u001b[0;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1178\u001b[0m         )\n\u001b[0;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[23], line 3\u001b[0m, in \u001b[0;36mpredict_label\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict_label\u001b[39m(text):\n\u001b[0;32m      2\u001b[0m     inputs \u001b[39m=\u001b[39m tokenizer(text, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> 3\u001b[0m     outputs\u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[0;32m      4\u001b[0m     predict_label \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits\n\u001b[0;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(predict_label)\n",
      "File \u001b[1;32mc:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[21], line 26\u001b[0m, in \u001b[0;36mDistilBertForMultilabelSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     24\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m---> 26\u001b[0m distilbert_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistilbert(\n\u001b[0;32m     27\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m     28\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m     29\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m     30\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m     31\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m     32\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m     33\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m     34\u001b[0m )\n\u001b[0;32m     35\u001b[0m hidden_state \u001b[39m=\u001b[39m distilbert_output[\u001b[39m0\u001b[39m]  \u001b[39m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[0;32m     36\u001b[0m pooled_output \u001b[39m=\u001b[39m hidden_state[:, \u001b[39m0\u001b[39m]  \u001b[39m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:583\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    579\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    581\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(input_ids, inputs_embeds)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[0;32m    584\u001b[0m     x\u001b[39m=\u001b[39;49membeddings,\n\u001b[0;32m    585\u001b[0m     attn_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    586\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    587\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    588\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    589\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    590\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:359\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m    357\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_state,)\n\u001b[1;32m--> 359\u001b[0m layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    360\u001b[0m     x\u001b[39m=\u001b[39;49mhidden_state, attn_mask\u001b[39m=\u001b[39;49mattn_mask, head_mask\u001b[39m=\u001b[39;49mhead_mask[i], output_attentions\u001b[39m=\u001b[39;49moutput_attentions\n\u001b[0;32m    361\u001b[0m )\n\u001b[0;32m    362\u001b[0m hidden_state \u001b[39m=\u001b[39m layer_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    364\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:314\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[39m# Feed Forward Network\u001b[39;00m\n\u001b[0;32m    313\u001b[0m ffn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffn(sa_output)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m--> 314\u001b[0m ffn_output: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_layer_norm(ffn_output \u001b[39m+\u001b[39;49m sa_output)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m    316\u001b[0m output \u001b[39m=\u001b[39m (ffn_output,)\n\u001b[0;32m    317\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\nn\\modules\\normalization.py:190\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 190\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlayer_norm(\n\u001b[0;32m    191\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalized_shape, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps)\n",
      "File \u001b[1;32mc:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\nn\\functional.py:2515\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2511\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[0;32m   2512\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2513\u001b[0m         layer_norm, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, normalized_shape, weight\u001b[39m=\u001b[39mweight, bias\u001b[39m=\u001b[39mbias, eps\u001b[39m=\u001b[39meps\n\u001b[0;32m   2514\u001b[0m     )\n\u001b[1;32m-> 2515\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mlayer_norm(\u001b[39minput\u001b[39;49m, normalized_shape, weight, bias, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 8.00 GiB total capacity; 7.29 GiB already allocated; 0 bytes free; 7.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "df_consumer['predicted_label'] = df_consumer['clean_text'].apply(predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b981f69e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "37f16e38780d10c35648b3bd09ed40a738946f51fc07e16d845fbaf5897e263f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
