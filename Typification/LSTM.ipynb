{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dc9b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c95a5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset with consumer complaints\n",
    "dataset_CC = pd.read_csv(\"C:\\\\Users\\\\hugoo\\\\complaint_data.csv\")\n",
    "\n",
    "#print(dataset_CC.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a437586",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count number of Products\n",
    "print(dataset_CC['Product'].count())\n",
    "\n",
    "#count number of null complaints\n",
    "print(dataset_CC['Consumer complaint narrative'].isnull().sum())\n",
    "\n",
    "#count number of non null complaints\n",
    "print(dataset_CC['Consumer complaint narrative'].count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e588aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of complaints for each product before pre procesing\n",
    "\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "cf.set_config_file(offline=False, world_readable=True)\n",
    "\n",
    "dataset_CC['Product'].value_counts().sort_values(ascending=False).iplot(kind='bar', yTitle='Number of Complaints', title='Number complaints in each product')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054b71fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify lines with empty columns\n",
    "# It is important to mention that there are 4 relevant category columns, namely 'Product', 'Sub-product', 'Issue' and 'Sub-issue'\n",
    "percent_missing = dataset_CC.isnull().sum() * 100 / len(dataset_CC)\n",
    "print(percent_missing.sort_values(ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46d9808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with five columns\n",
    "dataset_CC = dataset_CC[['Product', 'Sub-product', 'Issue', 'Sub-issue', 'Consumer complaint narrative']].copy()\n",
    "\n",
    "print(dataset_CC['Product'].count())\n",
    "\n",
    "# Remove missing values (NaN)\n",
    "dataset_CC = dataset_CC[pd.notnull(dataset_CC['Consumer complaint narrative'])]\n",
    "\n",
    "# after remove null complaints\n",
    "print(dataset_CC['Product'].count())\n",
    "\n",
    "# Renaming second column for a simpler name\n",
    "dataset_CC.columns = ['Product','Sub-product', 'Issue', 'Sub-issue', 'Consumer_complaint'] \n",
    "\n",
    "dataset_CC.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8abbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all lines without consumer narrative which is the text associated with the ticket\n",
    "# This step is crucial because we are gonna use this text to infer the previously mentioned categories.\n",
    "dataset_CC = dataset_CC.dropna(subset=['Consumer_complaint'])\n",
    "print(dataset_CC.head(1))\n",
    "print(dataset_CC['Product'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed94213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saw unique categories of Products\n",
    "pd.DataFrame(dataset_CC.Product.unique()).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d0dbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming categories\n",
    "dataset_CC.replace({'Product': \n",
    "             {'Credit reporting, credit repair services, or other personal consumer reports': \n",
    "              'Credit reporting, repair, or other', \n",
    "              'Credit reporting': 'Credit reporting, repair, or other',\n",
    "             'Credit card': 'Credit card or prepaid card',\n",
    "             'Prepaid card': 'Credit card or prepaid card',\n",
    "             'Payday loan': 'Payday loan, title loan, or personal loan',\n",
    "             'Money transfer': 'Money transfer, virtual currency, or money service',\n",
    "             'Virtual currency': 'Money transfer, virtual currency, or money service'}}, \n",
    "            inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd81c457",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dataset_CC.Product.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf65337",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_CC.Product.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ffb6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of complaints for each product\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "cf.set_config_file(offline=False, world_readable=True)\n",
    "\n",
    "dataset_CC['Product'].value_counts().sort_values(ascending=False).iplot(kind='bar', yTitle='Number of Complaints', title='Number complaints in each product')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc70d842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that have many erros in complaint like \"xxxxx\" and some pointation\n",
    "print(dataset_CC['Consumer_complaint'].iloc[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671c5590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "#start index from zero\n",
    "dataset_CC = dataset_CC.reset_index(drop=True)\n",
    "\n",
    "#replace all the special simbols by space\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "\n",
    "# remove everything that is not letters\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "\n",
    "#remove this stop words because the context of the complaint\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) \n",
    "    text = text.replace('x', '')\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text\n",
    "\n",
    "dataset_CC['Consumer_complaint'] = dataset_CC['Consumer_complaint'].apply(clean_text)\n",
    "dataset_CC['Consumer_complaint'] = dataset_CC['Consumer_complaint'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e82256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that stop words, everything that is not letters, special simbols and \"xxxx\" has been remove\n",
    "print(dataset_CC['Consumer_complaint'].iloc[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8135bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Const for max of words to mantain in vocabulary\n",
    "MAX_WORDS = 50000\n",
    "\n",
    "# Const for max of words to use per complaint\n",
    "MAX_WORDS_COMPLAINT = 2500\n",
    "\n",
    "# size of vector per word\n",
    "VECTOR_SIZE = 100\n",
    "\n",
    "# all words that is not in vocabulary are replace to oov_token\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<OOV>\", lower=True)\n",
    "tokenizer.fit_on_texts(dataset_CC['Consumer_complaint'].values) \n",
    "\n",
    "#get number of unique tokens\n",
    "word_index = tokenizer.word_index \n",
    "\n",
    "print('Encontrados %s tokens únicos.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee6fcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# receive a list of words and return a list of index (the smallest index its the most used word)\n",
    "X = tokenizer.texts_to_sequences(dataset_CC['Consumer_complaint'].values)\n",
    "\n",
    "# convert all sequences of text with the same lenght, if the sequence is more smallest that the length its increment zeros\n",
    "# else sequence its cut\n",
    "X = pad_sequences(X, maxlen=MAX_WORDS_COMPLAINT)\n",
    "\n",
    "# that result means, we have 481087 rows of complaints that have 2500 index of words\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f447e529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert categorical labels to numbers\n",
    "Y = pd.get_dummies(dataset_CC['Product']).values\n",
    "\n",
    "#that result means, we have 481087 products with 13 different numbers (categories ex: \"credict card or prepaid card\")\n",
    "print('Shape of label tensor:', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b226c13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# division in test data and training data\n",
    "# random_state=42 ensure that the data division is allways the same\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930e1154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "# use sequential because the LSTM model its a neurol network which learn by data sequences\n",
    "model = Sequential()\n",
    "\n",
    "# use Embedding to define the max words in vocabulary, max word per complaint and vetor_size por each word\n",
    "model.add(Embedding(MAX_WORDS, VECTOR_SIZE, input_length=MAX_WORDS_COMPLAINT))\n",
    "\n",
    "#hiperparameter -> dropout (turning off some neurons) and recurrent_dropout(turning of some recurrent conexions) -> to avoid \n",
    "# the overfitting\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "# softmax to convert numbers vector to probabilistic vector, highest probability is the chosen neuron\n",
    "model.add(Dense(13, activation='softmax'))\n",
    "\n",
    "# the loss function is for training to evaluate the difference between neural \n",
    "#network predictions and actual outputs, the optimizer the optimization algorithm that will be \n",
    "#used to adjust neural network weights during training\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# how many times the entire dataset will be passed through the model during training.\n",
    "epochs = 5\n",
    "\n",
    "#number of training samples to work through before the model’s internal parameters are updated\n",
    "batch_size = 10\n",
    "\n",
    "\n",
    "#validation_split means that 10% of the training data will be used for validation\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, verbose=1, min_delta=0.0001)])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
